gini, disini v1 udah bener bener kacau, anda harus fokus Pada penerapan fitur. pastiin sesuai dengan dokumentasi


KHUSUS UNTUK V1 Nih, pastiin bener bener ada.

gemini-3-pro-preview
gemini-2.5-pro
gemini-2.5-flash
gemini-2.0-flash

gemini-3-pro-image-preview
gemini-2.5-flash-image
imagen-3.0-generate-002
imagen-3.0-generate-001
imagen-3.0-fast-generate-001
imagen-3.0-capability-001

imagen-4.0-generate-001
imagen-4.0-fast-generate-001
imagen-4.0-ultra-generate-001

imagen-4.0-upscale-preview.
virtual-try-on-preview-08-04
imagen-product-recontext-preview-06-30

veo-2.0-generate-001
veo-2.0-generate-exp
veo-2.0-generate-preview

veo-3.0-generate-001
veo-3.0-generate-001
veo-3.0-generate-preview
veo-3.0-fast-generate-preview

veo-3.1-generate-001
veo-3.1-fast-generate-001
veo-3.1-generate-preview
veo-3.1-fast-generate-preview

lyria-002

	gemini-2.5-flash-tts
    gemini-2.5-flash-lite-preview-tts
    gemini-2.5-pro-tts

    Create audio from text by using client libraries

bookmark_border
This quickstart walks you through the process of using client libraries to make a request to Cloud TTS, creating audio from text.

To learn more about the fundamental concepts in Cloud Text-to-Speech, read Cloud Text-to-Speech Basics. To see which synthetic voices are available for your language, see the supported voices and languages page.

Before you begin
Before you can send a request to the Cloud Text-to-Speech API, you must have completed the following actions. See the before you begin page for details.

Enable Cloud Text-to-Speech on a Google Cloud project.
Make sure billing is enabled for Cloud Text-to-Speech.
Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

If you're using a local shell, then create local authentication credentials for your user account:



gcloud auth application-default login
You don't need to do this if you're using Cloud Shell.

If an authentication error is returned, and you are using an external identity provider (IdP), confirm that you have signed in to the gcloud CLI with your federated identity.

Install the client library
Go
Java
Node.js
Python
Additional languages
Before installing the library, make sure you've prepared your environment for Node.js development.



npm install @google-cloud/text-to-speech
Create audio data
Now you can use Cloud TTS to create an audio file of synthetic human speech. Use the following code to send a synthesize request to the Cloud Text-to-Speech API.

Go
Java
Node.js
Python
Before running the example, make sure you've prepared your environment for Node.js development.




// Imports the Google Cloud client library
const textToSpeech = require('@google-cloud/text-to-speech');

// Import other required libraries
const {writeFile} = require('node:fs/promises');

// Creates a client
const client = new textToSpeech.TextToSpeechClient();

async function quickStart() {
  // The text to synthesize
  const text = 'hello, world!';

  // Construct the request
  const request = {
    input: {text: text},
    // Select the language and SSML voice gender (optional)
    voice: {languageCode: 'en-US', ssmlGender: 'NEUTRAL'},
    // select the type of audio encoding
    audioConfig: {audioEncoding: 'MP3'},
  };

  // Performs the text-to-speech request
  const [response] = await client.synthesizeSpeech(request);

  // Save the generated binary audio content to a local file
  await writeFile('output.mp3', response.audioContent, 'binary');
  console.log('Audio content written to file: output.mp3');
}

await quickStart();
Congratulations! You've sent your first request to Cloud Text-to-Speech.

How did it go?
It worked!
I got stuck.
Clean up
To avoid incurring charges to your Google Cloud account for the resources used on this page, follow these steps.

Use the Google Cloud console to delete your project if you don't need it.


Create audio from text by using the command line

bookmark_border
This document walks you through the process of making a request to Cloud Text-to-Speech using the command line. To learn more about the fundamental concepts in Cloud Text-to-Speech, read Cloud Text-to-Speech Basics.

Before you begin
Before you can send a request to the Cloud Text-to-Speech API, you must have completed the following actions. See the before you begin page for details.

Enable Cloud Text-to-Speech on a GCP project.
Make sure billing is enabled for Cloud Text-to-Speech.
Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

Synthesize audio from text
You can convert text to audio by making an HTTP POST request to the https://texttospeech.googleapis.com/v1/text:synthesize endpoint. In the body of your POST command, specify the type of voice to synthesize in the voice configuration section, specify the text to synthesize in the text field of the input section, and specify the type of audio to create in the audioConfig section.

Execute the REST request below at the command line to synthesize audio from text using Cloud TTS. The command uses the gcloud auth application-default print-access-token command to retrieve an authorization token for the request.


Before using any of the request data, make the following replacements:

PROJECT_ID: the alphanumeric ID of your Google Cloud project.
HTTP method and URL:



POST https://texttospeech.googleapis.com/v1/text:synthesize
Request JSON body:



{
  "input": {
    "text": "Android is a mobile operating system developed by Google, based on the Linux kernel and designed primarily for touchscreen mobile devices such as smartphones and tablets."
  },
  "voice": {
    "languageCode": "en-gb",
    "name": "en-GB-Standard-A",
    "ssmlGender": "FEMALE"
  },
  "audioConfig": {
    "audioEncoding": "MP3"
  }
}
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "audioContent": "//NExAASCCIIAAhEAGAAEMW4kAYPnwwIKw/BBTpwTvB+IAxIfghUfW.."
}
The JSON output for the REST command contains the synthesized audio in base64-encoded format. Copy the contents of the audioContent field into a new file named synthesize-output-base64.txt. Your new file will look something like the following:


//NExAARqoIIAAhEuWAAAGNmBGMY4EBcxvABAXBPmPIAF//yAuh9Tn5CEap3/o
...
VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
Decode the contents of the synthesize-output-base64.txt file into a new file named synthesized-audio.mp3. For information on decoding base64, see Decoding Base64-Encoded Audio Content.

Linux
Mac OSX
Windows
Copy only the base-64 encoded content into a text file.

Decode the source text file using the base64 command line tool by using the -d flag:



    $ base64 SOURCE_BASE64_TEXT_FILE -d > DESTINATION_AUDIO_FILE
Play the contents of synthesized-audio.mp3 in an audio application or on an audio device. You can also open the synthesized-audio.mp3 in the Chrome browser to play the audio by navigating to the folder that contains the file, for example file://my_file_path/synthesized-audio.mp3

Clean up
To avoid unnecessary Google Cloud Platform charges, use the Google Cloud console to delete your project if you do not need it.

What's next

Learn more about Cloud Text-to-Speech by reading the basics.
Review the list of available voices you can use for synthetic speech.

Create voice audio files

bookmark_border
Cloud Text-to-Speech allows you to convert words and sentences into base64 encoded audio data of natural human speech. You can then convert the audio data into a playable audio file like an MP3 by decoding the base64 data. The Cloud Text-to-Speech API accepts input as raw text or Speech Synthesis Markup Language (SSML).

This document describes how to create an audio file from either text or SSML input using Cloud TTS. You can also review the Cloud TTS basics article if you are unfamiliar with concepts like speech synthesis or SSML.


These samples require that you have installed and initialized the Google Cloud CLI. For information about setting up the gcloud CLI, see Authenticate to Cloud TTS.

Convert text to synthetic voice audio
The following code samples demonstrate how to convert a string into audio data.

You can configure the output of speech synthesis in a variety of ways, including selecting a unique voice or modulating the output in pitch, volume, speaking rate, and sample rate.

Note: See endpoints documentation for specifics on using Neural2 voices.
Protocol
Go
Java
Node.js
Python
Additional languages
To learn how to install and use the client library for Cloud TTS, see Cloud TTS client libraries. For more information, see the Cloud TTS Node.js API reference documentation.

To authenticate to Cloud TTS, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.




const textToSpeech = require('@google-cloud/text-to-speech');
const fs = require('fs');
const util = require('util');

const client = new textToSpeech.TextToSpeechClient();

/**
 * TODO(developer): Uncomment the following lines before running the sample.
 */
// const text = 'Text to synthesize, eg. hello';
// const outputFile = 'Local path to save audio file to, e.g. output.mp3';

const request = {
  input: {text: text},
  voice: {languageCode: 'en-US', ssmlGender: 'FEMALE'},
  audioConfig: {audioEncoding: 'MP3'},
};
const [response] = await client.synthesizeSpeech(request);
const writeFile = util.promisify(fs.writeFile);
await writeFile(outputFile, response.audioContent, 'binary');
console.log(`Audio content written to file: ${outputFile}`);
Convert SSML to synthetic voice audio
Using SSML in your audio synthesis request can produce audio that is more similar to natural human speech. Specifically, SSML gives you finer-grain control over how the audio output represents pauses in the speech or how the audio pronounces dates, times, acronyms, and abbreviations.

For more details on the SSML elements supported by Cloud Text-to-Speech API, see the SSML reference.

Protocol
Go
Java
Node.js
Python
Additional languages
To learn how to install and use the client library for Cloud TTS, see Cloud TTS client libraries. For more information, see the Cloud TTS Node.js API reference documentation.

To authenticate to Cloud TTS, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.




const textToSpeech = require('@google-cloud/text-to-speech');
const fs = require('fs');
const util = require('util');

const client = new textToSpeech.TextToSpeechClient();

/**
 * TODO(developer): Uncomment the following lines before running the sample.
 */
// const ssml = '<speak>Hello there.</speak>';
// const outputFile = 'Local path to save audio file to, e.g. output.mp3';

const request = {
  input: {ssml: ssml},
  voice: {languageCode: 'en-US', ssmlGender: 'FEMALE'},
  audioConfig: {audioEncoding: 'MP3'},
};

const [response] = await client.synthesizeSpeech(request);
const writeFile = util.promisify(fs.writeFile);
await writeFile(outputFile, response.audioContent, 'binary');
console.log(`Audio content written to file: ${outputFile}`);


Create long-form audio

bookmark_border
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

This document walks you through the process of synthesizing long-form audio. Long Audio Synthesis asynchronously synthesizes up to 1 million bytes on input. To learn more about the fundamental concepts in Cloud Text-to-Speech, read Cloud Text-to-Speech Basics.

Before you begin
Before you can send a request to the Cloud Text-to-Speech API, you must have completed the following actions. See the before you begin page for details.

Enable Cloud Text-to-Speech on a Google Cloud project.
Make sure billing is enabled for Cloud Text-to-Speech.
Make sure you have the following Identity and Access Management (IAM) roles on the output Google Cloud bucket.
Storage Object Creator
Storage Object Viewer
Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

Synthesize long audio from text using the command line
You can convert long-form text to audio by making an HTTP POST request to the https://texttospeech.googleapis.com/v1beta1/projects/{$project_number}/locations/global:synthesizeLongAudio endpoint. In the body of your POST command, specify the following fields.

• voice: The type of voice to synthesize.

• input.text: The text to synthesize.

• audioConfig: The type of audio to create.

• output_gcs_uri: The Google Cloud output path under the form of "gs://bucket_name/file_name.wav".

• parent: The parent under the form "projects/{YOUR_PROJECT_NUMBER}/locations/{YOUR_PROJECT_LOCATION}".

The input can contain up to 1MB of characters, the exact limit can vary from different inputs.

Create a Google Cloud storage bucket under the project that is used to run the synthesis. Make sure the service account used to run the synthesis has read and write access to the output Google Cloud bucket.

Execute the REST request at the command line to synthesize the audio from the text using Cloud TTS. The command uses the gcloud auth application-default print-access-token command to retrieve an authorization token for the request.


HTTP method and URL:



POST https://texttospeech.googleapis.com/v1beta1/projects/12345/locations/global:synthesizeLongAudio
Request JSON body:



{
  "parent": "projects/12345/locations/global",
  "audio_config":{
      "audio_encoding":"LINEAR16"
  },
  "input":{
      "text":"hello"
  },
  "voice":{
      "language_code":"en-us",
      "name":"en-us-Standard-A"
  },
  "output_gcs_uri": "gs://bucket_name/file_name.wav"
}
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "name": "23456",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.texttospeech.v1beta1.SynthesizeLongAudioMetadata",
    "progressPercentage": 0,
    "startTime": "2022-12-20T00:46:56.296191037Z",
    "lastUpdateTime": "2022-12-20T00:46:56.296191037Z"
  },
  "done": false
}
The JSON output for the REST command contains the long operation name in the name field. Execute the REST request at the command line to query the state of the long-running operation.

Make sure that the service account running the GET operation is from the same project as the one used for synthesis.


HTTP method and URL:



GET https://texttospeech.googleapis.com/v1beta1/projects/12345/locations/global/operations/23456
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "name": "projects/12345/locations/global/operations/23456",
  "metadata": {
    "@type": "type.googleapis.com/google.cloud.texttospeech.v1beta1.SynthesizeLongAudioMetadata",
    "progressPercentage": 100
  },
  "done": true
}
Query the list of all operations running under a given project, execute the REST request.

Make sure that the service account running the LIST operation is from the same project as the one used for synthesis.


HTTP method and URL:



GET https://texttospeech.googleapis.com/v1beta1/projects/12345/locations/global/operations
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "operations": [
    {
      "name": "12345",
      "done": false
    },
    {
      "name": "23456",
      "done": false
    }
  ],
  "nextPageToken": ""
}
Once the long-running operation successfully completes, find the output audio file in the given bucket uri in the output_gcs_uri field. If the operation did not complete successfully, find the error by querying using the GET REST command, correct the error, and issue the RPC again.

Synthesize long audio from text using client libraries
Follow these instructions for synthesizing long audio.

Install the client library
Python
Before installing the library, make sure you've prepared your environment for Python development.



pip install --upgrade google-cloud-texttospeech
Create audio data
You can use Cloud TTS to create a long audio file of synthetic human speech. Use the following code to create a long audio file in your Google Cloud bucket.

Python
Before running the example, make sure you've prepared your environment for Python development.




# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from google.cloud import texttospeech


def synthesize_long_audio(project_id: str, output_gcs_uri: str) -> None:
    """
    Synthesizes long input, writing the resulting audio to `output_gcs_uri`.

    Args:
        project_id: ID or number of the Google Cloud project you want to use.
        output_gcs_uri: Specifies a Cloud Storage URI for the synthesis results.
            Must be specified in the format:
            ``gs://bucket_name/object_name``, and the bucket must
            already exist.
    """

    client = texttospeech.TextToSpeechLongAudioSynthesizeClient()

    input = texttospeech.SynthesisInput(
        text="Test input. Replace this with any text you want to synthesize, up to 1 million bytes long!"
    )

    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.LINEAR16
    )

    voice = texttospeech.VoiceSelectionParams(
        language_code="en-US", name="en-US-Standard-A"
    )

    parent = f"projects/{project_id}/locations/us-central1"

    request = texttospeech.SynthesizeLongAudioRequest(
        parent=parent,
        input=input,
        audio_config=audio_config,
        voice=voice,
        output_gcs_uri=output_gcs_uri,
    )

    operation = client.synthesize_long_audio(request=request)
    # Set a deadline for your LRO to finish. 300 seconds is reasonable, but can be adjusted depending on the length of the input.
    # If the operation times out, that likely means there was an error. In that case, inspect the error, and try again.
    result = operation.result(timeout=300)
    print(
        "\nFinished processing, check your GCS bucket to find your audio file! Printing what should be an empty result: ",
        result,
    )
Clean up
To avoid unnecessary Google Cloud charges, use the Google Cloud console to delete your project if you don't need it.

THIS IS How To Build:
Generate text
For testing and iterating on chat prompts, we recommend using the Google Cloud console. To send prompts programmatically to the model, you can use the REST API, Google Gen AI SDK, Vertex AI SDK for Python, or one of the other supported libraries and SDKs.

You can use system instructions to steer the behavior of the model based on a specific need or use case. For example, you can define a persona or role for a chatbot that responds to customer service requests. For more information, see the system instructions code samples.

You can use the Google Gen AI SDK to send requests if you're using Gemini 2.0 Flash.

Here is a simple text generation example.

Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateContent(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: 'How does AI work?',
  });

  console.log(response.text);

  return response.text;
}
Streaming and non-streaming responses
You can choose whether the model generates streaming responses or non-streaming responses. For streaming responses, you receive each response as soon as its output token is generated. For non-streaming responses, you receive all responses after all of the output tokens are generated.

Here is a streaming text generation example.

Before trying this sample, follow the Python setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Python API reference documentation.

To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.




from google import genai
from google.genai.types import HttpOptions

client = genai.Client(http_options=HttpOptions(api_version="v1"))
chat_session = client.chats.create(model="gemini-2.5-flash")

for chunk in chat_session.send_message_stream("Why is the sky blue?"):
    print(chunk.text, end="")
# Example response:
# The
#  sky appears blue due to a phenomenon called **Rayleigh scattering**. Here's
#  a breakdown of why:
# ...
Gemini multiturn chat behavior
When you use multiturn chat, Vertex AI locally stores the initial content and prompts that you sent to the model. Vertex AI sends all of this data with each subsequent request to the model. Consequently, the input costs for each message that you send is a running total of all the data that was already sent to the model. If your initial content is sufficiently large, consider using context caching when you create the initial model object to better control input costs.

This document describes what system instructions are and best practices for writing effective system instructions. To learn how to add system instructions to your prompts, see Use system instructions instead.

System instructions are a set of instructions that the model processes before it processes prompts. We recommend that you use system instructions to tell the model how you want it to behave and respond to prompts. For example, you can include things like a persona to adopt, contextual information, and formatting instructions.

System instructions are especially useful in cases when you want to provide the model with information that an end user can't see or change, which can help you give the model additional context to understand the task, provide more customized responses, adhere to specific guidelines over the full user interaction with the model. Additionally, system instructions are beneficial when you want to run multiple prompts with the same set of instructions.

When system instructions are set, they apply to the entire request. System instructions work across multiple user and model turns when included in the prompt. Though system instructions are separate from the contents of the prompt, they're still part of your overall prompts and therefore subject to standard data use policies.

Note: System instructions can help guide the model to follow instructions, but they don't fully prevent jailbreaks or leaks. We recommend exercising caution around putting any sensitive information in system instructions.
Use cases
Generally, we recommend using system instructions to do one or more of the following:

Define a persona or role
Define output format
Define output style and tone
Define goals or rules for the task
Provide additional context for the prompt

You can influence how a model responds to prompts by defining a persona or role in system instructions.

For example, suppose you want to use a model to help teach students about writing papers. Compare the difference in the model's responses when you define the following roles:

You are a bot, tasked with teaching college students about how to write a paper about a given subject.
You are a bot, tasked with helping primary school students about how to write a paper about a given subject.

You can influence how a model formats its response by defining an output format, such as Markdown or JSON, in system instructions.

For example, suppose you want to use a model to summarize customer reviews for your business. Compare the difference in the model's responses when you define the following output formats:

Output your response in JSON. At the very end, outside the JSON object, write 3-5 sentences of advice to the business on how to improve.
No formatting requirements.

You can influence a model's response by defining its output style and tone, such as verbosity, formality, and target reading level, in system instructions.

For example, suppose you want to use a model to write proposals for government initiatives. Compare the difference in the model's responses when you define the following tones:

Professional
Casual

You can influence a model's response by defining goals or rules, in other words, what the model should or shouldn't do, in system instructions.

For example, suppose you want to use a model to write Python code. Compare the difference in the model's responses when you define the following rules:

When generating code, make sure to include docstrings explaining the inputs, outputs, and usage of every method.
Only generate code, do not include any docstrings.

You can influence how a model responds to prompts by adding additional context in system instructions.

For example, suppose you want to use a model to write speeches for events. Compare the difference in the model's responses when you provide the following context:

Things to include in the speech.
No extra context.

Function calling, also known as tool use, provides the LLM with definitions of external tools (for example, a get_current_weather function). When processing a prompt, the model intelligently determines if a tool is needed and, if so, outputs structured data specifying the tool to call and its parameters (for example, get_current_weather(location='Boston')). Your application then executes this tool, feeds the result back to the model, allowing it to complete its response with dynamic, real-world information or the outcome of an action. This effectively bridges the LLM with your systems and extends its capabilities.

Diagram illustrating the Function Calling Interaction flow 

Function calling enables two primary use cases:

Fetching data: Retrieve up-to-date information for model responses, such as current weather, currency conversion, or specific data from knowledge bases and APIs (RAG).

Taking action: Perform external operations like submitting forms, updating application state, or orchestrating agentic workflows (e.g., conversation handoffs).

For more use cases and examples that are powered by function calling, see Use cases.

Features and limitations
The following models support function calling:

Gemini models:

Gemini 3 Pro preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.5 Flash with Live API native audio preview
Gemini 2.0 Flash with Live API preview
Gemini 2.0 Flash
Gemini 2.0 Flash-Lite

You can specify up to 512 FunctionDeclarations

Define your functions in the OpenAPI schema format.

For Gemini 3 Pro and later models, you can include multimodal content (images and PDFs) in functionResponse messages that you send back to the model. For more information, see Multimodal function responses.

For Gemini 3 Pro and later models, you can stream function call arguments as they are generated by setting streamFunctionCallArguments to true in the functionCallingConfig. For more information, see Streaming function call arguments.

For best practices related to the function declarations, including tips for names and descriptions, see Best practices.

For Open Models, follow this user guide.

How to create a function calling application
To use function calling, perform the following tasks:

Submit function declarations and prompt to the model.
Provide the API output to the model.
Step 1: Submit the prompt and function declarations to the model
Declare a Tool in a schema format that's compatible with the OpenAPI schema. For more information, see Schema examples.

The following examples submit a prompt and function declaration to the Gemini models:

This example demonstrates a text scenario with one function and one prompt.


Node.js
Before trying this sample, follow the Node.js setup instructions in the Vertex AI quickstart using client libraries. For more information, see the Vertex AI Node.js API reference documentation.

To authenticate to Vertex AI, set up Application Default Credentials. For more information, see Set up authentication for a local development environment.




const {
  VertexAI,
  FunctionDeclarationSchemaType,
} = require('@google-cloud/vertexai');

const functionDeclarations = [
  {
    function_declarations: [
      {
        name: 'get_current_weather',
        description: 'get weather in a given location',
        parameters: {
          type: FunctionDeclarationSchemaType.OBJECT,
          properties: {
            location: {type: FunctionDeclarationSchemaType.STRING},
            unit: {
              type: FunctionDeclarationSchemaType.STRING,
              enum: ['celsius', 'fahrenheit'],
            },
          },
          required: ['location'],
        },
      },
    ],
  },
];

const functionResponseParts = [
  {
    functionResponse: {
      name: 'get_current_weather',
      response: {name: 'get_current_weather', content: {weather: 'super nice'}},
    },
  },
];

/**
 * TODO(developer): Update these variables before running the sample.
 */
async function functionCallingStreamContent(
  projectId = 'PROJECT_ID',
  location = 'us-central1',
  model = 'gemini-2.0-flash-001'
) {
  // Initialize Vertex with your Cloud project and location
  const vertexAI = new VertexAI({project: projectId, location: location});

  // Instantiate the model
  const generativeModel = vertexAI.getGenerativeModel({
    model: model,
  });

  const request = {
    contents: [
      {role: 'user', parts: [{text: 'What is the weather in Boston?'}]},
      {
        role: 'ASSISTANT',
        parts: [
          {
            functionCall: {
              name: 'get_current_weather',
              args: {location: 'Boston'},
            },
          },
        ],
      },
      {role: 'USER', parts: functionResponseParts},
    ],
    tools: functionDeclarations,
  };
  const streamingResp = await generativeModel.generateContentStream(request);
  for await (const item of streamingResp.stream) {
    console.log(item.candidates[0].content.parts[0].text);
  }
}
If the model determines that it needs the output of a particular function, the response that the application receives from the model contains the function name and the parameter values that the function should be called with.

The following is an example of a model response to the user prompt "What is the weather like in Boston?". The model proposes calling the get_current_weather function with the parameter Boston, MA.


candidates {
  content {
    role: "model"
    parts {
      function_call {
        name: "get_current_weather"
        args {
          fields {
            key: "location"
            value {
              string_value: "Boston, MA"
            }
          }
        }
      }
    }
  }
  ...
}
Step 2: Provide the API output to the model
Invoke the external API and pass the API output back to the model.

The following example uses synthetic data to simulate a response payload from an external API and submits the output back to the model:

PROJECT_ID=myproject
MODEL_ID=gemini-2.5-flash
LOCATION="us-central1"

curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \
-d '{
"contents": [
{
  "role": "user",
  "parts": {
    "text": "What is the weather in Boston?"
  }
},
{
  "role": "model",
  "parts": [
    {
      "functionCall": {
        "name": "get_current_weather",
        "args": {
          "location": "Boston, MA"
        }
      }
    }
  ]
},
{
  "role": "user",
  "parts": [
    {
      "functionResponse": {
        "name": "get_current_weather",
        "response": {
          "temperature": 20,
          "unit": "C"
        }
      }
    }
  ]
}
],
"tools": [
{
  "function_declarations": [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a specific location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city name of the location for which to get the weather."
          }
        },
        "required": [
          "location"
        ]
      }
    }
  ]
}
]
}'
For best practices related to API invocation, see Best practices - API invocation.

If the model had proposed several parallel function calls, the application must provide all of the responses back to the model. To learn more, see Parallel function calling example.

The model may determine that the output of another function is necessary for responding to the prompt. In this case, the response that the application receives from the model contains another function name and another set of parameter values.

If the model determines that the API response is sufficient for responding to the user's prompt, it creates a natural language response and returns it to the application. In this case, the application must pass the response back to the user. The following is an example of a natural language response:


It is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies.
Function calling with thoughts
To see an example of how to use thought signatures, run the "Intro to thought signatures" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

When calling functions with thinking enabled, you'll need to get the thought_signature from the model response object and return it when you send the result of the function execution back to the model. For example:

Python


# Call the model with function declarations
# ...Generation config, Configure the client, and Define user prompt (No changes)

# Send request with declarations (using a thinking model)
response = client.models.generate_content(
  model="gemini-2.5-flash", config=config, contents=contents)

# See thought signatures
for part in response.candidates[0].content.parts:
  if not part.text:
    continue
  if part.thought and part.thought_signature:
    print("Thought signature:")
    print(part.thought_signature)
Viewing thought signatures isn't required, but you will need to adjust Step 2 to return them along with the result of the function execution so it can incorporate the thoughts into its final response:

Python


# Create user friendly response with function result and call the model again
# ...Create a function response part (No change)

# Append thought signatures, function call and result of the function execution to contents
function_call_content = response.candidates[0].content
# Append the model's function call message, which includes thought signatures
contents.append(function_call_content)
contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

final_response = client.models.generate_content(
    model="gemini-2.5-flash",
    config=config,
    contents=contents,
)

print(final_response.text)
When returning thought signatures, follow these guidelines:

The model returns signatures within other parts in the response, for example function calling or text, text, or thought summaries parts. Return the entire response with all parts back to the model in subsequent turns.
Don't merge part with one signature with another part which also contains a signature. Signatures can't be concatenated together.
Don't merge one part with a signature with another part without a signature. This breaks the correct positioning of the thought represented by the signature.
Learn more about limitations and usage of thought signatures, and about thinking models.

Parallel function calling
For prompts such as "Get weather details in Boston and San Francisco?", the model may propose several parallel function calls. For a list of models that support parallel function calling, see Supported models.

To learn about parallel function calling with an end-to-end Jupyter notebook tutorial, see Working with Parallel Function Calls and Multiple Function Responses in Gemini
REST
Python
Go
This example demonstrates a scenario with one get_current_weather function. The user prompt is "Get weather details in Boston and San Francisco?". The model proposes two parallel get_current_weather function calls: one with the parameter Boston and the other with the parameter San Francisco.

To learn more about the request parameters, see Gemini API.


{
"candidates": [
  {
    "content": {
      "role": "model",
      "parts": [
        {
          "functionCall": {
            "name": "get_current_weather",
            "args": {
              "location": "Boston"
            }
          }
        },
        {
          "functionCall": {
            "name": "get_current_weather",
            "args": {
              "location": "San Francisco"
            }
          }
        }
      ]
    },
    ...
  }
],
...
}
The following command demonstrates how you can provide the function output to the model. Replace my-project with the name of your Google Cloud project.

PROJECT_ID=my-project
MODEL_ID=gemini-2.5-flash
LOCATION="us-central1"
curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \
-d '{
"contents": [
{
  "role": "user",
  "parts": {
    "text": "What is difference in temperature in Boston and San Francisco?"
  }
},
{
  "role": "model",
  "parts": [
    {
      "functionCall": {
        "name": "get_current_weather",
        "args": {
          "location": "Boston"
        }
      }
    },
    {
      "functionCall": {
        "name": "get_current_weather",
        "args": {
          "location": "San Francisco"
        }
      }
    }
  ]
},
{
  "role": "user",
  "parts": [
    {
      "functionResponse": {
        "name": "get_current_weather",
        "response": {
          "temperature": 30.5,
          "unit": "C"
        }
      }
    },
    {
      "functionResponse": {
        "name": "get_current_weather",
        "response": {
          "temperature": 20,
          "unit": "C"
        }
      }
    }
  ]
}
],
"tools": [
{
  "function_declarations": [
    {
      "name": "get_current_weather",
      "description": "Get the current weather in a specific location",
      "parameters": {
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city name of the location for which to get the weather."
          }
        },
        "required": [
          "location"
        ]
      }
    }
  ]
}
]
}'
  
The natural language response created by the model is similar to the following:

[
{
    "candidates": [
        {
            "content": {
                "parts": [
                    {
                        "text": "The temperature in Boston is 30.5C and the temperature in San Francisco is 20C. The difference is 10.5C. \n"
                    }
                ]
            },
            "finishReason": "STOP",
            ...
        }
    ]
    ...
}
]


Multimodal function responses
Preview

This feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

For Gemini 3 Pro and later models, you can include multimodal content in the function response parts that you send to the model. The model can process this multimodal content in its next turn to produce a more informed response. The following MIME types are supported for multimodal content in function responses:

Images: image/png, image/jpeg, image/webp
Documents: application/pdf, text/plain
To include multimodal data in a function response, include it as one or more parts nested within the functionResponse part. Each multimodal part must contain inlineData or fileData. If you reference a multimodal part from within the structured response field, it must contain a unique displayName.

You can also reference a multimodal part from within the structured response field of the functionResponse part by using the JSON reference format {"$ref": "<displayName>"}. The model substitutes the reference with the multimodal content when processing the response. Each displayName can only be referenced once in the structured response field.

The following example shows a message containing a functionResponse for a function named get_image and a nested part containing image data with displayName: "wakeupcat.jpg". The functionResponse's response field references this image part:

Python
REST


"contents": [
  ...,
  {
    "role": "user",
    "parts": [
      {
        "functionResponse": {
          "name": "get_image",
          "response": {
            "image_ref": {
              "$ref": "wakeupcat.jpg"
            }
          },
          "parts": [
            {
              "fileData": {
                "displayName": "wakeupcat.jpg",
                "mimeType": "image/jpeg",
                "fileUri": "gs://cloud-samples-data/vision/label/wakeupcat.jpg"
              }
            }
          ]
        }
      }
    ]
  }
]
Streaming function call arguments
Preview

This feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

For Gemini 3 Pro and later models, you can request that function call arguments be streamed as they are generated by the model, rather than waiting for the full set of arguments to be generated. This is useful for reducing perceived latency when functions need to be called.

This feature has the following limitations:

This feature is available in both v1 and v1beta1 API versions.
To enable streaming for function call arguments, set streamFunctionCallArguments to true inside toolConfig.functionCallingConfig when calling streamGenerateContent.

When streamFunctionCallArguments is enabled, intermediate responses will contain a functionCall object with partialArgs and willContinue fields. partialArgs contains argument fragments as they are generated, and willContinue indicates if more fragments for the function call are expected.

partialArgs: An array of PartialArg objects, each containing:
jsonPath: A JSONPath string indicating the path to this fragment within the function's parameters object. The path can point to an argument (e.g. $.location) or an element inside an argument if the argument is an object (e.g. $.location.latitude).
A value for the fragment, which can be numberValue, stringValue, boolValue, or nullValue.
willContinue: A boolean within a partialArgs object. This field is only true for stringValue fragments when the string value is being streamed in chunks and more chunks are expected for this argument.
willContinue: A boolean within the functionCall object. If true, more partialArgs are expected for the overall function call in subsequent streamed responses. If false or absent, this is the final streamed response for the current function call.
The following example shows a sequence of streamed generateContent chunks where arguments for a single function call are streamed:



{
  "parts": [
    {
      "functionCall": {
        "name": "controlLight",
        "partialArgs": [
          {
            "jsonPath": "$.brightness",
            "numberValue": 50
          }
        ],
        "willContinue": true
      }
    }
  ],
  "role": "model"
}
{
  "parts": [
    {
      "functionCall": {
        "partialArgs": [
          {
            "jsonPath": "$.colorTemperature",
            "stringValue": "warm",
            "willContinue": true
          }
        ],
        "willContinue": true
      }
    }
  ],
  "role": "model"
}
{
  "parts": [
    {
      "functionCall": {
        "partialArgs": [
          {
            "jsonPath": "$.colorTemperature"
          }
        ],
        "willContinue": true
      }
    }
  ],
  "role": "model"
}
{
  "parts": [
    {
      "functionCall": {}
    }
  ],
  "role": "model"
}
The following example shows how arguments for parallel function calls might be streamed across multiple responses for the prompt "What is difference in temperature in New Delhi and San Francisco?":



{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "name": "get_current_weather",
          "willContinue": true
        },
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "partialArgs": [{
            "jsonPath": "$.location",
            "stringValue": "New Delhi",
            "willContinue": true
          }],
          "willContinue": true
        }
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "partialArgs": [{
            "jsonPath": "$.location",
            "stringValue": ""
          }],
          "willContinue": true
        }
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
        }
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "name": "get_current_weather",
          "willContinue": true
        },
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "partialArgs": [{
            "jsonPath": "$.location",
            "stringValue": "San Francisco",
            "willContinue": true
          }],
          "willContinue": true
        }
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
          "partialArgs": [{
            "jsonPath": "$.location",
            "stringValue": ""
          }],
          "willContinue": true
        }
      }]
    }
  }],
}
{
  "candidates": [{
    "content": {
      "role": "model",
      "parts": [{
        "functionCall": {
        }
      }]
    }
  }],
}
Python


from google import genai
from google.genai import types

client = genai.Client()

get_weather_declaration = types.FunctionDeclaration(
  name="get_weather",
  description="Gets the current weather temperature for a given location.",
  parameters={
      "type": "object",
      "properties": {"location": {"type": "string"}},
      "required": ["location"],
  },
)
get_weather_tool = types.Tool(function_declarations=[get_weather_declaration])

for chunk in client.models.generate_content_stream(
  model="gemini-3-pro-preview",
  contents="What's the weather in London and New York?",
  config=types.GenerateContentConfig(
      tools=[get_weather_tool],
      tool_config = types.ToolConfig(
          function_calling_config=types.FunctionCallingConfig(
              mode=types.FunctionCallingConfigMode.AUTO,
              stream_function_call_arguments=True,
          )
      ),
  ),
):
  function_call = chunk.function_calls[0]
  if function_call and function_call.name:
      print(f"{function_call.name}")
      print(f"will_continue={function_call.will_continue}")

Function calling modes
You can control how the model uses the provided tools (function declarations) by setting the mode within the function_calling_config.

Mode	Description
AUTO	The default model behavior. The model decides whether to predict function calls or respond with natural language based on the context. This is the most flexible mode and recommended for most scenarios.
VALIDATED (Preview)	The model is constrained to predict either function calls or natural language, and ensures function schema adherence. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions.
ANY	The model is constrained to always predict one or more function calls and ensures function schema adherence. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions. Use this mode when you require a function call response to every prompt (if applicable).
NONE	The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this mode to temporarily disable function calling without removing your tool definitions.
Note: Preview features are only available in the v1beta1 version of the API.
Forced function calling
Instead of allowing the model to choose between a natural language response and a function call, you can force it to only predict function calls. This is known as forced function calling. You can also choose to provide the model with a full set of function declarations, but restrict its responses to a subset of these functions.

The following example is forced to predict only get_weather function calls.

Python


response = model.generate_content(
    contents = [
      Content(
        role="user",
          parts=[
              Part.from_text("What is the weather like in Boston?"),
          ],
      )
    ],
    generation_config = GenerationConfig(temperature=0),
    tools = [
      Tool(
        function_declarations=[get_weather_func, some_other_function],
      )
    ],
    tool_config=ToolConfig(
        function_calling_config=ToolConfig.FunctionCallingConfig(
            # ANY mode forces the model to predict only function calls
            mode=ToolConfig.FunctionCallingConfig.Mode.ANY,
            # Allowed function calls to predict when the mode is ANY. If empty, any of
            # the provided function calls will be predicted.
            allowed_function_names=["get_weather"],
        )
    )
)
Function schema examples
Function declarations are compatible with the OpenAPI schema. We support the following attributes: type, nullable, required, format, description, properties, items, enum, anyOf, $ref, and $defs. Remaining attributes are not supported.

Function schema examples
Function declarations are compatible with the OpenAPI schema. We support the following attributes: type, nullable, required, format, description, properties, items, enum, anyOf, $ref, and $defs. Remaining attributes are not supported.

Function with object and array parameters
The following example uses a Python dictionary to declare a function that takes both object and array parameters:



extract_sale_records_func = FunctionDeclaration(
  name="extract_sale_records",
  description="Extract sale records from a document.",
  parameters={
      "type": "object",
      "properties": {
          "records": {
              "type": "array",
              "description": "A list of sale records",
              "items": {
                  "description": "Data for a sale record",
                  "type": "object",
                  "properties": {
                      "id": {"type": "integer", "description": "The unique id of the sale."},
                      "date": {"type": "string", "description": "Date of the sale, in the format of MMDDYY, e.g., 031023"},
                      "total_amount": {"type": "number", "description": "The total amount of the sale."},
                      "customer_name": {"type": "string", "description": "The name of the customer, including first name and last name."},
                      "customer_contact": {"type": "string", "description": "The phone number of the customer, e.g., 650-123-4567."},
                  },
                  "required": ["id", "date", "total_amount"],
              },
          },
      },
      "required": ["records"],
  },
)
  
Function with enum parameter
The following example uses a Python dictionary to declare a function that takes an integer enum parameter:



set_status_func = FunctionDeclaration(
  name="set_status",
  description="set a ticket's status field",
  # Function parameters are specified in JSON schema format
  parameters={
      "type": "object",
      "properties": {
        "status": {
          "type": "integer",
          "enum": [ "10", "20", "30" ],   # Provide integer (or any other type) values as strings.
        }
      },
  },
)
  
Function with ref and def
The following JSON function declaration uses the ref and defs attributes:



{
  "contents": ...,
  "tools": [
    {
      "function_declarations": [
        {
          "name": "get_customer",
          "description": "Search for a customer by name",
          "parameters": {
            "type": "object",
            "properties": {
              "first_name": { "ref": "#/defs/name" },
              "last_name": { "ref": "#/defs/name" }
            },
            "defs": {
              "name": { "type": "string" }
            }
          }
        }
      ]
    }
  ]
}
  
Usage notes:

Unlike, the OpenAPI schema, specify ref and defs without the $ symbol.
ref must refer to direct child of defs; no external references.
The maximum depth of nested schema is 32.
Recursion depth in defs (self-reference) is limited to two.
from_func with array parameter
The following code sample declares a function that multiplies an array of numbers and uses from_func to generate the FunctionDeclaration schema.



from typing import List

# Define a function. Could be a local function or you can import the requests library to call an API
def multiply_numbers(numbers: List[int] = [1, 1]) -> int:
  """
  Calculates the product of all numbers in an array.

  Args:
      numbers: An array of numbers to be multiplied.

  Returns:
      The product of all the numbers. If the array is empty, returns 1.
  """

  if not numbers:  # Handle empty array
      return 1

  product = 1
  for num in numbers:
      product *= num

  return product

multiply_number_func = FunctionDeclaration.from_func(multiply_numbers)

"""
multiply_number_func contains the following schema:

{'name': 'multiply_numbers',
  'description': 'Calculates the product of all numbers in an array.',
  'parameters': {'properties': {'numbers': {'items': {'type': 'INTEGER'},
    'description': 'list of numbers',
    'default': [1.0, 1.0],
    'title': 'Numbers',
    'type': 'ARRAY'}},
  'description': 'Calculates the product of all numbers in an array.',
  'title': 'multiply_numbers',
  'property_ordering': ['numbers'],
  'type': 'OBJECT'}}
"""
  
Best practices for function calling
To improve your results when using function calling, follow these best practices:

Write clear and detailed function names, parameter descriptions, and instructions
Function names should start with a letter or an underscore and contain only characters a-z, A-Z, 0-9, underscores, dots or dashes with a maximum length of 64.

Be extremely clear and specific in your function and parameter descriptions. The model relies on these to choose the correct function and provide appropriate arguments. For example, a book_flight_ticket function could have the description book flight tickets after confirming users' specific requirements, such as time, departure, destination, party size and preferred airline

Use strong typed parameters
If the parameter values are from a finite set, add an enum field instead of putting the set of values into the description. If the parameter value is always an integer, set the type to integer rather than number.

Tool Selection
While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. If you have a large total number of tools, consider dynamic tool selection based on conversation context.

If you provide generic, low-level tools (like bash) the model might use the tool more often, but with less accuracy. If you provide a specific, high-level tool (like get_weather), the model will be able to use the tool more accurately, but the tool might not be used as often.

Use system instructions
When using functions with date, time, or location parameters, include the current date, time, or relevant location information (for example, city and country) in the system instruction. This provides the model with the necessary context to process the request accurately, even if the user's prompt lacks details.

Prompt engineering
For best results, prepend the user prompt with the following details:

Additional context for the model-for example, You are a flight API assistant to help with searching flights based on user preferences.
Details or instructions on how and when to use the functions-for example, Don't make assumptions on the departure or destination airports. Always use a future date for the departure or destination time.
Instructions to ask clarifying questions if user queries are ambiguous-for example, Ask clarifying questions if not enough information is available.
Use generation configuration
For the temperature parameter, use 0 or another low value. This instructs the model to generate more confident results and reduces hallucinations.

Use structured output
Function calling can be used together with structured output to let the model always predict function calls or outputs that adheres to a specific schema, so that you receive consistently formatted responses when model does not generate function calls.

Validate the API call
If the model proposes the invocation of a function that would send an order, update a database, or otherwise have significant consequences, validate the function call with the user before executing it.

Use thought signatures
Thought signatures should always be used with function calling for best results.

Pricing
The pricing for function calling is based on the number of characters within the text inputs and outputs. To learn more, see Vertex AI pricing.

Here, text input (prompt) refers to the user prompt for the current conversation turn, the function declarations for the current conversation turn, and the history of the conversation. The history of the conversation includes the queries, the function calls, and the function responses of previous conversation turns. Vertex AI truncates the history of the conversation at 32,000 characters.

Text output (response) refers to the function calls and the text responses for the current conversation turn.

Structured output

bookmark_border
To see an example of structured output, run the "Intro to structured output" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can guarantee that a model's generated output always adheres to a specific schema so that you receive consistently formatted responses. For example, you might have an established data schema that you use for other tasks. If you have the model follow the same schema, you can directly extract data from the model's output without any post-processing.

To specify the structure of a model's output, define a response schema, which works like a blueprint for model responses. When you submit a prompt and include the response schema, the model's response always follows your defined schema.

You can control generated output when using the following models:

Gemini models:

Gemini 3 Pro preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.0 Flash
Gemini 2.0 Flash-Lite
Open models:

DeepSeek R1-0528
Llama 4 Maverick
Llama 4 Scout
Llama 3.3
For Open Models, follow this user guide.

Note: Using structured output on tuned Gemini models can result in decreased model quality.
Example use cases
One use case for applying a response schema is to ensure that a model's response produces valid JSON and conforms to your schema. Generative model outputs can have some degree of variability, so including a response schema ensures that you always receive valid JSON. Consequently, your downstream tasks can reliably expect valid JSON input from generated responses.

Another example is to constrain how a model can respond. For example, you can have a model annotate text with user-defined labels, not with labels that the model produces. This constraint is useful when you expect a specific set of labels such as positive or negative and don't want to receive a mixture of other labels that the model might generate like good, positive, negative, or bad.

Considerations
The following considerations discuss potential limitations if you plan on using a response schema:

You must use the API to define and use a response schema. There's no console support.
The size of your response schema counts towards the input token limit.
Only certain output formats are supported, such as application/json or text/x.enum. For more information, see the responseMimeType parameter in the Gemini API reference.
Structured output supports a subset of the Vertex AI schema reference. For more information, see Supported schema fields.
A complex schema can result in an InvalidArgument: 400 error. Complexity might come from long property names, long array length limits, enums with many values, objects with lots of optional properties, or a combination of these factors.

If you get this error with a valid schema, make one or more of the following changes to resolve the error:

Shorten property names or enum names.
Flatten nested arrays.
Reduce the number of properties with constraints, such as numbers with minimum and maximum limits.
Reduce the number of properties with complex constraints, such as properties with complex formats like date-time.
Reduce the number of optional properties.
Reduce the number of valid values for enums.
Supported schema fields
You can specify a response_schema that describes the output format.

The model will then generate a response that matches the provided schema. When using structured outputs, the model will produce outputs in the same order as the keys in the schema.

The following fields from the Vertex AI schema are supported. If you use an unsupported field, Vertex AI can still handle your request but ignores the field.

anyOf
enum: only string enums are supported
format
items
maximum
maxItems
minimum
minItems
nullable
properties
propertyOrdering*
required

* propertyOrdering is specifically for structured output and not part of the Vertex AI schema. This field defines the order in which properties are generated. The listed properties must be unique and must be valid keys in the properties dictionary.

When you define a schema, the model doesn't strictly follow the order of properties that you define in the properties field. To enforce a specific order for property generation, use the propertyOrdering field. Properties listed in propertyOrdering are generated first, in the specified order, followed by any other properties.

Important: If there are any descriptions (for example, in a bulleted list), schemas, or examples (for example, for few-shot learning or RAG) in the prompt, they must present the same property ordering as is specified in the responseSchema. A mismatch in ordering can confuse the model and lead to incorrect or malformed output.
If you use the Python SDK, the default property ordering follows the order that is defined in your schema. For all other cases, properties are generated alphabetically with the required properties grouped first followed by optional properties.

For the format field, Vertex AI supports the following values: date, date-time, duration, and time. The description and format of each value is described in the OpenAPI Initiative Registry

Before you begin
Define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field. Use only the supported fields as listed in the Considerations section. All other fields are ignored.

Include your response schema as part of the responseSchema field only. Don't duplicate the schema in your input prompt. If you do, the generated output might be lower in quality.

For sample schemas, see the Example schemas and model responses section.

Model behavior and response schema
When a model generates a response, it uses the field name and context from your prompt. As such, we recommend that you use a clear structure and unambiguous field names so that your intent is clear.

By default, fields are optional, meaning the model can populate the fields or skip them. You can set fields as required to force the model to provide a value. If there's insufficient context in the associated input prompt, the model generates responses mainly based on the data it was trained on.

If you aren't seeing the results you expect, add more context to your input prompts or revise your response schema. For example, review the model's response without structured output to see how the model responds. You can then update your response schema that better fits the model's output.

Send a prompt with a response schema
To see an example of a response schema and structured output, run the "Introduction to structured output" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

By default, all fields are optional, meaning a model might generate a response to a field. To force the model to always generate a response to a field, set the field as required.

Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateResponseSchema(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const prompt = 'List a few popular cookie recipes.';

  const responseSchema = {
    type: 'ARRAY',
    items: {
      type: 'OBJECT',
      properties: {
        recipeName: {type: 'STRING'},
        ingredients: {
          type: 'ARRAY',
          items: {type: 'STRING'},
        },
      },
      required: ['recipeName', 'ingredients'],
    },
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      responseMimeType: 'application/json',
      responseSchema: responseSchema,
    },
  });

  console.log(response.text);

  // Example output:
  // [
  //     {
  //         "ingredients": [
  //             "2 1/4 cups all-purpose flour",
  //             "1 teaspoon baking soda",
  //             "1 teaspoon salt",
  //             "1 cup (2 sticks) unsalted butter, softened",
  //             "3/4 cup granulated sugar",
  //             "3/4 cup packed brown sugar",
  //             "1 teaspoon vanilla extract",
  //             "2 large eggs",
  //             "2 cups chocolate chips",
  //         ],
  //         "recipe_name": "Chocolate Chip Cookies",
  //     }
  // ]

  return response.text;
}
Example schemas for JSON output
The following sections demonstrate a variety of sample prompts and response schemas. A sample model response is also included after each code sample.

Forecast the weather for each day of the week in an array
Classify a product with a well-defined enum
Forecast the weather for each day of the week
The following example outputs a forecast object for each day of the week that includes an array of properties such as the expected temperature and humidity level for the day. Some properties are set to nullable so the model can return a null value when it doesn't have enough context to generate a meaningful response. This strategy helps reduce hallucinations.


Python
Go
Node.js
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateNullableSchema(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const prompt = `
  The week ahead brings a mix of weather conditions.
  Sunday is expected to be sunny with a temperature of 77°F and a humidity level of 50%. Winds will be light at around 10 km/h.
  Monday will see partly cloudy skies with a slightly cooler temperature of 72°F and the winds will pick up slightly to around 15 km/h.
  Tuesday brings rain showers, with temperatures dropping to 64°F and humidity rising to 70%.
  Wednesday may see thunderstorms, with a temperature of 68°F.
  Thursday will be cloudy with a temperature of 66°F and moderate humidity at 60%.
  Friday returns to partly cloudy conditions, with a temperature of 73°F and the Winds will be light at 12 km/h.
  Finally, Saturday rounds off the week with sunny skies, a temperature of 80°F, and a humidity level of 40%. Winds will be gentle at 8 km/h.
`;

  const responseSchema = {
    type: 'object',
    properties: {
      forecast: {
        type: 'array',
        items: {
          type: 'object',
          properties: {
            Day: {type: 'string', nullable: true},
            Forecast: {type: 'string', nullable: true},
            Temperature: {type: 'integer', nullable: true},
            Humidity: {type: 'string', nullable: true},
            WindSpeed: {type: 'integer', nullable: true},
          },
          required: ['Day', 'Temperature', 'Forecast', 'WindSpeed'],
        },
      },
    },
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: prompt,
    config: {
      responseMimeType: 'application/json',
      responseSchema: responseSchema,
    },
  });
  console.log(response.text);

  // Example output:
  //  {"forecast": [{"Day": "Sunday", "Forecast": "sunny", "Temperature": 77, "Wind Speed": 10, "Humidity": "50%"},
  //   {"Day": "Monday", "Forecast": "partly cloudy", "Temperature": 72, "Wind Speed": 15},
  //   {"Day": "Tuesday", "Forecast": "rain showers", "Temperature": 64, "Wind Speed": null, "Humidity": "70%"},
  //   {"Day": "Wednesday", "Forecast": "thunderstorms", "Temperature": 68, "Wind Speed": null},
  //   {"Day": "Thursday", "Forecast": "cloudy", "Temperature": 66, "Wind Speed": null, "Humidity": "60%"},
  //   {"Day": "Friday", "Forecast": "partly cloudy", "Temperature": 73, "Wind Speed": 12},
  //   {"Day": "Saturday", "Forecast": "sunny", "Temperature": 80, "Wind Speed": 8, "Humidity": "40%"}]}

  return response.text;
}
Classify a product
The following example includes enums where the model must classify an object's type and condition from a list of given values.


Python
Go
Node.js
Java
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI, Type} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateContent(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const responseSchema = {
    type: Type.STRING,
    enum: ['Percussion', 'String', 'Woodwind', 'Brass', 'Keyboard'],
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: 'What type of instrument is an oboe?',
    config: {
      responseMimeType: 'text/x.enum',
      responseSchema: responseSchema,
    },
  });

  console.log(response.text);
  // Example output:
  //  Woodwind
  return response.text;
}


Content generation parameters

bookmark_border
This page shows the optional sampling parameters you can set in a request to a model. The parameters available for each model may differ. For more information, see the reference documentation.

Token sampling parameters
The parameters in this section influence how the model selects the next token from its vocabulary. By adjusting these parameters, you can control the randomness and diversity of the generated text.

Top-P
Top-P changes how the model selects tokens for output. Tokens are selected from the most probable to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is 0.5, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.

Specify a lower value for less random responses and a higher value for more random responses.

For more information, see topP.
Temperature
The temperature is used for sampling during response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.

If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature. If the model enters infinite generation, increasing the temperature to at least 0.1 may lead to improved results.

1.0 is the recommended starting value for temperature.
Lower temperatures lead to predictable (but not completely deterministic) results. For more information, see temperature.

Stopping parameters
The parameters in this section allow you to precisely control the length and content of the model's generated output by defining conditions under which the generation process should stop.

Maximum output tokens
Set maxOutputTokens to limit the number of tokens generated in the response. A token is approximately four characters, so 100 tokens correspond to roughly 60-80 words. Set a low value to limit the length of the response.

Stop sequences
Define strings in stopSequences to tell the model to stop generating text if one of the strings is encountered in the response. If a string appears multiple times in the response, then the response is truncated where the string is first encountered. The strings are case-sensitive.

Token penalization parameters
The parameters in this section allow you to control the likelihood of tokens being generated based on their frequency and presence in the output.

Frequency penalty
Positive values penalize tokens that repeatedly appear in the generated text, decreasing the probability of repeating content. The minimum value is -2.0. The maximum value is up to, but not including, 2.0. For more information, see frequencyPenalty.

Presence penalty
Positive values penalize tokens that already appear in the generated text, increasing the probability of generating more diverse content. The minimum value is -2.0. The maximum value is up to, but not including, 2.0. For more information, see presencePenalty.

Advanced parameters
Use these parameters to return more information about the tokens in the response or to control the variability of the response.

Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

Log probabilities of output tokens
Returns the log probabilities of the top candidate tokens at each generation step. The model's chosen token might not be the same as the top candidate token at each step. Specify the number of candidates to return by using an integer value in the range of 1-20. For more information, see logprobs. You also need to set the responseLogprobs parameter to true to use this feature.

The responseLogprobs parameter returns the log probabilities of the tokens that were chosen by the model at each step.

For more information, see the Intro to Logprobs notebook.

Seed
When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used. For more information, see seed.

Example
Here is an example that uses parameters to tune a model's response.

Python
Go
Node.js
Java
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateContent(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const config = {
    temperature: 0,
    candidateCount: 1,
    responseMimeType: 'application/json',
    topP: 0.95,
    topK: 20,
    seed: 5,
    maxOutputTokens: 500,
    stopSequences: ['STOP!'],
    presencePenalty: 0.0,
    frequencyPenalty: 0.0,
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: 'Why is the sky blue?',
    config: config,
  });

  console.log(response.text);

  // Example response:
  // {
  //   "explanation": "The sky appears blue due to a phenomenon called Rayleigh scattering. When ...
  // }

  return response.text;
}
URL context

bookmark_border
To see an example of URL context, run the "Intro to URL Context" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can use the URL context tool to provide Gemini with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.

This tool is useful for tasks like the following:

Extracting key data points or talking points from articles
Comparing information across multiple links
Synthesizing data from several sources
Answering questions based on the content of a specific page or pages
Analyzing content for specific purposes (like writing a job description or creating test questions)
Note that the index used to pull data may not necessarily be up to date, so some information may be stale.

This guide explains how to use the URL context tool in the Gemini API in Vertex AI.

Supported models
The following models provide support for URL Context:

Gemini 3 Pro preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.0 Flash
Use URL context
You can use the URL context tool in two main ways, by itself or in conjunction with Grounding with Google Search.

URL context only
You can provide specific URLs that you want the model to analyze directly in your prompt:



Summarize this document: YOUR_URLs

Extract the key features from the product description on this page: YOUR_URLs
Python
Javascript
REST


# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True


import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({
  vertexai: true,
  project: process.env.GOOGLE_CLOUD_PROJECT,
  location: process.env.GOOGLE_CLOUD_LOCATION,
  apiVersion: 'v1',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Compare recipes from YOUR_URL1 and YOUR_URL2",
    ],
    config: {
      tools: [{urlContext: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
Grounding with Google Search with URL context
You can also enable both URL context and Grounding with Google Search, using prompts with or without URLs. The model may first search for relevant information and then use the URL context tool to read the content of the search results for a more in-depth understanding.

This feature is experimental and available in API version v1beta1.

Example prompts:



Give me a three day event schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.

Recommend 3 books for beginners to read to learn more about the latest YOUR_SUBJECT.
Python
Javascript
REST


# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True


import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({
  vertexai: true,
  project: process.env.GOOGLE_CLOUD_PROJECT,
  location: process.env.GOOGLE_CLOUD_LOCATION,
  apiVersion: 'v1beta1',
});

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.5-flash",
    contents: [
        "Give me a three day event schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.",
    ],
    config: {
      tools: [{urlContext: {}}, {googleSearch: {}}],
    },
  });
  console.log(response.text);
  // To get URLs retrieved for context
  console.log(response.candidates[0].urlContextMetadata)
}

await main();
For more details about Grounding with Google Search, see the overview page.

Web Grounding for Enterprise with URL context
You can enable both URL context and Web Grounding for Enterprise if you have specific compliance needs or are in a regulated industry like health, finance, or the public sector. The web index used in Web Grounding for Enterprise is more limited than the standard Grounding with Google Search index, as it uses a subset of what's available on Google Search.

For more details about Web Grounding for Enterprise, see the Web Grounding for Enterprise page.

Contextual response
The model's response is based on the content it retrieved from the URLs. If the model retrieved content from URLs, the response will include url_context_metadata. Such a response might look something like the following (parts of the response have been omitted for brevity):


{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata":
      {
          "url_metadata":
          [
            {
              "retrieved_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
          ]
        }
    }
  ]
}
For more information about this object, see the UrlContextMetadata API reference.

Live fetch
The URL context tool fetches live versions of web pages to keep your information current.

To efficiently retrieve web page content, the URL context tool uses a two-stage process designed to balance speed, cost, and access to the most current information:

Indexed content retrieval: This is the first stage. When you provide a URL, the tool first attempts to fetch the content from Google's extensive and highly optimized web index. This method provides fast access to a vast collection of crawled web pages.

Live fetch fallback: This is the second stage. If the content for a given URL isn't available in the index (for example, if the page is very new and not yet indexed), the tool automatically performs a live fetch. This fallback mechanism can directly access the URL, and retrieve the latest version of the content in real-time.

Safety checks
The system performs a content moderation check on the URL to confirm that the URL meets safety standards. If the URL that you provided fails this check, you get a url_retrieval_status of URL_RETRIEVAL_STATUS_UNSAFE.

Token count
The input token count includes content retrieved from the URLs that you specify in your prompt. From the model output, you can see the token count for your prompt and tools usage in the usage_metadata object. The following is an example output:



'usage_metadata': {
  'candidates_token_count': 45,
  'prompt_token_count': 27,
  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 27}],
  'thoughts_token_count': 31,
  'tool_use_prompt_token_count': 10309,
  'tool_use_prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 10309}],
  'total_token_count': 10412
  }
Price per token depends on the model that you use. For more information, see Cost of building and deploying AI models in Vertex AI.

Supported and unsupported content types
The URL context tool can extract content from URLs with the following content types:

Content	Type
Text	text/html
application/json
text/plain
text/xml
text/css
text/javascript
text/csv
text/rtf
Image	image/png
image/jpeg
image/bmp
image/webp
PDF	application/pdf
The URL context tool doesn't support the following content types:

Paywalled content
YouTube videos (For more information, see Video understanding.)
Google Workspace files like Google Docs or Google Sheets
Video and audio files

Thinking models are trained to generate the "thinking process" the model goes through as part of its response. As a result, thinking models are capable of stronger reasoning capabilities in its responses than equivalent base models.

The thinking process is enabled by default. When you use Vertex AI Studio, you can view the full thinking process together with the model's generated response.

Supported models
Thinking is supported in the following models:

Gemini 3 Pro preview
Gemini 3 Pro Image preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Use a thinking model
Note: Fine-tuning isn't supported when thinking is enabled.
To use thinking with a supported model, do the following:

Console
Python
Go
Install


pip install --upgrade google-genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



from google import genai

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="solve x^2 + 4x + 4 = 0",
)
print(response.text)
# Example Response:
#     Okay, let's solve the quadratic equation x² + 4x + 4 = 0.
#
#     We can solve this equation by factoring, using the quadratic formula, or by recognizing it as a perfect square trinomial.
#
#     **Method 1: Factoring**
#
#     1.  We need two numbers that multiply to the constant term (4) and add up to the coefficient of the x term (4).
#     2.  The numbers 2 and 2 satisfy these conditions: 2 * 2 = 4 and 2 + 2 = 4.
#     3.  So, we can factor the quadratic as:
#         (x + 2)(x + 2) = 0
#         or
#         (x + 2)² = 0
#     4.  For the product to be zero, the factor must be zero:
#         x + 2 = 0
#     5.  Solve for x:
#         x = -2
#
#     **Method 2: Quadratic Formula**
#
#     The quadratic formula for an equation ax² + bx + c = 0 is:
#     x = [-b ± sqrt(b² - 4ac)] / (2a)
#
#     1.  In our equation x² + 4x + 4 = 0, we have a=1, b=4, and c=4.
#     2.  Substitute these values into the formula:
#         x = [-4 ± sqrt(4² - 4 * 1 * 4)] / (2 * 1)
#         x = [-4 ± sqrt(16 - 16)] / 2
#         x = [-4 ± sqrt(0)] / 2
#         x = [-4 ± 0] / 2
#         x = -4 / 2
#         x = -2
#
#     **Method 3: Perfect Square Trinomial**
#
#     1.  Notice that the expression x² + 4x + 4 fits the pattern of a perfect square trinomial: a² + 2ab + b², where a=x and b=2.
#     2.  We can rewrite the equation as:
#         (x + 2)² = 0
#     3.  Take the square root of both sides:
#         x + 2 = 0
#     4.  Solve for x:
#         x = -2
#
#     All methods lead to the same solution.
#
#     **Answer:**
#     The solution to the equation x² + 4x + 4 = 0 is x = -2. This is a repeated root (or a root with multiplicity 2).
Control model thinking
You can control the amount of thinking the model performs before returning a response. The method for controlling thinking differs depending on the model version.

Gemini 3 and later models
Gemini 3 models introduce the thinking_level parameter, which simplifies thinking budget configuration into levels. By default, Gemini 3 Pro uses dynamic thinking to reason through prompts. For faster, lower-latency responses when complex reasoning isn't required, you can constrain the model's thinking_level.

LOW: Constrains the model to use fewer tokens for thinking and is suitable for simpler tasks where extensive reasoning is not required. LOW is ideal for high-throughput tasks where speed is essential:



from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="How does AI work?",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(
            thinking_level=types.ThinkingLevel.LOW
        )
    ),
)
print(response.text)
HIGH: Allows the model to use more tokens for thinking and is suitable for complex prompts requiring deep reasoning, such as multi-step planning, verified code generation, or advanced function calling scenarios. This is the default level for Gemini 3 Pro. Use this configuration when replacing tasks you might have previously relied on specialized reasoning models for:



from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
    model="gemini-3-pro-preview",
    contents="Find the race condition in this multi-threaded C++ snippet: [code here]",
    config=types.GenerateContentConfig(
        thinking_config=types.ThinkingConfig(
            thinking_level=types.ThinkingLevel.HIGH
        )
    ),
)
print(response.text)
Thinking cannot be turned off for Gemini 3 Pro.

If you specify both thinking_level and thinking_budget in the same request for a Gemini 3 model, the model returns an error.

Gemini 2.5 and earlier models
For models earlier than Gemini 3, you can control thinking using the thinking_budget parameter, which sets an upper limit on the number of tokens the model can use for its thought process. By default, if thinking_budget is not set, the model automatically controls how much it thinks up to a maximum of 8,192 tokens. To use dynamic budget through the API, set thinking_budget to -1.

You can manually set thinking_budget to a specific token limit in situations where you might need more or less thinking than the default. Set a lower token limit for less complex tasks, or a higher limit for more complex ones. If latency is more important, use a lower budget or disable thinking by setting the budget to 0.

The following table shows the minimum and maximum amounts you can set the thinking_budget to for each supported model:

Model	Minimum token amount	Maximum token amount
Gemini 2.5 Flash	1	24,576
Gemini 2.5 Pro	128	32,768
Gemini 2.5 Flash-Lite	512	24,576
If you set thinking_budget to 0 when using Gemini 2.5 Flash and Gemini 2.5 Flash-Lite, thinking is turned off. Thinking can't be turned off for Gemini 2.5 Pro.

If you use the thinking_level parameter with a model earlier than Gemini 3, the model returns an error.

Console
Python
Node.js
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateWithThoughts(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: 'solve x^2 + 4x + 4 = 0',
    config: {
      thinkingConfig: {
        thinkingBudget: 1024,
      },
    },
  });

  console.log(response.text);
  // Example response:
  //  To solve the equation $x^2 + 4x + 4 = 0$, you can use several methods:
  //  **Method 1: Factoring**
  //  1.  Look for two numbers that multiply to the constant term (4) and add up to the coefficient of the $x$ term (4).
  //  2.  The numbers are 2 and 2 ($2 \times 2 = 4$ and $2 + 2 = 4$).
  //  ...
  //  ...
  //  All three methods yield the same solution. This quadratic equation has exactly one distinct solution (a repeated root).
  //  The solution is **x = -2**.

  // Token count for `Thinking`
  console.log(response.usageMetadata.thoughtsTokenCount);
  // Example response:
  //  886

  // Total token count
  console.log(response.usageMetadata.totalTokenCount);
  // Example response:
  //  1525
  return response.text;
}
View thought summaries
Experimental

This feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

Thought summaries are the abbreviated output of the thinking process that the model went through when generating its response. You can view thought summaries in both Gemini 2.5 Flash and Gemini 2.5 Pro. To view thought summaries, do the following:

Console
Python
Node.js
Install


pip install --upgrade google-genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



from google import genai
from google.genai.types import GenerateContentConfig, ThinkingConfig

client = genai.Client()
response = client.models.generate_content(
    model="gemini-2.5-pro",
    contents="solve x^2 + 4x + 4 = 0",
    config=GenerateContentConfig(
        thinking_config=ThinkingConfig(include_thoughts=True)
    ),
)

print(response.text)
# Example Response:
#     Okay, let's solve the quadratic equation x² + 4x + 4 = 0.
#     ...
#     **Answer:**
#     The solution to the equation x² + 4x + 4 = 0 is x = -2. This is a repeated root (or a root with multiplicity 2).

for part in response.candidates[0].content.parts:
    if part and part.thought:  # show thoughts
        print(part.text)
# Example Response:
#     **My Thought Process for Solving the Quadratic Equation**
#
#     Alright, let's break down this quadratic, x² + 4x + 4 = 0. First things first:
#     it's a quadratic; the x² term gives it away, and we know the general form is
#     ax² + bx + c = 0.
#
#     So, let's identify the coefficients: a = 1, b = 4, and c = 4. Now, what's the
#     most efficient path to the solution? My gut tells me to try factoring; it's
#     often the fastest route if it works. If that fails, I'll default to the quadratic
#     formula, which is foolproof. Completing the square? It's good for deriving the
#     formula or when factoring is difficult, but not usually my first choice for
#     direct solving, but it can't hurt to keep it as an option.
#
#     Factoring, then. I need to find two numbers that multiply to 'c' (4) and add
#     up to 'b' (4). Let's see... 1 and 4 don't work (add up to 5). 2 and 2? Bingo!
#     They multiply to 4 and add up to 4. This means I can rewrite the equation as
#     (x + 2)(x + 2) = 0, or more concisely, (x + 2)² = 0. Solving for x is now
#     trivial: x + 2 = 0, thus x = -2.
#
#     Okay, just to be absolutely certain, I'll run the quadratic formula just to
#     double-check. x = [-b ± √(b² - 4ac)] / 2a. Plugging in the values, x = [-4 ±
#     √(4² - 4 * 1 * 4)] / (2 * 1). That simplifies to x = [-4 ± √0] / 2. So, x =
#     -2 again – a repeated root. Nice.
#
#     Now, let's check via completing the square. Starting from the same equation,
#     (x² + 4x) = -4. Take half of the b-value (4/2 = 2), square it (2² = 4), and
#     add it to both sides, so x² + 4x + 4 = -4 + 4. Which simplifies into (x + 2)²
#     = 0. The square root on both sides gives us x + 2 = 0, therefore x = -2, as
#      expected.
#
#     Always, *always* confirm! Let's substitute x = -2 back into the original
#     equation: (-2)² + 4(-2) + 4 = 0. That's 4 - 8 + 4 = 0. It checks out.
#
#     Conclusion: the solution is x = -2. Confirmed.
Thought signatures
Thought signatures are encrypted representations of the model's internal thought process that preserve the Gemini reasoning state during multi-turn conversations, especially when using function calling.

To ensure the model maintains full context across multiple turns of a conversation, you must return the thought signatures from previous responses in your subsequent requests. If you are using the official Google Gen AI SDK (Python, Node.js, Go, or Java) and using the standard chat history features or appending the full model response to the history, thought signatures are handled automatically.

For more information, see Thought signatures.

Prompting techniques
Effective prompting is crucial for maximizing the benefits of the Gemini thinking models. This section outlines some prompting practices for thinking models.

Begin with general instructions that describe the preferred outcome. Start with a low-complexity prompt and observe the model's initial thoughts on how to respond.

Prompt:

Describe the most unusual or striking architectural feature you've ever seen in a building or structure.
Response:

<...>
The most unusual architectural feature that has truly stayed with me is the "floating roof" of the
Heydar Aliyev Center in Baku, Azerbaijan, designed by Zaha Hadid Architects.

Visually, it's breathtaking. Imagine a vast, undulating landscape of pure white, seemingly draped
over the building. It's not just a roof in the traditional sense; it's an extension of the ground,
flowing seamlessly upwards and over the building's various spaces before curving back down to meet
the earth again. There are no sharp angles, no rigid lines. Instead, everything is fluid, organic,
and continuous. It resembles a giant, pristine white wave frozen in motion, or perhaps a sand dune
sculpted by an unseen wind.
<...>
(gemini-2.5-pro-exp-03-25)
Provide step-by-step instructions
If the initial attempt does not yield the expected result, refine your prompt by providing more explicit step-by-step instructions to guide Gemini through the problem-solving process for a more focused approach. This allows you steer the model's initial understanding of the task with explicit suggestions and guidance.

Prompt:

Now imagine that architectural feature is not just for looks, but has a specific function or
purpose related to the building's environment or the people who use it. Explain what that
function could be and how it enhances the building's overall design and usability.
Response:

<...>
Let's imagine that the "floating roof" of the Heydar Aliyev Center is not purely aesthetic but
serves a crucial environmental and user-centric function: it's designed as a sophisticated climate
moderator and natural ventilation system for the Baku environment.

Here's how this function would work and enhance the building:

Function: Passive Climate Control and Natural Ventilation System

Shading and Solar Heat Mitigation: The undulating curves of the white GFRC roof are strategically
shaped and oriented to provide optimal shading throughout the day, year-round. Baku experiences
intense sun, especially during the summer. The overhanging curves, acting like giant brise-soleils,
would block direct sunlight from hitting the building's glass facade, significantly reducing solar
heat gain. The varying heights and depths of the curves would create dynamic shade patterns, ensuring
that different parts of the building are shaded at different times of the day, optimizing comfort
and reducing the need for excessive air conditioning. The white color of the GFRC further enhances
this by reflecting a large portion of the solar radiation.
<...>
(gemini-2.5-pro-exp-03-25)
Multishot prompting with thinking
Multishot prompting in conjunction with thinking can further enhance Gemini's understanding and improve the accuracy of its responses. Provide several examples of input-output pairs, demonstrating the preferred behavior and output format in your prompt.

Prompt:

Example 1:
User: What is the tallest mountain in the world?
Assistant: Mount Everest

Example 2:
User: What is the largest ocean?
Assistant: Pacific Ocean

User: What is the longest river in the world?
Assistant:
Response:

Amazon River
(gemini-2.5-pro-exp-03-25)
Defining output and behavior
When building an application where users will interact directly with the model, you may want to provide guidance for how Gemini's output and responses will sound and be formatted.

System instructions
System instructions are a set of instructions that the model processes before it processes prompts. They are recalled each time the model is prompted and tell the model how you want it to behave and respond. For example, you can include things like the role or persona, contextual information, and formatting instructions.

Prompt:

System Instruction: You are a helpful and concise coding tutor. Always provide code examples and explain them briefly.

User Prompt: Explain what is a closure in Javascript and give an example.
Response:

function outerFunction(outerVar) {
function innerFunction(innerVar) {
console.log(outerVar); // Accessing outerVar from outerFunction's scope
console.log(innerVar);
}
return innerFunction;
}

const closureExample = outerFunction('I am from outer');
closureExample('I am from inner');

Explanation:

A closure is created when a function (innerFunction in this example) is defined inside another function
(outerFunction) and the inner function remembers and has access to the variables from the outer
function's scope, even after the outer function has finished executing.

In the example:

outerFunction is called with 'I am from outer' and returns innerFunction.

closureExample now holds a reference to innerFunction.

When closureExample('I am from inner') is called, innerFunction executes.

Even though outerFunction has already finished, innerFunction can still access and use outerVar
(which is 'I am from outer') because of the closure.

In essence, the innerFunction "closes over" the variables of its outer function's scope, creating
a closure.
(gemini-2.5-pro-exp-03-25)
Verification and reflection
Include specific verification steps in either the system instructions or your prompts directly. For example, ask Gemini to verify its sources, review its reasoning, identify potential errors, and check its final answer.

Prompt:

Verify the following statement and provide your sources: The capital of Brazil is Rio de Janeiro.
Response:

The statement "The capital of Brazil is Rio de Janeiro" is incorrect.

The current capital of Brazil is Brasília.

Rio de Janeiro was the capital of Brazil for a significant period, from 1763 to 1960. However, the
capital was moved to Brasília in 1960 as part of a plan to develop the country's interior and create
a more centrally located capital.

Verification and Sources:

You can easily verify this information from numerous reliable sources:

Encyclopedias:

Britannica: Searching for "Brazil" on Britannica will clearly state Brasília as the capital.

https://www.britannica.com/place/Brazil (Look for the "Capital" information in the introductory summary or "Geography" section)

Wikipedia: The Wikipedia article for "Brazil" also states Brasília as the capital.

https://en.wikipedia.org/wiki/Brazil (Look in the infobox or introductory section)

Government and Official Sources:

CIA World Factbook: This is a highly reliable source for country information. Searching for "Brazil"
will list Brasília as the capital.

https://www.cia.gov/the-world-factbook/countries/brazil/ (Look under "Government" > "Capital")

Official Website of the Brazilian Government (if accessible and in English): While navigating official
government websites can sometimes be complex, they would definitively list Brasília as the capital.

Geographic Resources:

National Geographic: Their website and publications will confirm Brasília as the capital of Brazil.

Atlases and Geography Textbooks: Any modern atlas or geography textbook will list Brasília as the capital.

Why the confusion?

The confusion might arise because:

Rio de Janeiro is a very famous Brazilian city: It's a major tourist destination, known for its
beaches, Carnival, and iconic landmarks like Christ the Redeemer. Its past status as capital adds
to its historical significance and public recognition.

Historical context: Rio de Janeiro was the capital for a long time. People who learned about Brazil
in the past might remember it as the capital.

In conclusion, the statement is false. The capital of Brazil is Brasília, not Rio de Janeiro. Rio
de Janeiro was the former capital but is no longer the current capital.

You can confidently use the sources listed above to verify this information.
(gemini-2.5-pro-exp-03-25)
Debugging and steering
Vertex AI Studio provides developers access to the model's thoughts in the UI. This is helpful as a developer reviewing your prompt when debugging:

Analyze the thought process
Carefully review Gemini's thought process to identify any areas where it may have gone astray. This analysis can provide valuable clues for refining your prompts and getting your expected response consistently in your application.

Example Scenario: If Gemini's thought process shows it misunderstood a step in a complex instruction, you can refine the prompt to clarify that specific step.

Prompt:

Step 1: Summarize the following article about quantum physics.
Step 2: Translate the summary into Spanish.
Step 3: Extract the key entities from the Spanish summary.

Article: ... [Article Text] ...
Gemini's Thought Process Analysis (Example):

The user wants me to process an article about Google's new quantum chip called Willow in three steps:
Step 1: Summarize the article.
Step 2: Translate the summary into Spanish.
Step 3: Extract the key entities from the Spanish summary.

Let's break down each step.

Step 1: Summarize the article.
I need to read the article and identify the main points. The article is about Google's new quantum
chip, Willow, and its achievements. The key achievements are:

Exponential error reduction as qubits scale up.

Outperforming supercomputers in a benchmark computation (RCS).

Willow is a step towards commercially relevant quantum computing.

It's the first system below threshold for quantum error correction.

It achieved a benchmark computation in under 5 minutes that would take a supercomputer 10 septillion years.

It was fabricated in a new facility.

It has state-of-the-art performance and high quality qubits.

Google is looking towards real-world applications with Willow.

Quantum computing will benefit AI.

I will synthesize these points into a concise summary.

Step 2: Translate the summary into Spanish.
Once I have the summary in English, I will translate it into Spanish. I need to ensure the translation
is accurate and natural-sounding in Spanish.

Step 3: Extract the key entities from the Spanish summary.
After translating the summary, I will identify and extract the key entities. Key entities are typically
nouns or noun phrases that represent important objects, concepts, or actors. In this context, they
will likely include:

Names of chips (Willow, Sycamore)

Companies/Organizations (Google Quantum AI, Google Research)

Technologies (quantum chip, quantum computer, qubits, quantum error correction, superconducting
quantum system, classical computers, supercomputers, AI)

Benchmarks (random circuit sampling, RCS)

People (Julian Kelly, Peter Shor, David Deutsch, Sergio Boixo, Hartmut Neven, John Preskill)

Concepts (exponential error reduction, below threshold, beyond breakeven, state-of-the-art performance,
commercially relevant applications, quantum mechanics, multiverse, T1 times, system benchmarks,
algorithmic benchmarks)

I will need to carefully review the Spanish summary to identify these entities and list them out.

Provide corrective feedback
If you observe errors or inconsistencies in Gemini's reasoning, provide corrective feedback in your prompt and the system instructions to anticipate where the model's response may deviate from your desired outcome.

Prompt:

You are an expert in botany. When asked to identify a plant, focus on leaf shape, flower structure, and stem characteristics.

Identify this plant based on these characteristics: ... [Plant Characteristics Description] ...

Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.
Refined Prompt (if needed based on thought process):

Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.
Response:

Tabebuia vellosoi
(gemini-2.5-pro-exp-03-25)
Making the best of long outputs and long-form thinking
Thinking generates "thoughts" as part of the Token Output that showcases Gemini's detailed reasoning. To make the most of these outputs:

Review the reasoning
Pay close attention to Gemini's reasoning process (available in the Vertex AI Studio UI), how it broke down the task and the choices it made for your output. This can help you understand how it arrived at its conclusion.

Provide guidance in reasoning
If you want your output to be particularly lengthy, you may want to provide more guidance in your prompt to constrain the amount of thinking needed to accomplish the task presented in the prompt. This lets you reserve more of the token output for your response.

*Example: For a detailed report, you might specify sections and subsections in your prompt to guide Gemini's thinking and structure the output efficiently.

Prompt:

Generate a detailed report on the impact of climate change on coastal ecosystems.

Report Structure:

Introduction

Rising Sea Levels
2.1. Impact on Mangroves
2.2. Impact on Coral Reefs

Ocean Acidification
3.1. Impact on Shellfish
3.2. Impact on Plankton

Extreme Weather Events
4.1. Increased Storm Intensity
4.2. Coastal Erosion

Conclusion

Provide detailed analysis within each section.
Response:

<...>
Climate Change Impact on Coastal Ecosystems: A Detailed Report

1. Introduction

Coastal ecosystems, the dynamic interface between land and sea, are among the most productive and
biodiverse environments on Earth. They provide crucial ecosystem services, including coastal protection,
fisheries, carbon sequestration, and support for a vast array of species. However, these vital
ecosystems are increasingly threatened by the multifaceted impacts of climate change. Driven by
anthropogenic greenhouse gas emissions, climate change is manifesting in several interconnected ways
that are profoundly altering the physical and chemical conditions of coastal environments. This report
will delve into the key impacts of climate change on coastal ecosystems, focusing on rising sea levels,
ocean acidification, and extreme weather events, and their specific consequences for critical habitats
like mangroves and coral reefs, and key organisms like shellfish and plankton. Understanding these
impacts is crucial for developing effective conservation and management strategies to mitigate the
damage and enhance the resilience of these invaluable ecosystems.

2. Rising Sea Levels

Rising sea levels are a direct and significant consequence of global warming, driven by thermal
expansion of seawater and the melting of land ice (glaciers and ice sheets). This phenomenon poses a
direct threat to low-lying coastal ecosystems by inundating habitats, altering salinity regimes, and
increasing erosion rates. The rate of sea level rise is accelerating, and projections for the 21st
century indicate a continued upward trend, with potentially devastating consequences for coastal
environments.

2.1. Impact on Mangroves

Mangrove forests are unique coastal ecosystems found in tropical and subtropical intertidal zones.
They provide numerous benefits, including coastal protection against storms, nursery grounds for
fish and invertebrates, and significant carbon sequestration...
<...>

Thought signatures are encrypted representations of the model's internal thought process. Thought signatures preserve the Gemini reasoning state during multi-turn and multi-step conversations, which can be useful when using function calling. Responses can include a thought_signature field within any content part (e.g., text, functionCall).

Gemini 3 Pro enforces stricter validation on thought signatures than previous Gemini versions because they improve model performance for function calling. To ensure the model maintains full context across multiple turns of a conversation, you must return the thought signatures from previous responses in your subsequent requests. If a required thought signature is not returned when using Gemini 3 Pro, the model returns a 400 error.

Gemini 3 Pro Image doesn't enforce this validation. To ensure the model maintains full context across multiple turns of a conversation, you must return the thought signatures from previous responses in your subsequent requests. Gemini 3 Pro Image doesn't return a 400 error if a thought signature isn't returned. For code samples related to multi-turn image editing using Gemini 3 Pro Image, see Example of multi-turn image editing using thought signatures.

If you are using the official Google Gen AI SDK (Python, Node.js, Go, or Java) and using the standard chat history features or appending the full model response to the history, thought signatures are handled automatically.

Why are they important?
When a Thinking model calls an external tool, it pauses its internal reasoning process. The thought signature acts as a "save state," allowing the model to resume its chain of thought seamlessly once you provide the function's result. Without thought signatures, the model "forgets" its specific reasoning steps during the tool execution phase. Passing the signature back ensures:

Context continuity: The model preserves and can check the reasoning steps that justified calling the tool.
Complex reasoning: Enables multi-step tasks where the output of one tool informs the reasoning for the next.
Turns and steps
A multi-turn conversation flow with a large language model (LLM) that uses function calling (FC) and the function responses (FR) to generate a final response. The process is broken down into two turns: Turn 1 consists of three steps. Step 1: User Prompt leads to Model FC1. Step 2: The model receives FR1, which leads to Model FC2. Step 3: The model receives FR2, which leads to the final Model Text output for Turn 1. Turn 2 begins with a new User Prompt, utilizing the full context of Turn 1 to generate the final Model Text output for Turn 2.
Multi-turn conversation flow with function calling and responses.
In the context of function calling, it's important to understand the difference between turns and steps:

A turn represents a complete conversation exchange, starting with a user's prompt and ending when the model provides a final, non-function-call response to that prompt.
A step occurs within a single turn when the model invokes a function and requires a function response to continue its reasoning process. As shown in the diagram, a single turn can involve multiple steps if the model needs to call several functions sequentially to fulfill the user's request.
How to use thought signatures
The simplest way to handle thought signatures is to include all Parts from all previous messages in the conversation history when sending a new request, exactly as they were returned by the model.

If you are not using one of the Google Gen AI SDKs, or you need to modify or trim conversation history, you must ensure that thought signatures are preserved and sent back to the model.

When using the Google Gen AI SDK (recommended)
When using the chat history features of the SDKs or appending the model's content object from the previous response to the contents of the next request, signatures are handled automatically.

The following Python example shows automatic handling:



from google import genai
from google.genai.types import Content, FunctionDeclaration, GenerateContentConfig, Part, ThinkingConfig, Tool

client = genai.Client()

# 1. Define your tool
get_weather_declaration = FunctionDeclaration(
   name="get_weather",
   description="Gets the current weather temperature for a given location.",
   parameters={
       "type": "object",
       "properties": {"location": {"type": "string"}},
       "required": ["location"],
   },
)
get_weather_tool = Tool(function_declarations=[get_weather_declaration])

# 2. Send a message that triggers the tool
prompt = "What's the weather like in London?"
response = client.models.generate_content(
   model="gemini-2.5-flash",
   contents=prompt,
   config=GenerateContentConfig(
       tools=[get_weather_tool],
       thinking_config=ThinkingConfig(include_thoughts=True)
   ),
)

# 3. Handle the function call
function_call = response.function_calls[0]
location = function_call.args["location"]
print(f"Model wants to call: {function_call.name}")

# Execute your tool (for example, call an API)
# (This is a mock response for the example)
print(f"Calling external tool for: {location}")
function_response_data = {
   "location": location,
   "temperature": "30C",
}

# 4. Send the tool's result back
# Append this turn's messages to history for a final response.
# The `content` object automatically attaches the required thought_signature behind the scenes.
history = [
   Content(role="user", parts=[Part(text=prompt)]),
   response.candidates[0].content, # Signature preserved here
   Content(
     role="tool",
     parts=[
         Part.from_function_response(
             name=function_call.name,
             response=function_response_data,
         )
     ],
   )
]

response_2 = client.models.generate_content(
   model="gemini-2.5-flash",
   contents=history,
   config=GenerateContentConfig(
       tools=[get_weather_tool],
       thinking_config=ThinkingConfig(include_thoughts=True)
   ),
)

# 5. Get the final, natural-language answer
print(f"\nFinal model response: {response_2.text}")
When using REST or manual handling
If you are interacting with the API directly, you must implement signature handling based on the following rules for Gemini 3 Pro:

Function calling:
If the model response contains one or more functionCall parts, a thought_signature is required for correct processing.
In cases of parallel function calls in a single response, only the first functionCall part will contain the thought_signature.
In cases of sequential function calls across multiple steps in a turn, each functionCall part will contain a thought_signature.
Rule: When constructing the next request, you must include the part containing the functionCall and its thought_signature exactly as it was returned by the model. For sequential (multi-step) function calling, validation is performed on all steps in the current turn, and omitting a required thought_signature for the first functionCall part in any step of the current turn results in a 400 error. A turn begins with the most recent user message that is not a functionResponse.
If the model returns parallel function calls (for example, FC1+signature, FC2), your response must contain all function calls followed by all function responses (FC1+signature, FC2, FR1, FR2). Interleaving responses (FC1+signature, FR1, FC2, FR2) results in a 400 error.
There are rare cases where you need to provide functionCall parts that were not generated by the API and therefore don't have an associated thought signature (for example, when transferring history from a model that does not include thought signatures). You can set thought_signature to skip_thought_signature_validator, but, this should be a last resort as it will negatively impact model performance.
Non-function calling:
If the model response does not contain a functionCall, it might include a thought_signature in the last part of the response (for example, the last text part).
Rule: Including this signature in the next request is recommended for best performance, but omitting it won't cause an error. When streaming, this signature might be returned in a part with empty text content, so be sure to parse all parts until finish_reason is returned by the model.
Follow these rules to ensure the model's context is preserved:

Always send the thought_signature back to the model inside its original Part.
Don't merge a Part containing a signature with one that does not. This breaks the positional context of the thought.
Don't combine two Parts that both contain signatures, because the signature strings cannot be merged.
Sequential function calling example
The following example shows a multi-step function calling example where the user asks "Check flight status for AA100 and book a taxi if delayed", which requires multiple tasks.

REST
Chat Completions
The following example demonstrates how to handle thought signatures across multiple steps in a sequential function calling workflow using the Chat Completions API.

Turn 1, Step 1 (user request)


{
  "model": "google/gemini-3-pro-preview",
  "messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "check_flight",
        "description": "Gets the current status of a flight",
        "parameters": {
          "type": "object",
          "properties": {
            "flight": {
              "type": "string",
              "description": "The flight number to check."
            }
          },
          "required": [
            "flight"
          ]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "book_taxi",
        "description": "Book a taxi",
        "parameters": {
          "type": "object",
          "properties": {
            "time": {
              "type": "string",
              "description": "time to book the taxi"
            }
          },
          "required": [
            "time"
          ]
        }
      }
    }
  ]
}
Turn 1, Step 1 (model response)


{
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_A>"
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    }
Turn 1, Step 2 (user response - sending tool outputs)
Since this user turn only contains a functionResponse (no fresh text), we are still in Turn 1. You must preserve <SIGNATURE_A>.



"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_A>"
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"
    }
  ]
Turn 1, Step 2 (model response)
The model now decides to book a taxi based on the previous tool output.



{
"role": "model",
"tool_calls": [
{
"extra_content": {
"google": {
"thought_signature": "<SIGNATURE_B>"
}
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-2",
            "type": "function"
          }
       ]
}
Turn 1, Step 3 (user response - sending tool output)
To send the taxi booking confirmation, you must include signatures for all function calls in this loop (<SIGNATURE_A> and <SIGNATURE_B>).



"messages": [
    {
      "role": "user",
      "content": "Check flight status for AA100 and book a taxi 2 hours before if delayed."
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_A>"
              }
            },
            "function": {
              "arguments": "{\"flight\":\"AA100\"}",
              "name": "check_flight"
            },
            "id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "check_flight",
      "tool_call_id": "function-call-1d6a1a61-6f4f-4029-80ce-61586bd86da5",
      "content": "{\"status\":\"delayed\",\"departure_time\":\"12 PM\"}"
    },
    {
      "role": "model",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_B>"
              }
            },
            "function": {
              "arguments": "{\"time\":\"10 AM\"}",
              "name": "book_taxi"
            },
            "id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
            "type": "function"
          }
        ]
    },
    {
      "role": "tool",
      "name": "book_taxi",
      "tool_call_id": "function-call-65b325ba-9b40-4003-9535-8c7137b35634",
      "content": "{\"booking_status\":\"success\"}"
    }
  ]
Parallel function calling example
The following example shows a parallel function calling example where the user asks "Check weather in Paris and London".

REST
Chat Completions
The following example demonstrates how to handle thought signatures in a parallel function calling workflow using the Chat Completions API.

Turn 1, Step 1 (user request)


{
  "contents": [
    {
      "role": "user",
      "parts": [
        {
          "text": "Check the weather in Paris and London."
        }
      ]
    }
  ],
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": "get_current_temperature",
          "description": "Gets the current temperature for a given location.",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city name, e.g. San Francisco"
              }
            },
            "required": [
              "location"
            ]
          }
        }
      ]
    }
  ]
}
Turn 1, Step 1 (model response)


{
"role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_A>"
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": {
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function"
          }
        ]
}
Turn 1, Step 2 (user response - sending tool outputs)
You must preserve <SIGNATURE_A> on the first part exactly as received.



"messages": [
    {
      "role": "user",
      "content": "Check the weather in Paris and London."
    },
    {
      "role": "assistant",
        "tool_calls": [
          {
            "extra_content": {
              "google": {
                "thought_signature": "<SIGNATURE_A>"
              }
            },
            "function": {
              "arguments": "{\"location\":\"Paris\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
            "type": "function"
          },
          {
            "function": {
              "arguments": "{\"location\":\"London\"}",
              "name": "get_current_temperature"
            },
            "id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
            "type": "function"
          }
        ]
    },
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-f3b9ecb3-d55f-4076-98c8-b13e9d1c0e01",
      "content": "{\"temp\":\"15C\"}"
    },
    {
      "role":"tool",
      "name": "get_current_temperature",
      "tool_call_id": "function-call-335673ad-913e-42d1-bbf5-387c8ab80f44",
      "content": "{\"temp\":\"12C\"}"
    }
  ]
Signatures in non-functionCall Parts
Gemini may also return a thought_signature in the final Part of a response, even if no function call is present.

Behavior: The final content Part (text, inlineData, etc.) returned by the model may contain a thought_signature.
Requirement: Returning this signature is recommended to ensure the model maintains high-quality reasoning, especially for complex instruction following or simulated agentic workflows.
Validation: The API does not strictly enforce validation for signatures in non-functionCall parts. You won't receive a blocking error if you omit them, though performance may degrade.
Example model response with signature in text part:
The following examples show a model response where a thought_signature is included in a non-functionCall Part and how to handle it in a subsequent request.

Turn 1, Step 1 (model response)


{
  "role": "model",
  "parts": [
    {
      "text": "I need to calculate the risk. Let me think step-by-step...",
      "thought_signature": "<SIGNATURE_C>" // OPTIONAL (Recommended)
    }
  ]
}
Turn 2, Step 1 (user)


[
  { "role": "user", "parts": [{ "text": "What is the risk?" }] },
  {
    "role": "model",
    "parts": [
      {
        "text": "I need to calculate the risk. Let me think step-by-step...",
        // If you omit <SIGNATURE_C> here, no error will occur.
      }
    ]
  },
  { "role": "user", "parts": [{ "text": "Summarize it." }] }
]
Example of multi-turn image editing using thought signatures
The following samples illustrate how to retrieve and pass thought signatures during multi-turn image creation and editing with Gemini 3 Pro Image.

Turn 1: Get the response and save data that includes thought signatures


chat = client.chats.create(
   model="gemini-3-pro-image-preview",
   config=types.GenerateContentConfig(
       response_modalities=['TEXT', 'IMAGE']
   )
)
message = "Create an image of a clear perfume bottle sitting on a vanity."
response = chat.send_message(message)
data = b''
for part in response.candidates[0].content.parts:
   if part.text:
       display(Markdown(part.text))
   if part.inline_data:
       data = part.inline_data.data
       display(Image(data=data, width=500))
Turn 2: pass the data which includes thought signatures


response = chat.send_message(
   message=[
       types.Part.from_bytes(
           data=data,
           mime_type="image/png",
       ),
       "Make the perfume bottle purple and add a vase of hydrangeas next to the bottle.",
   ],
)
for part in response.candidates[0].content.parts:
   if part.text:
       display(Markdown(part.text))
   if part.inline_data:
       display(Image(data=part.inline_data.data, width=500))

You can use the URL context tool to provide Gemini with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.

This tool is useful for tasks like the following:

Extracting key data points or talking points from articles
Comparing information across multiple links
Synthesizing data from several sources
Answering questions based on the content of a specific page or pages
Analyzing content for specific purposes (like writing a job description or creating test questions)
Note that the index used to pull data may not necessarily be up to date, so some information may be stale.

This guide explains how to use the URL context tool in the Gemini API in Vertex AI.

Supported models
The following models provide support for URL Context:

Gemini 3 Pro preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.0 Flash
Use URL context
You can use the URL context tool in two main ways, by itself or in conjunction with Grounding with Google Search.

URL context only
You can provide specific URLs that you want the model to analyze directly in your prompt:



Summarize this document: YOUR_URLs

Extract the key features from the product description on this page: YOUR_URLs
Python
Javascript
REST


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://aiplatform.googleapis.com/v1beta1/projects/GOOGLE_CLOUD_PROJECT/locations/global/publishers/google/models/gemini-2.5-flash:generateContent \
  -d '{
      "contents": [
          {
              "role": "user",
              "parts": [
                  {"text": "Compare recipes from YOUR_URL1 and YOUR_URL2"}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          }
      ]
  }' > result.json

cat result.json
Grounding with Google Search with URL context
You can also enable both URL context and Grounding with Google Search, using prompts with or without URLs. The model may first search for relevant information and then use the URL context tool to read the content of the search results for a more in-depth understanding.

This feature is experimental and available in API version v1beta1.

Example prompts:



Give me a three day event schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute.

Recommend 3 books for beginners to read to learn more about the latest YOUR_SUBJECT.
Python
Javascript
REST


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://aiplatform.googleapis.com/v1beta1/projects/GOOGLE_CLOUD_PROJECT/locations/global/publishers/google/models/gemini-2.5-flash:generateContent \
  -d '{
      "contents": [
          {
              "role": "user",
              "parts": [
                  {"text": "Give me a three day event schedule based on YOUR_URL. Also let me know what needs to taken care of considering weather and commute."}
              ]
          }
      ],
      "tools": [
          {
              "url_context": {}
          },
          {
              "google_search": {}
          }
      ]
  }' > result.json

cat result.json
For more details about Grounding with Google Search, see the overview page.

Web Grounding for Enterprise with URL context
You can enable both URL context and Web Grounding for Enterprise if you have specific compliance needs or are in a regulated industry like health, finance, or the public sector. The web index used in Web Grounding for Enterprise is more limited than the standard Grounding with Google Search index, as it uses a subset of what's available on Google Search.

For more details about Web Grounding for Enterprise, see the Web Grounding for Enterprise page.

Contextual response
The model's response is based on the content it retrieved from the URLs. If the model retrieved content from URLs, the response will include url_context_metadata. Such a response might look something like the following (parts of the response have been omitted for brevity):


{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "... \n"
          }
        ],
        "role": "model"
      },
      ...
      "url_context_metadata":
      {
          "url_metadata":
          [
            {
              "retrieved_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/code-execution",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
            {
              "retrieved_url": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/grounding-with-google-search",
              "url_retrieval_status": <UrlRetrievalStatus.URL_RETRIEVAL_STATUS_SUCCESS: "URL_RETRIEVAL_STATUS_SUCCESS">
            },
          ]
        }
    }
  ]
}
For more information about this object, see the UrlContextMetadata API reference.

Live fetch
The URL context tool fetches live versions of web pages to keep your information current.

To efficiently retrieve web page content, the URL context tool uses a two-stage process designed to balance speed, cost, and access to the most current information:

Indexed content retrieval: This is the first stage. When you provide a URL, the tool first attempts to fetch the content from Google's extensive and highly optimized web index. This method provides fast access to a vast collection of crawled web pages.

Live fetch fallback: This is the second stage. If the content for a given URL isn't available in the index (for example, if the page is very new and not yet indexed), the tool automatically performs a live fetch. This fallback mechanism can directly access the URL, and retrieve the latest version of the content in real-time.

Safety checks
The system performs a content moderation check on the URL to confirm that the URL meets safety standards. If the URL that you provided fails this check, you get a url_retrieval_status of URL_RETRIEVAL_STATUS_UNSAFE.

Token count
The input token count includes content retrieved from the URLs that you specify in your prompt. From the model output, you can see the token count for your prompt and tools usage in the usage_metadata object. The following is an example output:



'usage_metadata': {
  'candidates_token_count': 45,
  'prompt_token_count': 27,
  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 27}],
  'thoughts_token_count': 31,
  'tool_use_prompt_token_count': 10309,
  'tool_use_prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,
    'token_count': 10309}],
  'total_token_count': 10412
  }
Price per token depends on the model that you use. For more information, see Cost of building and deploying AI models in Vertex AI.

Supported and unsupported content types
The URL context tool can extract content from URLs with the following content types:

Content	Type
Text	text/html
application/json
text/plain
text/xml
text/css
text/javascript
text/csv
text/rtf
Image	image/png
image/jpeg
image/bmp
image/webp
PDF	application/pdf
The URL context tool doesn't support the following content types:

Paywalled content
YouTube videos (For more information, see Video understanding.)
Google Workspace files like Google Docs or Google Sheets
Video and audio files

Grounding with Google Search

bookmark_border
This page explains how to ground a model's responses using Google Search, which uses publicly-available web data. Also, Search suggestions are explained, which are included in your responses.

Grounding with Google Search
If you want to connect your model with world knowledge, a wide possible range of topics, or up-to-date information on the Internet, then use Grounding with Google Search.


Important: If you receive Google Search suggestions with a response, that response is a "Grounded Result" subject to the Grounding with Google Search terms in the Service Terms section of the Service Specific Terms. To use Google Search suggestions, see Use Google Search suggestions.
Note for web publishers: Grounding with Google Search on Vertex AI does not use web pages for grounding that have disallowed Google-Extended. Web publishers can manage inclusion in Google-Extended with a robots.txt file.
To learn more about model grounding in Vertex AI, see the Grounding overview.

Supported models
This section lists the models that support grounding with Search.

Gemini 3 Pro preview
Gemini 3 Pro Image preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.5 Flash with Live API native audio preview
Gemini 2.0 Flash with Live API preview
Gemini 2.0 Flash
Gemini 3 Pro and Gemini 3 Pro Image are limited to 5,000 prompts that enable Grounding with Google Search per day. Starting January 5th, 2026, billing for Grounding with Google Search on Gemini 3 models start, and you can then submit up to 1 million prompts per day that have Grounding with Google Search enabled. For more information about pricing for Grounding with Google Search, see Cost of building and deploying AI models in Vertex AI.

Supported languages
For a list of supported languages, see Languages.

Ground your model with Google Search
Use the following instructions to ground a model with publicly available web data.

Considerations
To use grounding with Google Search, you must enable Google Search Suggestions. For more information, see Use Google Search suggestions.

For ideal results, use a temperature of 1.0. To learn more about setting this configuration, see the Gemini API request body from the model reference.

Grounding with Google Search has a limit of one million queries per day. If you require more queries, contact Google Cloud support for assistance.

Important: If you receive Search suggestions with a response, that response is a grounded result subject to the Grounding with Search terms in the Service Terms section of the Service Specific Terms. For more information about using Google Search suggestions, see Use Search suggestions.
Search results can be customized for a specific geographic location of the end user by using the latitude and longitude coordinates. For more information, see the Grounding API.

Console
Python
Go
Java
REST
Before using any of the request data, make the following replacements:

LOCATION: The region to process the request. To use the global endpoint, exclude the location from the endpoint name and configure the location of the resource to global.
PROJECT_ID: Your project ID.
MODEL_ID: The model ID of the multimodal model.
TEXT: The text instructions to include in the prompt.
EXCLUDE_DOMAINS: Optional: List of domains that aren't to be used for grounding.
LATITUDE: Optional: The latitude of the end user's location. For example, a latitude of 37.7749 represents San Francisco. You can obtain latitude and longitude coordinates using services like Google Maps or other geocoding tools.
LONGITUDE: Optional: The longitude of the end user's location. For example, a longitude of -122.4194 represents San Francisco.
HTTP method and URL:



POST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:generateContent
Request JSON body:



{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "TEXT"
    }]
  }],
  "tools": [{
    "googleSearch": {
      "exclude_domains": [ "domain.com", "domain2.com" ]
    }
  }],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {
        "latitude": LATITUDE,
        "longitude": LONGITUDE
      }
    }
  },
  "model": "projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID"
}
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "candidates": [
    {
      "content": {
        "role": "model",
        "parts": [
          {
            "text": "The weather in Chicago this weekend, will be partly cloudy. The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday. There is a slight chance of rain on both days.\n"
          }
        ]
      },
      "finishReason": "STOP",
      "groundingMetadata": {
        "webSearchQueries": [
          "weather in Chicago this weekend"
        ],
        "searchEntryPoint": {
          "renderedContent": "..."
        },
        "groundingChunks": [
          {
            "web": {
              "uri": "https://www.google.com/search?q=weather+in+Chicago,+IL",
              "title": "Weather information for locality: Chicago, administrative_area: IL",
              "domain": "google.com"
            }
          },
          {
            "web": {
              "uri": "...",
              "title": "weatherbug.com",
              "domain": "weatherbug.com"
            }
          }
        ],
        "groundingSupports": [
          {
            "segment": {
              "startIndex": 85,
              "endIndex": 214,
              "text": "The temperature will be between 49°F (9°C) and 55°F (13°C) on Saturday and between 51°F (11°C) and 56°F (13°C) on Sunday."
            },
            "groundingChunkIndices": [
              0
            ],
            "confidenceScores": [
              0.8662828
            ]
          },
          {
            "segment": {
              "startIndex": 215,
              "endIndex": 261,
              "text": "There is a slight chance of rain on both days."
            },
            "groundingChunkIndices": [
              1,
              0
            ],
            "confidenceScores": [
              0.62836814,
              0.6488607
            ]
          }
        ],
        "retrievalMetadata": {}
      }
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 10,
    "candidatesTokenCount": 98,
    "totalTokenCount": 108,
    "trafficType": "ON_DEMAND",
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 10
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 98
      }
    ]
  },
  "modelVersion": "gemini-2.0-flash",
  "createTime": "2025-05-19T14:42:55.000643Z",
  "responseId": "b0MraIMFoqnf-Q-D66G4BQ"
}
Understand your response
If your model prompt successfully grounds to Google Search from the Vertex AI Studio or from the API, then the responses include metadata with source links (web URLs). However, there are several reasons this metadata might not be provided, and the prompt response won't be grounded. These reasons include low source relevance or incomplete information within the model's response.

Grounding support
Displaying grounding support is required, because it aids you in validating responses from the publishers and adds avenues for further learning.

Grounding support for responses from Google Search sources should be shown both inline and in aggregate. For example, see the following image as a suggestion on how to do this.

Grounding support examples

Use of alternative search engine options
When using Grounding with Google Search a Customer Application can:

Offer alternative search engine options,
Make other search engines the default option,
Display their own or third-party search suggestions or search results as long as: any non-Google results must be displayed separately from Google's Grounded Results and Search Suggestions, and shown in a way that does not confuse users or suggest they are from Google.
Benefits
The following complex prompts and workflows that require planning, reasoning, and thinking can be done when you use Google Search as a tool:

You can ground to help ensure responses are based on the latest and most accurate information.
You can retrieve artifacts from the web to do analysis.
You can find relevant images, videos, or other media to assist in multimodal reasoning or task generation.
You can perform coding, technical troubleshooting, and other specialized tasks.
You can find region-specific information, or assist in translating content accurately.
You can find relevant websites for browsing.
Use Google Search suggestions
When you use grounding with Google Search, and you receive Search suggestions in your response, you must display the Search suggestions in production and in your applications.

For more information on grounding with Google Search, see Grounding with Google Search.

Specifically, you must display the search queries that are included in the grounded response's metadata. The response includes:

"content": LLM-generated response.
"webSearchQueries": The queries to be used for Search suggestions.
For example, in the following code snippet, Gemini responds to a Search grounded prompt, which is asking about a type of tropical plant.



"predictions": [
  {
    "content": "Monstera is a type of vine that thrives in bright indirect light…",
    "groundingMetadata": {
      "webSearchQueries": ["What's a monstera?"],
    }
  }
]
You can take this output, and display it by using Search suggestions.

Requirements for Search suggestions
The following are requirements for suggestions:

Requirement	Description
Do	
While complying with the display requirements, the Search suggestion is displayed exactly as provided without any changes.
When you interact with the Search suggestion, you are taken directly to the Search results page (SRP).
Don't	
Include any screens or additional steps between the user's tap and the display of the SRP.
Display any other search results or suggestions next to the Search suggestion or the associated grounded LLM response.
Display requirements
The following are the display requirements:

Display the Search suggestion exactly as provided, and don't make any modifications to colors, fonts, or appearance. Ensure the Search suggestion renders as specified in the following mocks such as light and dark mode:


Whenever a grounded response is shown, its corresponding Search suggestion should remain visible.
For branding, you must strictly follow Google's guidelines for third-party use of Google brand features at the Welcome to our Brand Resource Center.
When you use grounding with Search, Search suggestion chips display. The field that contains the suggestion chips must be the same width as the grounded response from the LLM.
Behavior on tap
When a user taps the chip, they are taken directly to a Search results page (SRP) for the search term displayed in the chip. The SRP can open either within your in-application browser or in a separate browser application. It's important to not minimize, remove, or obstruct the SRP's display in any way. The following animated mockup illustrates the tap-to-SRP interaction.

app/desktop example

Code to implement a Search suggestion
When you use the API to ground a response to search, the model response provides compliant HTML and CSS styling in the renderedContent field, which you implement to display Search suggestions in your application.

Note: The provided HTML and CSS provided in the API response automatically adapts to your device settings, displaying in either light or dark mode based on the your preference indicated by @media(prefers-color-scheme).
Also note that Search Suggestions were previously referred to as Search Entry Points. While you might encounter references to Search entry points in some API fields, both terms refer to the current Search Suggestions that you receive in your API response.

Billing changes with Gemini 3
When you are using Grounding with Google Search on Gemini 3 models, the billing occurs for each search query that is generated by Gemini and sent to Search. A single prompt might lead to one or more search queries.

Example
This example shows you a sample user prompt and search queries that might be charged.

User prompt: Tell me about the life of Albert Einstein.
Gemini might generate these search queries:
Albert Einstein birth and early life education
Albert Einstein theory of relativity
Albert Einstein life in the US
In this example, these three search queries are charged.

Grounding with Google Maps in Vertex AI

bookmark_border
This page describes how Grounding with Google Maps with Vertex AI can help to enhance your generative AI applications by providing geospatial context.

Overview
Grounding with Google Maps with Vertex AI is a service that connects Gemini models with geospatial data from Google Maps. Google Maps has access to information on millions of locations, including businesses, landmarks, and points of interest. This data gives you access to information on over 250 million places that can be used to ground your model's responses, enabling your AI applications and agents to provide local data and geospatial context.

You can also enable concurrent grounding with Google Maps, Google Search, and grounding with your data sources.

Uses of Grounding with Google Maps
You can use Grounding with Google Maps for various applications, such as:

Conversational assistants that can answer questions about nearby places, such as "Where's the nearest place to get an Italian espresso?"
Personalized descriptions and community insights, such as"Can you tell me more about family-friendly restaurants that are within a walkable distance?"
Summaries of areas around specific locations, such as an EV charging station or a hotel.
This can be beneficial for use cases in real estate, travel, mobility, and social-media apps.

Try the Grounding with Google Maps demoopen_in_new
Supported models
This section lists the models that support Grounding with Google Maps.

Gemini 3 Pro preview
Gemini 3 Pro Image preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.5 Flash with Live API native audio preview
Gemini 2.0 Flash with Live API preview
Gemini 2.0 Flash
Gemini 3 Pro and Gemini 3 Pro Image are limited to 5,000 search queries per day.

For more information about the Gemini models, see Gemini models.

Use Grounding with Google Maps to ground your model's responses
This code sample demonstrates how to use Grounding with Google Maps to ground your model's responses.

Search results can be customized for a specific geographic location by using the latitude and longitude coordinates. For more information, see the Grounding API.

Console
Python
REST
Before using any of the request data, make the following replacements:

LOCATION: The region to process the request. To use the global endpoint, exclude the location from the endpoint name and configure the location of the resource to global.
PROJECT_ID: Your project ID.
MODEL_ID: The model ID of the multimodal model.
PROMPT: The prompt to send to the model.
LATITUDE: The latitude of the location. For example, a latitude of 37.7749 represents San Francisco. You can obtain latitude and longitude coordinates using services like Google Maps or other geocoding tools.
LONGITUDE: The longitude of the location. For example, a longitude of -122.4194 represents San Francisco.
ENABLE_WIDGET: Whether to return a token and enable the Google Maps widget (default is false).
HTTP method and URL:



POST https://LOCATION-aiplatform.googleapis.com/v1beta1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:generateContent
Request JSON body:



{
  "contents": [{
    "role": "user",
    "parts": [{
      "text": "PROMPT"
    }]
  }],
  "tools": [{
    "googleMaps": {
      "enableWidget": "ENABLE_WIDGET"
    }
  }],
  "toolConfig": {
    "retrievalConfig": {
      "latLng": {
        "latitude": LATITUDE,
        "longitude": LONGITUDE
      },
      "languageCode": "en_US"
    }
  },
  "model": "projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID"
}
To send your request, expand one of these options:

curl (Linux, macOS, or Cloud Shell)
PowerShell (Windows)
You should receive a JSON response similar to the following:


{
  "candidates": [
    {
      "content": {
        "role": "model",
        "parts": [
          {
            "text": "\"The Italian Place\" in Alexandria, VA, is good for children and offers takeout. It has a rating of 4.2 stars based on 411 reviews."
          }
        ]
      },
      "finishReason": "STOP",
      "groundingMetadata": {
        "groundingChunks": [
          {
            "maps": {
              "uri": "https://maps.google.com/?cid=9001322937822692826",
              "title": "The Italian Place",
              "placeId": "places/ChIJOTRDf_qwt4kR2kV_WYUf63w"
            }
          },
          {
            "maps": {
              "uri": "https://maps.google.com/?cid=9001322937822692826",
              "title": "Hank's Pasta Bar",
              "placeId": "places/MMVtPzn9FGcevML89",
              "placeAnswerSources": {
                "reviewSnippets": [
                  {
                    "id": "places/ChIJOTRDf_qwt4kR2kV_WYUf63w",
                    "title": "Google Maps Review",
                    "uri": "https://maps.google.com/?cid=9001322937822692826"
                  },
                ]
              }
            }
          },
          ...
        ],
        "groundingSupports": [
          {
            "segment": {
              "endIndex": 79,
              "text": "\"The Italian Place\" in Alexandria, VA, is good for children and offers takeout."
            },
            "groundingChunkIndices": [
              0
            ]
          },
        ],
        "googleMapsWidgetContextToken": "widgetcontent/..."
      }
    }
  ],
  ...
}
Place properties
This section lists place properties that are used to describe locations and used by Grounding with Google Maps to generate responses. These properties are used to determine the types of questions that Grounding with Google Maps can answer.

Sample place properties
This list provides an alphabetized sampling of properties about places that can be used by your model to generate responses.

Address
Curbside pickup
Debit card
Distance
Free parking lot
Live music
Menu for children
Opening hours
Payment options (such as cash or credit card)
Place answer
Pet friendly
Serves beer
Serves vegetarian food
Wheelchair accessible
Wifi
Place answers are a response from Grounding with Google Maps based on information derived from user reviews.

Examples of using place properties
The following examples use place properties in questions about different types of places. Grounding with Google Maps uses the properties to understand your intent and then provides relevant answers based on the data associated with places in Google Maps.

Plan a family dinner: You might ask, Is "The Italian Place" good for children, and do they offer takeout? What is their rating?

Answers to these questions help you to determine if a restaurant is suitable for a family and if the restaurant offers a convenient service.

Check accessibility for a friend: You might ask, I need a restaurant that has a wheelchair accessible entrance.

A response to this prompt might help you to determine if the location meets specific accessibility needs.

Find a location for a late-night snack: You might ask, Is "Burger Joint" open now? Do they serve dinner? What are their opening hours for Friday?

Answers to these questions help you to find an open establishment serving a specific meal during a particular time.

Meet a client for coffee: You might ask, Does "Cafe Central" have Wifi? Do they serve coffee? What is their price level, and do they accept credit cards?

Answers to these questions help you to assess the suitability of a cafe for a business meeting based on amenities, offerings, and payment options.

Information in the Google Maps Grounded Results might differ from actual conditions of the road.

Understand your response
Google Maps sources are returned in groundingMetadata within groundingChunks. Sources are returned for places and for user reviews, which have been used to help generate the Google Maps Grounded Result.

This code sample demonstrates a place source and a place answer source in the response:



"groundingChunks": [
  {
    "maps": {
      "uri": "{Link to Maps Content}",
      "title": "{Name of Maps Place}",
      "placeId": "{Place ID}",
      "placeAnswerSources": {
        "reviewSnippets" : [
          {
            "reviewId": "{Review ID}",
            "googleMapsUri": "{Link to Maps Content}",
            "title": "{Title of review}"
          }
        ]
      }
    },
  }
],
Service usage requirements
This section describes the service usage requirements for Grounding with Google Maps.

Inform the user about the use of Google Maps sources
With each Google Maps Grounded Result, you'll receive sources in groundingChunks that support each response. The following metadata is also returned:

source uri
title
ID
When presenting results from Grounding with Google Maps with Vertex AI, you must specify the associated Google Maps sources, and inform your users of the following:

The Google Maps sources must immediately follow the generated content that the sources support. This generated content is also referred to as Google Maps Grounded Result.
The Google Maps sources must be viewable within one user interaction.
Display Google Maps sources with Google Maps links
For each source in groundingChunks and in grounding_chunks.maps.placeAnswerSources.reviewSnippets, a link preview must be generated following these requirements:

Attribute each source to Google Maps following the Google Maps text attribution guidelines.
Display the source title provided in the response.
Link to the source using the uri or googleMapsUri from the response.
These images show the minimum requirements for displaying the sources and Google Maps links.

Prompt with response showing sources

You can collapse the view of the sources.

Prompt with response and sources collapsed

Optional: Enhance the link preview with additional content, such as:

A Google Maps favicon is inserted before the Google Maps text attribution.
A photo from the source URL (og:image).
For more information about some of our Google Maps data providers and their license terms, see the Google Maps and Google Earth legal notices.

Google Maps text attribution guidelines
When you attribute sources to Google Maps in text, follow these guidelines:

Don't modify the text Google Maps in any way:
Don't change the capitalization of Google Maps.
Don't wrap Google Maps onto multiple lines.
Don't localize Google Maps into another language.
Prevent browsers from translating Google Maps by using the HTML attribute translate="no".
Style Google Maps text as described in the following table:
Property	Style
Font family	Roboto. Loading the font is optional.
Fallback font family	Any sans serif body font already used in your product or "Sans-Serif" to invoke the default system font
Font style	Normal
Font weight	400
Font color	White, black (#1F1F1F), or gray (#5E5E5E). Maintain accessible (4.5:1) contrast against the background.
Font size	Minimum font size: 12sp
Maximum font size: 16sp
To learn about sp, see Font size units on the Material Design website.
Letter spacing	Normal
Example CSS
The following CSS renders Google Maps with the appropriate typographic style and color on a white or light background.



@import url('https://fonts.googleapis.com/css2?family=Roboto&display=swap');

.GMP-attribution {
font-family: Roboto, Sans-Serif;
font-style: normal;
font-weight: 400;
font-size: 1rem;
letter-spacing: normal;
white-space: nowrap;
color: #5e5e5e;
}
Context token, place ID, and review ID
The Google Maps data includes context token, place ID, and review ID. You might cache, store, and export the following response data:

googleMapsWidgetContextToken
placeId
reviewId
The restrictions against caching in the Grounding with Google Maps Terms don't apply.

Prohibited Territory
Grounding with Google Maps has restrictions for certain content and activities to maintain a safe and reliable platform. Customer will not distribute or market a Customer Application that offers Grounding with Google Maps in a Prohibited Territory. The current Prohibited Territories are:

China
Crimea
Cuba
Donetsk People's Republic
Iran
Luhansk People's Republic
North Korea
Syria
Vietnam
This list may be updated from time to time.

Optional: Google Maps contextual widget
The contextual widget is a Google Maps Pre-GA Offering that's a visual container used to support or supplement other Google Maps content. The Google Maps contextual widget lets you integrate Grounding with Google Maps into your applications to create a conversational LLM-powered chat experience. The contextual widget is rendered using the context token, googleMapsWidgetContextToken, which is returned in the Vertex AI API response and can be used to render visual content.

The contextual widget serves different functions depending on your scenario:

It displays user reviews and photos, which is user-generated content (UGC), in the scenario where Google Maps prompting is used for answer generation.

It helps to enrich results with map visualizations and data when Vertex AI generates just a text response.

For more information on the contextual widget, see Maps grounding widget.

Render the Google Maps contextual widget
To render and use the Google Maps contextual widget, use the alpha version of the Google Maps JavaScript API on the page that displays the widget. For more information, see Load the Maps JavaScript API.

The following code samples demonstrate how to use a contextual widget:

Create a contextual widget.



  <body>
   <gmp-place-contextual id="widget"></gmp-place-contextual>
  </body>
In any response that is grounded with Google Maps, there's a corresponding googleMapsWidgetContextToken that's used to render the contextual widget and placed in close proximity to the generated response.

To update the context token, set the widget.contextToken property.



"googleMapsWidgetContextToken": "widgetcontent/AcBXPQdpWQWbap9H-OH8sEKmOXxmEKAYvff0tvthhneMQC3VrqWCjpnPBl4-Id98FGiA_S_t8aeAeJj0T6JkWFX56Bil8oBSR0W8JH3C_RSYLbTjxKdpxc9yNn6JcZTtolIRZon9xi6WpNGuSyjcIxWu2S0hwpasNOpUlWrG1RxVCB4WD1fsz_pwR236mG36lMxevXTQ_JnfdYNuQwQ4Lc3vn...<snip>...
Ts5VJE_b3IC5eE_6wez0nh61r7foTUZpP7BXMwxR-7Wyfcj6x1v6mIWsFGr1o0p_HSAMNqWPg-aFVnkPLhAkOR6MaNZOfezTva-gxHlu7z_haFvYxcUE1qfNVQ",


function updateWidget(contextToken) {
  let widget = document.querySelector('#widget');
  widget.contextToken = contextToken;
}
Optional: Specify the list layout. Valid values include the following:

Compact layout: <gmp-place-contextual-list-config layout="compact">
Vertical layout: <gmp-place-contextual-list-config layout="vertical">
This code sample demonstrates changing the list layout to a compact layout.



   <gmp-place-contextual id="widget">
     <gmp-place-contextual-list-config layout="compact">
     </gmp-place-contextual-list-config>
   </gmp-place-contextual>
Optional: Change the map mode. Valid values include the following:

2D roadmap map: map-mode="roadmap"
3D hybrid map: map-mode="hybrid"
No map: map-mode="none"
This code sample demonstrates changing the map mode to a 2D map.



   <gmp-place-contextual id="widget">
     <gmp-place-contextual-list-config map-mode="roadmap">
     </gmp-place-contextual-list-config>
   </gmp-place-contextual>

The following Gemini models support the ability to generate images in addition to text:

Gemini 2.5 Flash Image, otherwise known as Gemini 2.5 Flash (with Nano Banana)

Gemini 3 Pro Image (preview), otherwise known as Gemini 3 Pro (with Nano Banana)

This expands Gemini's capabilities to include the following:

Iteratively generate images through conversation with natural language, adjusting images while maintaining consistency and context.
Generate images with high-quality long text rendering.
Generate interleaved text-image output. For example, a blog post with text and images in a single turn. Previously, this required stringing together multiple models.
Generate images using Gemini's world knowledge and reasoning capabilities.
Gemini 2.5 Flash Image (gemini-2.5-flash-image) and Gemini 3 Pro Image preview (gemini-3-pro-image-preview) support generating images of people and contains updated safety filters that provide a more flexible and less restrictive user experience. Gemini 2.5 Flash Image can generate images in 1024px. Gemini 3 Pro Image can generate images up to 4096px.

Both models support the following modalities and capabilities:

Text to image

Example prompt: "Generate an image of the Eiffel tower with fireworks in the background."
Text to image (text rendering)

Example prompt: "generate a cinematic photo of a large building with this giant text projection mapped on the front of the building: "Gemini 3 can now generate long form text""
Text to image(s) and text (interleaved)

Example prompt: "Generate an illustrated recipe for a paella. Create images alongside the text as you generate the recipe."
Example prompt: "Generate a story about a dog in a 3D cartoon animation style. For each scene, generate an image"
Image(s) and text to image(s) and text (interleaved)

Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? Can you update the image?"
Best practices

To improve your image generation results, follow these best practices:

Be specific: More details give you more control. For example, instead of "fantasy armor," try "ornate elven plate armor, etched with silver leaf patterns, with a high collar and pauldrons shaped like falcon wings."

Provide context and intent: Explain the purpose of the image to help the model understand the context. For example, "Create a logo for a high-end, minimalist skincare brand" works better than "Create a logo."

Iterate and refine: Don't expect a perfect image on your first attempt. Use follow-up prompts to make small changes, for example, "Make the lighting warmer" or "Change the character's expression to be more serious."

Use step-by-step instructions: For complex scenes, split your request into steps. For example, "First, create a background of a serene, misty forest at dawn. Then, in the foreground, add a moss-covered ancient stone altar. Finally, place a single, glowing sword on top of the altar."

Describe what you want, not what you don't: Instead of saying "no cars", describe the scene positively by saying, "an empty, deserted street with no signs of traffic."

Control the camera: Guide the camera view. Use photographic and cinematic terms to describe the composition, for example, "wide-angle shot", "macro shot", or "low-angle perspective".

Prompt for images: Describe the intent by using phrases such as "create an image of" or "generate an image of". Otherwise, the multimodal model might respond with text instead of the image.

Pass Thought Signatures: When using Gemini 3 Pro Image, we recommend that you pass thought signatures back to the model during multi-turn image creation and editing. This lets you preserve reasoning context across interactions. For code samples related to multi-turn image editing using Gemini 3 Pro Image, see Example of multi-turn image editing using thought signatures.

Limitations:

For best performance with Gemini 2.5 Flash Image, use the following languages: EN, es-MX, ja-JP, zh-CN, or hi-IN. For best performance with Gemini 3 Pro Image, use the following languages: ar-EG, de-DE, EN, es-MX, fr-FR, hi-IN, id-ID, it-IT, ja-JP, ko-KR, pt-BR, ru-RU, ua-UA, vi-VN, and zh-CN

Image generation doesn't support audio or video inputs.

The model might not create the exact number of images you ask for.

For best results using Gemini 2.5 Flash Image, include a maximum of three images in an input. For best results using Gemini 3 Pro Image, include a maximum of 14 images in an input.

When generating an image containing text, first generate the text and then generate an image with that text.

Image or text generation might not work as expected in these situations:

The model might only create text and no image if the prompt is ambiguous. If you want images, clearly ask for images in your request. For example, "provide images as you go along."

The model might create text as an image. To generate text, specifically ask for text output. For example, "generate narrative text along with illustrations."

The model might stop generating content even when it's not finished. If this occurs, try again or use a different prompt.

If a prompt is potentially unsafe, the model might not process the request and returns a response indicating that it can't create unsafe images. In this case, the FinishReason is STOP.

Generate images
The following sections cover how to generate images using either Vertex AI Studio or using the API.

For guidance and best practices for prompting, see Design multimodal prompts.

Console
Python
Node.js
Java
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const fs = require('fs');
const {GoogleGenAI, Modality} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION =
  process.env.GOOGLE_CLOUD_LOCATION || 'us-central1';

async function generateImage(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const response = await client.models.generateContentStream({
    model: 'gemini-2.5-flash-image',
    contents:
      'Generate an image of the Eiffel tower with fireworks in the background.',
    config: {
      responseModalities: [Modality.TEXT, Modality.IMAGE],
    },
  });

  const generatedFileNames = [];
  let imageIndex = 0;

  for await (const chunk of response) {
    const text = chunk.text;
    const data = chunk.data;
    if (text) {
      console.debug(text);
    } else if (data) {
      const outputDir = 'output-folder';
      if (!fs.existsSync(outputDir)) {
        fs.mkdirSync(outputDir, {recursive: true});
      }
      const fileName = `${outputDir}/generate_content_streaming_image_${imageIndex++}.png`;
      console.debug(`Writing response image to file: ${fileName}.`);
      try {
        fs.writeFileSync(fileName, data);
        generatedFileNames.push(fileName);
      } catch (error) {
        console.error(`Failed to write image file ${fileName}:`, error);
      }
    }
  }

  // Example response:
  //  I will generate an image of the Eiffel Tower at night, with a vibrant display of
  //  colorful fireworks exploding in the dark sky behind it. The tower will be
  //  illuminated, standing tall as the focal point of the scene, with the bursts of
  //  light from the fireworks creating a festive atmosphere.

  return generatedFileNames;
}
Generate interleaved images and text
Gemini 2.5 Flash Image can generate interleaved images with its text responses. For example, you can generate images of what each step of a generated recipe might look like to go along with the text of that step, without having to make separate requests to the model to do so.

Console
Python
Java
REST
Run the following command in the terminal to create or overwrite this file in the current directory:



curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://${API_ENDPOINT}:generateContent \
  -d '{
    "contents": {
      "role": "USER",
      "parts": [
        {
          "text": "Create a tutorial explaining how to make a peanut butter and jelly sandwich in three easy steps. For each step, provide a title with the number of the step, an explanation, and also generate an image, generate each image in a 1:1 aspect ratio."
        }
      ]
    },
    "generationConfig": {
      "responseModalities": ["TEXT", "IMAGE"],
      "imageConfig": {
        "aspectRatio": "16:9",
      },
    },
    "safetySettings": {
      "method": "PROBABILITY",
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
      "threshold": "BLOCK_MEDIUM_AND_ABOVE"
    },
  }' 2>/dev/null >response.json
Note: Gemini 2.5 Flash Image and Gemini 3 Pro Image support the following aspect ratios: 1:1, 3:2, 2:3, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, and 21:9.
Gemini will generate an image based on your description. This process takes a few seconds, but can be comparatively slower depending on capacity.

Edit images
Gemini 2.5 Flash Image for image generation (gemini-2.5-flash-image) supports the ability to edit images in addition generating them. Gemini 2.5 Flash Image supports improved editing of images and multi-turn editing, and contains updated safety filters that provide a more flexible and less restrictive user experience.

It supports the following modalities and capabilities:

Image editing (text and image to image)

Example prompt: "Edit this image to make it look like a cartoon"
Example prompt: [image of a cat] + [image of a pillow] + "Create a cross stitch of my cat on this pillow."
Multi-turn image editing (chat)

Example prompts: [upload an image of a blue car.] "Turn this car into a convertible."

[Model returns an image of a convertible in the same scene] "Now change the color to yellow."
[Model returns an image with a yellow convertible] "Add a spoiler."
[Model returns an image of the convertible with a spoiler]
Edit an image
Console
Python
Java
REST
Run the following command in the terminal to create or overwrite this file in the current directory:



curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://${API_ENDPOINT}:generateContent \
  -d '{
    "contents": {
      "role": "USER",
      "parts": [
        {"fileData": {
          "mimeType": "image/jpg",
          "fileUri": "FILE_NAME"
          }
        },
        {"text": "Convert this photo to black and white, in a cartoonish style."},
      ]

    },
    "generationConfig": {
      "responseModalities": ["TEXT", "IMAGE"],
      "imageConfig": {
        "aspectRatio": "16:9",
      },
    },
    "safetySettings": {
      "method": "PROBABILITY",
      "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
      "threshold": "BLOCK_MEDIUM_AND_ABOVE"
    },
  }' 2>/dev/null >response.json
Note: Gemini 2.5 Flash Image supports the following aspect ratios: 1:1, 3:2, 2:3, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, and 21:9.
Gemini will generate an image based on your description. This process takes a few seconds, but can be comparatively slower depending on capacity.

Multi-turn image editing
Gemini 2.5 Flash Image and Gemini 3 Pro Image support improved multi-turn editing, letting you respond to the model with changes after receiving an edited image response. This lets you continue to make edits to the image conversationally.

Note that it's recommended to limit the entire request file size to 50MB maximum.

To test out multi-turn image editing, try the following notebooks:

Gemini 3 Pro Image Generation in Vertex AI,

Gemini 2.5 Flash Image Generation in Vertex AI

For code samples related to multi-turn image creation and editing using Gemini 3 Pro Image, see Example of multi-turn image editing using thought signatures.

Responsible AI
To ensure a safe and responsible experience, Vertex AI's image generation capabilities are equipped with a multi-layered safety approach. This is designed to prevent the creation of inappropriate content, including sexually explicit, dangerous, violent, hateful, or toxic material.

All users must adhere to the Generative AI Prohibited Use Policy. This policy strictly forbids the generation of content that:

Relates to child sexual abuse or exploitation.
Facilitates violent extremism or terrorism.
Facilitates non-consensual intimate imagery. Facilitates self-harm.
Is sexually explicit.
Constitutes hate speech.
Promotes harassment or bullying.
When provided with an unsafe prompt, the model might refuse to generate an image, or the prompt or generated response might be blocked by our safety filters.

Model refusal: If a prompt is potentially unsafe, the model might refuse to process the request. If this happens, the model usually gives a text response saying it can't generate unsafe images. The FinishReason will be STOP.
Safety filter blocking:
If the prompt is identified as potentially harmful by a safety filter, the API returns BlockedReason in PromptFeedback.
If the response is identified as potentially harmful by a safety filter, the API response will include a FinishReason of IMAGE_SAFETY, IMAGE_PROHIBITED_CONTENT, or similar.
Safety filter code categories
Depending on the safety filters you configure, your output may contain a safety reason code similar to the following:



    {
      "raiFilteredReason": "ERROR_MESSAGE. Support codes: 56562880"
    }
The code listed corresponds to a specific harmful category. These code to category mappings are as follows:

Error code	Safety category	Description	Content filtered: prompt input or image output
58061214
17301594	Child	Detects child content where it isn't allowed due to the API request settings or allowlisting.	input (prompt): 58061214
output (image): 17301594
29310472
15236754	Celebrity	Detects a photorealistic representation of a celebrity in the request.	input (prompt): 29310472
output (image): 15236754
62263041	Dangerous content	Detects content that's potentially dangerous in nature.	input (prompt)
57734940
22137204	Hate	Detects hate-related topics or content.	input (prompt): 57734940
output (image): 22137204
74803281
29578790
42876398	Other	Detects other miscellaneous safety issues with the request.	input (prompt): 42876398
output (image): 29578790, 74803281
39322892	People/Face	Detects a person or face when it isn't allowed due to the request safety settings.	output (image)
92201652	Personal information	Detects Personally Identifiable Information (PII) in the text, such as the mentioning a credit card number, home addresses, or other such information.	input (prompt)
89371032
49114662
72817394	Prohibited content	Detects the request of prohibited content in the request.	input (prompt): 89371032
output (image): 49114662, 72817394
90789179
63429089
43188360	Sexual	Detects content that's sexual in nature.	input (prompt): 90789179
output (image): 63429089, 43188360
78610348	Toxic	Detects toxic topics or content in the text.	input (prompt)
61493863
56562880	Violence	Detects violence-related content from the image or text.	input (prompt): 61493863
output (image): 56562880
32635315	Vulgar	Detects vulgar topics or content from the text.	input (prompt)
64151117	Celebrity or child	Detects photorealistic respresentation of a celebrity or of a child that violates Google's safety policies.	input (prompt)
output (image)

Generate images with text
You can generate novel images using only descriptive text as an input. The following samples show you basic instructions to generate images.

Important: imagen-4.0-fast-generate-001 may generate undesireable results if the prompt is complex and you use enhanced prompts. To fix this, avoid using Help me write in Google Cloud console, or set enhancePrompt to false.
Console
Python
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
MODEL_VERSION: The Imagen model version to use. For more information about available models, see Imagen models.

TEXT_PROMPT: The text prompt that guides what images the model generates. This field is required for both generation and editing.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
Additional optional parameters

HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/MODEL_VERSION:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT"
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/MODEL_VERSION:predict"
The following sample response is for a request with "sampleCount": 2. The response returns two prediction objects, with the generated image bytes base64-encoded.


{
  "predictions": [
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    },
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}
If you use a model that supports prompt enhancement, the response includes an additional prompt field with the enhanced prompt used for generation:


{
  "predictions": [
    {
      "mimeType": "MIME_TYPE",
      "prompt": "ENHANCED_PROMPT_1",
      "bytesBase64Encoded": "BASE64_IMG_BYTES_1"
    },
    {
      "mimeType": "MIME_TYPE",
      "prompt": "ENHANCED_PROMPT_2",
      "bytesBase64Encoded": "BASE64_IMG_BYTES_2"
    }
  ]
}

Edit images with Imagen on Vertex AI

bookmark_border
Imagen on Vertex AI lets you edit Imagen-generated or existing images. You can specify part of the image to modify in addition to a text description of the updates (mask-base editing).

Types of available editing tools
Imagen supports mask-based editing. Mask-based editing lets you specify a targeted area to apply edits to. This method of editing is good for edits that apply to only parts of an image. Mask-based editing is useful for the following types of image edits:

Insert objects into an image
Remove objects from an image
Expand the content of an image
Replace the background of an image
Locations
A location is a region you can specify in a request to control where data is stored at rest. For a list of available regions, see Generative AI on Vertex AI locations.

Safety filtering
Both input data and output content are checked for offensive material when you send an image editing request to Imagen. This means offensive input data from a text prompt or from a base image you provide can be blocked. Similarly, offensive output images might also be blocked, affecting the number of generated images you get in a response.

For more information about safety filtering and blocked content handling, see Responsible AI and usage guidelines for Imagen.


Model versions
There are multiple image generation models that you can use. Each model may have limits that are specific to that model. For more information, see Imagen models.

Generate images
Python
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_PERSON_IMAGE: The Base64-encoded image of the person image.
BASE64_PRODUCT_IMAGE: The Base64-encoded image of the product image.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
GCS_OUTPUT_PATH: The Cloud Storage path to store the virtual try-on output to.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict
Request JSON body:



{
  "instances": [
    {
      "personImage": {
        "image": {
          "bytesBase64Encoded": "BASE64_PERSON_IMAGE"
        }
      },
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_PRODUCT_IMAGE"
          }
        }
      ]
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT,
    "storageUri": "GCS_OUTPUT_PATH"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]

  Generate images
Python
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_PERSON_IMAGE: The Base64-encoded image of the person image.
BASE64_PRODUCT_IMAGE: The Base64-encoded image of the product image.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
GCS_OUTPUT_PATH: The Cloud Storage path to store the virtual try-on output to.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict
Request JSON body:



{
  "instances": [
    {
      "personImage": {
        "image": {
          "bytesBase64Encoded": "BASE64_PERSON_IMAGE"
        }
      },
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_PRODUCT_IMAGE"
          }
        }
      ]
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT,
    "storageUri": "GCS_OUTPUT_PATH"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]
  Generate images
Python
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_PERSON_IMAGE: The Base64-encoded image of the person image.
BASE64_PRODUCT_IMAGE: The Base64-encoded image of the product image.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
GCS_OUTPUT_PATH: The Cloud Storage path to store the virtual try-on output to.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict
Request JSON body:



{
  "instances": [
    {
      "personImage": {
        "image": {
          "bytesBase64Encoded": "BASE64_PERSON_IMAGE"
        }
      },
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_PRODUCT_IMAGE"
          }
        }
      ]
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT,
    "storageUri": "GCS_OUTPUT_PATH"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]
}

Recontextualize product images

bookmark_border
Preview

This product or feature is a Generative AI Preview offering, subject to the "Pre-GA Offerings Terms" of the Google Cloud Service Specific Terms, as well as the Additional Terms for Generative AI Preview Products. For this Generative AI Preview offering, Customers may elect to use it for production or commercial purposes, or disclose Generated Output to third-parties, and may process personal data as outlined in the Cloud Data Processing Addendum, subject to the obligations and restrictions described in the agreement under which you access Google Cloud. Pre-GA products are available "as is" and might have limited support. For more information, see the launch stage descriptions.

Imagen product recontext on Vertex AI lets you edit product images into different scenes or backgrounds. You provide images of a product and an optional prompt, and Imagen product recontext generates images of your product in recontextualized in new scenes or backgrounds.

To request access to Imagen product recontext, complete the Vertex AI - Generative Media for Marketing Access Request form.

Supported product types
Imagen product recontext supports the following product types:

Appliances
Business and industrial
Clothing
Electronics
Furniture
Garden and yard
Hardware
Health and beauty
Jewelry
Pets
Shoes
Sporting goods
Toys and games
Vehicles
Product recontextualization example
The following shows an example use case for product recontextualization:

Sample Inputs	Sample Outputs	Prompt used
A black laptop with multicolor backlighting on the keyboard
             and a colorful test image on the screen, placed on a white
             background.	A black laptop with multicolor backlighting on the keyboard
             and a colorful test image on the screen, now placed on a workbench.
             Behind the workbench is a window, with multicolor advertisements
             displayed outside.	On a cluttered workbench in a rainy, neon-lit cyberpunk alley, with holographic advertisements flickering in the background.
A digital SLR camera with a large lens attached, placed on a
             whilte background.	A digital SLR camera with a large lens attached, placed on a
             whilte background. The camera is now placed on a cliff edge, with a
             wide waterfall in the background.	Placed on a mossy rock on the edge of a cliff, overlooking a massive, powerful waterfall in a dramatic, overcast Icelandic landscape.
A pair of sunglasses placed on a white background.	A pair of sunglasses, now placed on a table with a cup of
             coffee placed on a saucer in the foreground, and a scenic view of
             the top of a church and a body of water with hills in the
             background.	Resting on a white cafe table in Santorini, with a cup of coffee, overlooking the Aegean Sea and iconic blue-domed buildings.
Try Imagen product recontext in a Colab

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Verify that billing is enabled for your Google Cloud project.

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Generate product images in different scenes
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
TEXT_PROMPT: Optional. A text prompt to guide the images that the model generates.
BASE64_SUBJECT_IMAGE: A base64-encoded image of the subject image.
PERSON_SETTING: Optional: A string value that controls the type of person or face generation that the model allows. You can use the following values for personGeneration:
"allow_adult": Allows generation of adults only, except for celebrity generation. Celebrity generation isn't allowed for any setting. This is the default setting.
"allow_all": Allows generation of people of all ages, except for celebrity generation. Celebrity generation is not allowed for any setting.
"dont_allow": Doesn't allow generation of people or faces in generated output.
SAFETY_SETTING: Optional: A string value that controls the safety filter threshold for generated images. You can use the following values to select your safety setting:
"block_low_and_above": The most stringent safety threshold. "block_low_and_above" filters the largest number of generated images.
"block_medium_and_above": The medium safety threshold, which balances filtering for potentially harmful and safe content. "block_medium_and_above" is the default safety setting.
"block_only_high": The lowest safety threshold, which reduces the number of requests blocked due to safety filter. Using "block_only_high" safety threshold might increase the number of objectionable images generated by the model.
WATERMARK_SETTING: Optional: A boolean. When this values is set to true, the model adds a digital watermark that you can use to verify the generated image. The default value is true.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1-4.
PROMPT_SETTING: Optional: A boolean. When this value is set to true, the model uses enhanced prmopts. The default value is true.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-product-recontext-preview-06-30:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_SUBJECT_IMAGE"
          },
        }
      ]
    }
  ],
  "parameters": {
    "personGeneration": PERSON_SETTING,
    "safetySetting": SAFETY_SETTING,
    "addWatermark": WATERMARK_SETTING,
    "sampleCount": IMAGE_COUNT,
    "enhancePrompt": PROMPT_SETTING
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-product-recontext-preview-06-30:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]
}


Upscale images using Imagen

bookmark_border
You can use Imagen on Vertex AI to upscale images, which increases their size without losing image quality.

imagen-4.0-upscale-preview supports image upscaling in Preview.

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Verify that billing is enabled for your Google Cloud project.

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Upscale an image
Important: The output image resolution, which is the result of multiplying the input image's resolution by the chosen upscale factor, must not exceed 17 megapixels.
Console
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_SUBJECT_IMAGE: A base64-encoded image of the subject image.
ADD_WATERMARK: An optional Boolean value. Set to true to enable watermarked images, or false to disable watermarked images. The default value is true.
GCS_IMAGE_PATH: A Cloud Storage path to an image file.
GCS_OUTPUT_PATH: the Cloud Storage path to store the generated output to.
OUTPUT_MIMETYPE: An optional string that defines the output file type of the image. The following values are accepted: "image/png" or "image/jpeg". The default is "image/png".
COMPRESSION_QUALITY: An optional integer value that specifies the level of detail that the model preserves for JPEG images. The following range of values are accepted: 0 - 100. The higher value specifies a higher compression level. The default is 75.
UPSCALE_FACTOR: The scaling factor for the upscaled image. The final output resolution of the image after applying the upscale factor must not exceed 17 megapixels. The following values are accepted:
"x2"
"x3"
"x4"
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-4.0-upscale-preview:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "Upscale the image",
      "image": {
        // use one of the following to specify the image to upscale
        "bytesBase64Encoded": "BASE64_SUBJECT_IMAGE"
        "gcsUri": "GCS_IMAGE_PATH"
        // end of base image input options
      },
    }
  ],
  "parameters": {
    "mode": "upscale",
    "storageUri": "GCS_OUTPUT_PATH",
    "outputOptions": {
      "mimeType": "OUTPUT_MIMETYPE",
      "compressionQuality": COMPRESSION_QUALITY
    },
    "upscaleConfig": {
      "upscaleFactor": "UPSCALE_FACTOR"
    }
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-4.0-upscale-preview:predict"
The request returns an image object. In this example, an image objects is returned as a prediction object with a base64-encoded image.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}

Base64 encode and decode files

bookmark_border
Base64 encode images
To make image generation requests you must send image data as Base64 encoded text.

Using the command line
Within a gRPC request, you can simply write binary data out directly; however, JSON is used when making a REST request. JSON is a text format that does not directly support binary data, so you will need to convert such binary data into text using Base64 encoding.

Most development environments contain a native base64 utility to encode a binary into ASCII text data. To encode a file:

Linux
macOS
Windows
PowerShell
Encode the file using the Base64.exe tool:



Base64.exe -e INPUT_FILE > OUTPUT_FILE
Create a JSON request file, inlining the base64-encoded data:

JSON



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "image": {
        "bytes_base64_encoded": "B64_BASE_IMAGE"
      }
    }
  ]
}
Using client libraries
Embedding binary data into requests through text editors is neither desirable or practical. In practice, you will be embedding base64 encoded files within client code. All supported programming languages have built-in mechanisms for base64 encoding content.

Python
Node.js
Java
Go


# Import the base64 encoding library.
import base64

# Pass the image data to an encoding function.
def encode_image(image):
    with open(image, "rb") as image_file:
        encoded_string = base64.b64encode(image_file.read())
    return encoded_string
Base64 decode images
API requests return generated or edited images as base64-encoded strings. You can use the following client library samples to decode this data and save it locally as an image file.

Python
Node.js
Java
Go


# Import the base64 encoding library.
import base64

# Pass the base64 encoded image data to a decoding function and save image file.
def decode_image(b64_encoded_string):
   with open("b64DecodedImage.png", "wb") as fh:
     fh.write(base64.decodebytes(b64_encoded_string))

     Generate videos with Veo on Vertex AI from text prompts

bookmark_border
You can use Veo on Vertex AI to generate new videos from a text prompt. Supported interfaces include the Google Cloud console and the Vertex AI API.

For more information about writing effective text prompts for video generation, see the Veo prompt guide.

Try Veo in a Colab

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Generate videos from text
The following examples show how you can use text prompts to generate videos:

Console
Python
REST
After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint.

For more information about the Veo API, see the Veo on Vertex AI API.

Use the following command to send a video generation request. This request begins a long-running operation and stores output to a Cloud Storage bucket you specify.


Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string respresenting the model ID to use. The following are accepted values:
Veo 2: "veo-2.0-generate-001"
Veo 3:"veo-3.0-generate-001"
Veo 3:"veo-3.0-fast-generate-001"
Veo 3:"veo-3.0-generate-preview" (Preview)
Veo 3:"veo-3.0-fast-generate-preview" (Preview)
Veo 3.1: "veo-3.1-generate-001"
Veo 3.1: "veo-3.1-fast-generate-001"
TEXT_PROMPT: The text prompt used to guide video generation.
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
DURATION: An integer representing the length of the generated video files. The following are the accepted values for each model:
Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4, 6, or 8. The default is 8.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT"
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": "RESPONSE_COUNT"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Optional: Check the status of the video generation long-running operation.


Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: The model ID to use.
OPERATION_ID: The unique operation ID returned in the original generate video request.
HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
Request JSON body:



{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
This request returns information about the operation, including if the operation is still running or is done.
Response

{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID",
  "done": true,
  "response": {
    "raiMediaFilteredCount": 0,
    "@type": "type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
    "videos": [
      {
        "gcsUri":"gs://BUCKET_NAME/TIMESTAMPED_FOLDER/sample_0.mp4",
        "mimeType": "video/mp4"
      }
    ]
  }
}

Generate videos with Veo on Vertex AI from an image

bookmark_border
You can use Veo on Vertex AI to generate new videos from an image and text prompt. Supported interfaces include the Google Cloud console and the Vertex AI API.

For more information about writing effective text prompts for video generation, see the Veo prompt guide.

Try Veo in a Colab

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Generate videos from an image
Sample input	Sample output
Input image1
Input PNG file of a crocheted elephant
Text prompt: the elephant moves around naturally

Output video of a crocheted elephant
1 Image generated using Imagen on Vertex AI from the prompt: A Crochet elephant in intricate patterns walking on the savanna

You can generate novel videos using only an image as an input, or and image and descriptive text as the inputs. The following samples show you basic instructions to generate videos from image and text.

Console
Python
REST

After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint.

For more information about the Veo API, see the Veo on Vertex AI API.

Use the following command to send a video generation request. This request begins a long-running operation and stores output to a Cloud Storage bucket you specify.


Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string respresenting the model ID to use. The following are accepted values:
Veo 2:veo-2.0-generate-001
Veo 3:veo-3.0-generate-001
Veo 3.1:veo-3.1-generate-001
Veo 3.1:veo-3.1-fast-generate-001
TEXT_PROMPT: The text prompt used to guide video generation.
INPUT_IMAGE: A base64-encoded string that represents the input image. For best quality, we recommend that the input image's resolution be 720p (1280 x 720 pixels) or higher, and have an aspect ratio of either 16:9 or 9:16. Images of other aspect ratios or sizes may be resized or centrally cropped when the image is uploaded.
MIME_TYPE: A string representing the MIME type of the input image. Only the images of the following MIME types are supported:
"image/jpeg"
"image/png"
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESIZE_MODE: A string that represents the resize mode to use. The following are accepted values:
"crop": Crop the video to fit the new size.
"pad": Pad the video to fit the new size.
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
DURATION: An integer representing the length of the generated video files. The following are the accepted values for each model:
Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4, 6, or 8. The default is 8.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "image": {
        "bytesBase64Encoded": "INPUT_IMAGE",
        "mimeType": "MIME_TYPE"
      }
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
    "resizeMode": "RESIZE_MODE"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Optional: Check the status of the video generation long-running operation.


Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: The model ID to use.
OPERATION_ID: The unique operation ID returned in the original generate video request.
HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
Request JSON body:



{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
This request returns information about the operation, including if the operation is still running or is done.

Generate videos with Veo on Vertex AI using first and last video frames

bookmark_border

You can generate Veo on Vertex AI videos by specifying the first and last frames of the video. Supported interfaces include the Google Cloud console and the Vertex AI API.

For more information about writing effective text prompts for video generation, see the Veo prompt guide.

Try Veo in a Colab

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Create a video from first and last frames
The following examples show how you can specify the first and last frames of a Veo video:

Console
Python
REST
After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint.

For more information about the Veo API, see the Veo on Vertex AI API.

Use the following command to send a video generation request. This request begins a long-running operation and stores output to a Cloud Storage bucket you specify.


Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string that represents the model ID to use. Use one of the following when specifying a first or last video frame:
Veo 2: veo-2.0-generate-001
Veo 3: veo-3.1-generate-001
Veo 3: veo-3.1-fast-generate-001
TEXT_PROMPT: The text prompt used to guide video generation.
PATH_TO_FIRST_FRAME_IMAGE: A string that represents the Cloud Storage path to the first frame's image. For example: "gs://video-bucket/input/first-frame.jpg".
IMAGE_MIME_TYPE: A string representing the MIME type of the input image. Only images of the following MIME types are supported:
image/jpeg
image/png
PATH_TO_LAST_FRAME_IMAGE: A string that represents the Cloud Storage path to the last frame's image. For example: "gs://video-bucket/input/last-frame.jpg".
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
DURATION: An integer representing the length of the generated video files. The following are accepted values for each model:
Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4, 6, or 8. The default is 8.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
       "image": {
         "gcsUri": "PATH_TO_FIRST_FRAME_IMAGE,
         "mimeType": "IMAGE_MIME_TYPE"
       },
       "lastFrame:" {
         "gcsUri": "PATH_TO_LAST_FRAME_IMAGE,
         "mimeType": "IMAGE_MIME_TYPE"
       }
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Optional: Check the status of the video generation long-running operation.


Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: The model ID to use.
OPERATION_ID: The unique operation ID returned in the original generate video request.
HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
Request JSON body:



{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
This request returns information about the operation, including if the operation is still running or is done.
Response

Extend Veo on Vertex AI-generated videos

bookmark_border
You can use Veo on Vertex AI to extend videos that you previously generated using Veo that are 1 to 30 seconds in length. You can extend videos using either the Google Cloud console or the Vertex AI API.

For information about writing effective text prompts for video generation, see the Veo prompt guide.

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Extend a video
The following examples show how you can extend a Veo video:

Console
Python
REST

After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint.

For more information about the Veo API, see the Veo on Vertex AI API.

Use the following command to send a video generation request. This request begins a long-running operation and stores output to a Cloud Storage bucket you specify.


Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string that represents the model ID to use. Use one of the following when specifying a first or last video frame:
Veo 2: veo-2.0-generate-001
Veo 3: veo-3.1-generate-preview
Veo 3: veo-3.1-fast-generate-preview
TEXT_PROMPT: The text prompt used to guide video generation.
PATH_TO_VIDEO: A string that represents the Cloud Storage path to the Veo video that you're extending. For example: "gs://video-bucket/input/veo-video.mp4".
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
       "video": {
         "gcsUri": "PATH_TO_VIDEO",
         "mimeType": "video/mp4"
       }
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Optional: Check the status of the video generation long-running operation.


Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: The model ID to use.
OPERATION_ID: The unique operation ID returned in the original generate video request.
HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
Request JSON body:



{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
This request returns information about the operation, including if the operation is still running or is done.
Response

Lyria | AI Music Generator

bookmark_border

To see an example of Lyria Music Generation, run the "Lyria Music Generation with Python SDK" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can use Lyria to generate new instrumental music tracks from a text prompt that you provide in the Google Cloud console or send in a request to the Gemini API in Vertex AI API.

Try Lyria on Vertex AI (Vertex AI Studio)

Lyria features and launch stage
Lyria offers generative AI features for music creation.

The following table describes features that are Generally Available (GA) to all users:

Feature	Description	Launch stage
Generate music from text	Generate instrumental music tracks from descriptive text input.	General Availability
Negative prompting	Guide the model to avoid specific elements in the generated music.	General Availability
Seed for reproducibility	Use a seed to ensure consistent generation outputs for the same prompt and parameters.	General Availability
Locations
A location is a region you can specify in a request to control where data is stored at rest. Lyria 2 is offered globally. For the list of available regions for Vertex AI services, see Generative AI on Vertex AI locations.

Performance and limitations
Limits	Value
Modalities	Text-to-music generation (instrumental only)
API calls (prompts per project per minute)	Refer to the Quotas and limits documentation for standard Vertex AI Inference limits.
Request latency (Generation time)	Audio clips are typically generated within 10 to 20 seconds. Some requests may be queued during peak usage.
Maximum number of audio clips returned per request	Up to the specified sample_count. (Max value for sample_count TBD, typically a small integer e.g., 1-4).
Audio length per clip	32.8 seconds
Supported returned audio format	WAV audio at 48kHz sample rate
Input prompt language	US English (en-us)
Responsible AI
Lyria generates instrumental music from text prompts. Lyria applies safety measures including content safety filters, recitation checking, and artist intent checks to prevent the input and generation of harmful or inappropriate content. SynthID watermarking is used on generated audio. Prompts that violate responsible AI guidelines may be blocked.

Lyria Vertex AI model versions and lifecycle
The Lyria model and version are the following:

Model name	Identifier
Lyria 2	lyria-002
Generate music from text

To see an example of Lyria Music Generation, run the "Lyria Music Generation with Python SDK" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can generate novel instrumental music tracks using descriptive text as input.

Before you begin
In the Google Cloud console, on the project selector page, select or create a Google Cloud project.

Roles required to select or create a project

Note: If you don't plan to keep the resources that you create in this procedure, create a project instead of selecting an existing project. After you finish these steps, you can delete the project, removing all resources associated with the project.
Go to project selector

Enable the Vertex AI API.

Roles required to enable APIs

Enable the API

Set up authentication for your environment.

Select the tab for how you plan to use the samples on this page:

Console
REST
To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.

Install the Google Cloud CLI. After installation, initialize the Google Cloud CLI by running the following command:



gcloud init
If you're using an external identity provider (IdP), you must first sign in to the gcloud CLI with your federated identity.

For more information, see Authenticate for using REST in the Google Cloud authentication documentation.

Console
REST

Use the predict method to send a music generation request. The response will directly contain the audio data, typically base64-encoded if the response is JSON.

For more information about lyria-002 model requests, see the lyria-002 model API reference.

To generate music, send a POST request to the model's predict endpoint.

Request:



curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/lyria-002:predict \
  -d '{
    "instances": [
      {
        "prompt": "An uplifting and hopeful orchestral piece with a soaring string melody and triumphant brass.",
        "negative_prompt": " dissonant, minor key",
        "seed": 12345
      }
    ],
    "parameters": {
      // "sample_count": 1 // Use either seed or sample_count
    }
  }'
Response:

A successful request returns a JSON object containing the generated audio data. This should look something like this:



{
  "predictions": [
    {
      "audioContent": "BASE64_ENCODED_WAV_STRING_SAMPLE_1",
      "mimeType": "audio/wav"
    },
    {
      "audioContent": "BASE64_ENCODED_WAV_STRING_SAMPLE_2",
      "mimeType": "audio/wav"
    }
  ],
  "deployedModelId": "xxxxxxxxxxxxxxx",
  "model": "projects/PROJECT_ID/locations/LOCATION/publishers/google/models/lyria-002",
  "modelDisplayName": "Lyria 2"
}
You would then decode the audioContent (base64) to get the WAV audio file. Each clip is 32.8 seconds long.

Image understanding

bookmark_border
To see an example of image understanding, run the "Intro to Multimodal Use Cases with the Gemini API" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can add images to Gemini requests to perform tasks that involve understanding the contents of the included images. This page shows you how to add images to your requests to Gemini in Vertex AI by using the Google Cloud console and the Vertex AI API.

Supported models
The following table lists the models that support image understanding:

Model	Media details	MIME types
Gemini 3 Pro preview	
Maximum images per prompt: 900
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Default resolution tokens: 1120
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 3 Pro Image preview	
Maximum images per prompt: 14
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum number of output images per prompt: Limited to 32,768 output tokens
Supported aspect ratios: 1:1, 3:2, 2:3, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, and 21:9
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Pro	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash preview	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash-Lite preview	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum number of output images per prompt: 10
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash Image	
Maximum images per prompt: 3
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum number of output images per prompt: 10
Supported aspect ratios: 1:1, 3:2, 2:3, 3:4, 4:3, 4:5, 5:4, 9:16, 16:9, and 21:9
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash-Lite	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum number of output images per prompt: 10
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.5 Flash with Live API native audio preview	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.0 Flash with image generation preview	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum number of output images per prompt: 10
Maximum tokens per minute (TPM) per project:
High/Medium/Default media resolution:
US/Asia: 40 M
EU: 10 M
Low media resolution:
US/Asia: 10 M
EU: 3 M
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.0 Flash	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum tokens per minute (TPM) per project:
High/Medium/Default media resolution:
US/Asia: 40 M
EU: 10 M
Low media resolution:
US/Asia: 10 M
EU: 2.6 M
image/png
image/jpeg
image/webp
image/heic
image/heif
Gemini 2.0 Flash-Lite	
Maximum images per prompt: 3,000
Maximum file size per file for inline data or direct uploads through the console: 7 MB
Maximum file size per file from Google Cloud Storage: 30 MB
Maximum tokens per minute (TPM):
High/Medium/Default media resolution:
US/Asia: 6.7 M
EU: 2.6 M
Low media resolution:
US/Asia: 2.6 M
EU: 2.6 M
image/png
image/jpeg
image/webp
image/heic
image/heif
For a list of languages supported by Gemini models, see model information Google models. To learn more about how to design multimodal prompts, see Design multimodal prompts. If you're looking for a way to use Gemini directly from your mobile and web apps, see the Firebase AI Logic client SDKs for Swift, Android, Web, Flutter, and Unity apps.

Add images to a request
You can add a single image or multiple images in your request to Gemini.

Single image
The sample code in each of the following tabs shows a different way to identify what's in an image. This sample works with all Gemini multimodal models.

Console
Python
Go
Node.js
Java
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateContent(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const image = {
    fileData: {
      fileUri: 'gs://cloud-samples-data/generative-ai/image/scones.jpg',
      mimeType: 'image/jpeg',
    },
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: [image, 'What is shown in this image?'],
  });

  console.log(response.text);

  return response.text;
}
Multiple images
Each of the following tabs show you a different way to include multiple images in a prompt request. Each sample takes in two sets of the following inputs:

An image of a popular city landmark
The media type of the image
Text indicating the city and landmark in the image
The sample also takes in a third image and media type, but no text. The sample returns a text response indicating the city and landmark in the third image.

These image samples work with all Gemini multimodal models.

Console
Python
Go
Node.js
Java
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateContent(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const image1 = {
    fileData: {
      fileUri: 'gs://cloud-samples-data/generative-ai/image/scones.jpg',
      mimeType: 'image/jpeg',
    },
  };

  const image2 = {
    fileData: {
      fileUri: 'gs://cloud-samples-data/generative-ai/image/fruit.png',
      mimeType: 'image/png',
    },
  };

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: [
      image1,
      image2,
      'Generate a list of all the objects contained in both images.',
    ],
  });

  console.log(response.text);

  return response.text;
}
Set optional model parameters
Each model has a set of optional parameters that you can set. For more information, see Content generation parameters.

Image tokenization
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms, and the Additional Terms for Generative AI Preview Products. You can process personal data for this product or feature as outlined in the Cloud Data Processing Addendum, subject to the obligations and restrictions described in the agreement under which you access Google Cloud. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

With Gemini 3 Pro and Gemini 3 Pro Image, image tokenization uses a variable sequence length which replaces the Pan and Scan method used in previous models for better quality and latency.

You can specify a media resolution for image and PDF inputs, which affects how images are tokenized and how many tokens are used for each image. You can set media_resolution in generationConfig to apply to all media in the request, or set it for individual media parts, which will override the top-level setting.

The following resolutions are available for Gemini 3 Pro:

MEDIA_RESOLUTION_HIGH: 1120 tokens for images and PDFs
MEDIA_RESOLUTION_MEDIUM: 560 tokens for images and PDFs
MEDIA_RESOLUTION_LOW: 280 tokens for images and PDFs
MEDIA_RESOLUTION_UNSPECIFIED: 1120 tokens for images and 560 for PDFs (default)
The following token counts apply to Gemini 3 Pro Image:

Input images: 560 tokens per image
Output images:
1K resolution: 1120 tokens per image (default)
2K resolution: 1120 tokens per image
4K resolution: 2000 tokens per image
For models earlier than Gemini 3 Pro, each image is processed using Pan and Scan and costs 258 tokens.

This code sample demonstrates how to adjust media_resolution:



from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model="gemini-3-pro-preview",
  contents=[
      types.Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png",
              mime_type="image/jpeg",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH
          ),
      ),
      Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4",
              mime_type="video/mp4",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW
          ),
      ),
      "When does the image appear in the video? What is the context?",
  ],
)
print(response.text)
Best practices

When using images, use the following best practices and information for the best results:

If you want to detect text in an image, use prompts with a single image to produce better results than prompts with multiple images.
If your prompt contains a single image, place the image before the text prompt in your request.
If your prompt contains multiple images, and you want to refer to them later in your prompt or have the model refer to them in the model response, it can help to give each image an index before the image. Use a b c or image 1 image 2 image 3 for your index. The following is an example of using indexed images in a prompt:

image 1 
image 2 
image 3 

Write a blogpost about my day using image 1 and image 2. Then, give me ideas
for tomorrow based on image 3.
Use images with higher resolution; they yield better results.
Include a few examples in the prompt.
Rotate images to their proper orientation before adding them to the prompt.
Avoid blurry images.
Limitations

While Gemini multimodal models are powerful in many multimodal use cases, it's important to understand the limitations of the models:

Content moderation: The models refuse to provide answers on images that violate our safety policies.
Spatial reasoning: The models aren't precise at locating text or objects in images. They might only return the approximated counts of objects.
Medical uses: The models aren't suitable for interpreting medical images (for example, x-rays and CT scans) or providing medical advice.
People recognition: The models aren't meant to be used to identify people who aren't celebrities in images.
Accuracy: The models might hallucinate or make mistakes when interpreting low-quality, rotated, or extremely low-resolution images. The models might also hallucinate when interpreting handwritten text in images documents.
What's next


Video understanding

bookmark_border
To see an example of video understanding, run the "YouTube Video Analysis with Gemini" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can add videos to Gemini requests to perform tasks that involve understanding the contents of the included videos. This page shows you how to add videos to your requests to Gemini in Vertex AI by using the Google Cloud console and the Vertex AI API.

Supported models
The following table lists the models that support video understanding:

Model	Media details	MIME types
Gemini 3 Pro preview	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
Default resolution tokens per frame: 70
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Pro	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Flash preview	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Flash-Lite preview	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Flash	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Flash-Lite	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.5 Flash with Live API native audio preview	
Standard resolution: 768 x 768
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.0 Flash with Live API preview	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
Maximum tokens per minute (TPM):
High/Medium/Default media resolution:
US/Asia: 37.9 M
EU: 9.5 M
Low media resolution:
US/Asia: 1 G
EU: 2.5 M
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.0 Flash with image generation preview	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
Maximum tokens per minute (TPM):
High/Medium/Default media resolution:
US/Asia: 37.9 M
EU: 9.5 M
Low media resolution:
US/Asia: 1 G
EU: 2.5 M
Gemini 2.0 Flash	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
Maximum tokens per minute (TPM):
High/Medium/Default media resolution:
US/Asia: 38 M
EU: 10 M
Low media resolution:
US/Asia: 10 M
EU: 2.5 M
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
Gemini 2.0 Flash-Lite	
Maximum video length (with audio): Approximately 45 minutes
Maximum video length (without audio): Approximately 1 hour
Maximum number of videos per prompt: 10
Maximum tokens per minute (TPM):
High/Medium/Default media resolution:
US/Asia: 6.3 M
EU: 3.2 M
Low media resolution:
US/Asia: 3.2 M
EU: 3.2 M
video/x-flv
video/quicktime
video/mpeg
video/mpegs
video/mpg
video/mp4
video/webm
video/wmv
video/3gpp
For a list of languages supported by Gemini models, see model information Google models. To learn more about how to design multimodal prompts, see Design multimodal prompts. If you're looking for a way to use Gemini directly from your mobile and web apps, see the Firebase AI Logic client SDKs for Swift, Android, Web, Flutter, and Unity apps.

Add videos to a request
You can add a single video or multiple videos in your request to Gemini and the video can include audio.

Single video
The sample code in each of the following tabs shows a different way to identify what's in a video. This sample works with all Gemini multimodal models.

Console
Python
Go
Java
Node.js
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateText(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: [
      {
        role: 'user',
        parts: [
          {
            fileData: {
              mimeType: 'video/mp4',
              fileUri:
                'gs://cloud-samples-data/generative-ai/video/ad_copy_from_video.mp4',
            },
          },
          {
            text: 'What is in the video?',
          },
        ],
      },
    ],
  });

  console.log(response.text);

  // Example response:
  // The video shows several people surfing in an ocean with a coastline in the background. The camera ...

  return response.text;
}
Video with audio
The following shows you how to summarize a video file with audio and return chapters with timestamps. This sample works with Gemini 2.0.

Python
REST
Console
Install


pip install --upgrade google-genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



from google import genai
from google.genai.types import HttpOptions, Part

client = genai.Client(http_options=HttpOptions(api_version="v1"))
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[
        Part.from_uri(
            file_uri="gs://cloud-samples-data/generative-ai/video/ad_copy_from_video.mp4",
            mime_type="video/mp4",
        ),
        "What is in the video?",
    ],
)
print(response.text)
# Example response:
# The video shows several people surfing in an ocean with a coastline in the background. The camera ...
Customize video processing
You can customize video processing in the Gemini for Google Cloud API by setting clipping intervals or providing custom frame rate sampling.

Tip: Video clipping and frames per second (FPS) are supported by all models, but the quality is significantly higher from 2.5 series models.
Set clipping intervals
You can clip videos by specifying videoMetadata with start and end offsets.

Set a custom frame rate
You can set custom frame rate sampling by passing an fps argument to videoMetadata.

By default 1 frame per second (FPS) is sampled from the video. You might want to set low FPS (< 1) for long videos. This is especially useful for mostly static videos (for example, lectures). If you want to capture more details in rapidly changing visuals, consider setting a higher FPS value.

Adjust media resolution
You can adjust MediaResolution to process your videos with fewer tokens.

Set optional model parameters
Each model has a set of optional parameters that you can set. For more information, see Content generation parameters.

Video tokenization
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms, and the Additional Terms for Generative AI Preview Products. You can process personal data for this product or feature as outlined in the Cloud Data Processing Addendum, subject to the obligations and restrictions described in the agreement under which you access Google Cloud. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

With Gemini 3 Pro, video tokenization uses a variable sequence length which replaces the Pan and Scan method used in previous models for better quality and latency.

You can specify a media resolution for video inputs, which affects how videos are tokenized and how many tokens are used for each video. You can set media_resolution in generationConfig to apply to all media in the request, or set it for individual media parts, which will override the top-level setting. The default resolution for videos is 70 tokens per frame.

The following resolutions are available for Gemini 3 Pro:

MEDIA_RESOLUTION_HIGH: 280 tokens per frame
MEDIA_RESOLUTION_MEDIUM: 70 tokens per frame
MEDIA_RESOLUTION_LOW: 70 tokens per frame
MEDIA_RESOLUTION_UNSPECIFIED: 70 tokens per frame (default)
For models earlier than Gemini 3 Pro, each frame is tokenized at 258 tokens per frame for default resolution, or 66 tokens per frame for low resolution.

This code sample demonstrates how to adjust media_resolution:



from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model="gemini-3-pro-preview",
  contents=[
      types.Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png",
              mime_type="image/jpeg",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH
          ),
      ),
      Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4",
              mime_type="video/mp4",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW
          ),
      ),
      "When does the image appear in the video? What is the context?",
  ],
)
print(response.text)
Best practices

When using video, use the following best practices and information for the best results:

If your prompt contains a single video, place the video before the text prompt.
If you require timestamp localization in a video with audio, ask the model to generate timestamps that follow the format as described in "Timestamp format".
Limitations

While Gemini multimodal models are powerful in many multimodal use cases, it's important to understand the limitations of the models:

Content moderation: The models refuse to provide answers on videos that violate our safety policies.
Non-speech sound recognition: The models that support audio might make mistakes recognizing sound that's not speech.
Technical details about videos
File API processing: When using the File API, videos are sampled at 1 frame per second (FPS) and audio is processed at 1Kbps (single channel). Timestamps are added every second.

These rates are subject to change in the future for improvements in inference.
Timestamp format: When referring to specific moments in a video within your prompt, the timestamp format depends on your video's frame per second (FPS) sampling rate:

For sampling rates at 1 FPS or below: Use the MM:SS format, where the first two digits represent minutes and the last two digits represent seconds. If you have offsets that are greater than 1 hour, use the H:MM:SS format.

For sampling rates above 1 FPS: Use the MM:SS.sss format, or, if you have offsets that are greater than 1 hour, use the H:MM:SS.sss format, described as follows:

The first digit represents the hour.
The second two digits two digits represent minutes.
The third two digits represent seconds.
The final three digits represent subseconds.
Best practices:

Use only one video per prompt request for optimal results.

If combining text and a single video, place the text prompt after the video part in the contents array.

Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

Audio understanding (speech only)

bookmark_border
To see an example of audio understanding, run the "Multimodal Sentiment Analysis with Gemini" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can add audio to Gemini requests to perform tasks that involve understanding the contents of the included audio. This page shows you how to add audio to your requests to Gemini in Vertex AI by using the Google Cloud console and the Vertex AI API.

Supported models
The following table lists the models that support audio understanding:

Model	Media details	MIME types
Gemini 3 Pro preview	
Maximum audio length per prompt: Approximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Pro	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Flash preview	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Flash-Lite preview	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Flash	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Flash-Lite	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.5 Flash with Live API native audio preview	
Maximum conversation length: Default 10 minutes that can be extended.
Required audio input format: Raw 16-bit PCM audio at 16kHz, little-endian
Required audio output format: Raw 16-bit PCM audio at 24kHz, little-endian
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.0 Flash with Live API preview	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
Maximum tokens per minute (TPM):
US/Asia: 1.7 M
EU: 0.4 M
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.0 Flash with image generation preview	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
Maximum tokens per minute (TPM):
US/Asia: 1.7 M
EU: 0.4 M
Gemini 2.0 Flash	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
Maximum tokens per minute (TPM):
US/Asia: 3.5 M
EU: 3.5 M
audio/x-aac
audio/flac
audio/mp3
audio/m4a
audio/mpeg
audio/mpga
audio/mp4
audio/ogg
audio/pcm
audio/wav
audio/webm
Gemini 2.0 Flash-Lite	
Maximum audio length per prompt: Appropximately 8.4 hours, or up to 1 million tokens
Maximum number of audio files per prompt: 1
Speech understanding for: Audio summarization, transcription, and translation
Maximum tokens per minute (TPM):
US/Asia: 3.5 M
EU: 3.5 M
For a list of languages supported by Gemini models, see model information Google models. To learn more about how to design multimodal prompts, see Design multimodal prompts. If you're looking for a way to use Gemini directly from your mobile and web apps, see the Firebase AI Logic client SDKs for Swift, Android, Web, Flutter, and Unity apps.

Add audio to a request
You can add audio files in your requests to Gemini.

Single audio
The following shows you how to use an audio file to summarize a podcast:

Console
Python
Go
Node.js
Java
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateText(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const prompt =
    'Provide a concise summary of the main points in the audio file.';

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: [
      {
        fileData: {
          fileUri: 'gs://cloud-samples-data/generative-ai/audio/pixel.mp3',
          mimeType: 'audio/mpeg',
        },
      },
      {text: prompt},
    ],
  });

  console.log(response.text);

  // Example response:
  //  Here's a summary of the main points from the audio file:
  //  The Made by Google podcast discusses the Pixel feature drops with product managers Aisha Sheriff and De Carlos Love.  The key idea is that devices should improve over time, with a connected experience across phones, watches, earbuds, and tablets.

  return response.text;
}
Audio transcription
The following shows you how to use an audio file to transcribe an interview. To enable timestamp understanding for audio-only files, enable the audioTimestamp parameter in GenerationConfig:

Console
Python
Go
Node.js
Java
REST
Install


npm install @google/genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



const {GoogleGenAI} = require('@google/genai');

const GOOGLE_CLOUD_PROJECT = process.env.GOOGLE_CLOUD_PROJECT;
const GOOGLE_CLOUD_LOCATION = process.env.GOOGLE_CLOUD_LOCATION || 'global';

async function generateText(
  projectId = GOOGLE_CLOUD_PROJECT,
  location = GOOGLE_CLOUD_LOCATION
) {
  const client = new GoogleGenAI({
    vertexai: true,
    project: projectId,
    location: location,
  });

  const prompt = `Transcribe the interview, in the format of timecode, speaker, caption.
    Use speaker A, speaker B, etc. to identify speakers.`;

  const response = await client.models.generateContent({
    model: 'gemini-2.5-flash',
    contents: [
      {text: prompt},
      {
        fileData: {
          fileUri: 'gs://cloud-samples-data/generative-ai/audio/pixel.mp3',
          mimeType: 'audio/mpeg',
        },
      },
    ],
    // Required to enable timestamp understanding for audio-only files
    config: {
      audioTimestamp: true,
    },
  });

  console.log(response.text);

  // Example response:
  // [00:00:00] **Speaker A:** your devices are getting better over time. And so ...
  // [00:00:14] **Speaker B:** Welcome to the Made by Google podcast where we meet ...
  // [00:00:20] **Speaker B:** Here's your host, Rasheed Finch.
  // [00:00:23] **Speaker C:** Today we're talking to Aisha Sharif and DeCarlos Love. ...
  // ...

  return response.text;
}
Set optional model parameters
Each model has a set of optional parameters that you can set. For more information, see Content generation parameters.

Limitations

While Gemini multimodal models are powerful in many multimodal use cases, it's important to understand the limitations of the models:

Non-speech sound recognition: The models that support audio might make mistakes recognizing sound that's not speech.
Audio-only timestamps: To accurately generate timestamps for audio-only files, you must configure the audio_timestamp parameter in generation_config.

Document understanding

bookmark_border
To see an example of document understanding, run the "Document Processing with Gemini" notebook in one of the following environments:

Open in Colab | Open in Colab Enterprise | Open in Vertex AI Workbench | View on GitHub

You can add documents (PDF and TXT files) to Gemini requests to perform tasks that involve understanding the contents of the included documents. This page shows you how to add PDFs to your requests to Gemini in Vertex AI by using the Google Cloud console and the Vertex AI API.

Supported models
The following table lists the models that support document understanding:

Model	Media details	MIME types
Gemini 3 Pro preview	
Maximum number of files per prompt: 900
Maximum number of pages per file: 900
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
Default resolution tokens: 560
OCR for scanned PDFs: Not used by default
application/pdf
text/plain
Gemini 3 Pro Image preview	
Maximum number of files per prompt: As supported by the 65,536 token context window
Maximum number of pages per file: As supported by the 65,536 token context window
Maximum file size per file: 50 MB (API and Cloud Storage imports) or 7 MB (direct upload through Google Cloud console)
application/pdf
text/plain
Gemini 2.5 Pro	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
application/pdf
text/plain
Gemini 2.5 Flash preview	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
application/pdf
text/plain
Gemini 2.5 Flash-Lite preview	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file: 50 MB
application/pdf
text/plain
Gemini 2.5 Flash Image	
Maximum number of files per prompt: 3
Maximum number of pages per file: 3
Maximum file size per file: 50 MB (API and Cloud Storage imports) or 7 MB (direct upload through Google Cloud console)
application/pdf
text/plain
Gemini 2.5 Flash	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
application/pdf
text/plain
Gemini 2.5 Flash-Lite	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file: 50 MB
application/pdf
text/plain
Gemini 2.0 Flash with image generation preview	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file: 50 MB
application/pdf
text/plain
Gemini 2.0 Flash	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
Maximum tokens per minute (TPM) per project1:
US/Asia: 3.4 M
EU: 3.4 M
application/pdf
text/plain
Gemini 2.0 Flash-Lite	
Maximum number of files per prompt: 3,000
Maximum number of pages per file: 1,000
Maximum file size per file for the API or Cloud Storage imports: 50 MB
Maximum file size per file for direct uploads through the console: 7 MB
Maximum tokens per minute (TPM) per project1:
US/Asia: 3.4 M
EU: 3.4 M
1This is the maximum TPM from document inputs across all requests of a project. Also use the maximum TPM for other modalities.

The quota metric is generate_content_document_input_per_base_model_id_and_resolution.

For a list of languages supported by Gemini models, see model information Google models. To learn more about how to design multimodal prompts, see Design multimodal prompts. If you're looking for a way to use Gemini directly from your mobile and web apps, see the Firebase AI Logic client SDKs for Swift, Android, Web, Flutter, and Unity apps.

Add documents to a request
The following code sample shows you how to include a PDF in a prompt request. This PDF sample works with all Gemini multimodal models.

Python
Go
Java
Node.js
REST
Console
After you set up your environment, you can use REST to test a text prompt. The following sample sends a request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

PROJECT_ID: Your project ID.
FILE_URI: The URI or URL of the file to include in the prompt. Acceptable values include the following:
Cloud Storage bucket URI: The object must either be publicly readable or reside in the same Google Cloud project that's sending the request. For gemini-2.0-flash and gemini-2.0-flash-lite, the size limit is 2 GB.
HTTP URL: The file URL must be publicly readable. You can specify one video file, one audio file, and up to 10 image files per request. Audio files, video files, and documents can't exceed 15 MB.
YouTube video URL:The YouTube video must be either owned by the account that you used to sign in to the Google Cloud console or is public. Only one YouTube video URL is supported per request.
When specifying a fileURI, you must also specify the media type (mimeType) of the file. If VPC Service Controls is enabled, specifying a media file URL for fileURI is not supported.

If you don't have a PDF file in Cloud Storage, then you can use the following publicly available file: gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf with a mime type of application/pdf. To view this PDF, open the sample PDF file.

MIME_TYPE: The media type of the file specified in the data or fileUri fields. Acceptable values include the following:
Click to expand MIME types

TEXT: The text instructions to include in the prompt. For example, You are a very professional document summarization specialist. Please summarize the given document.
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json. Run the following command in the terminal to create or overwrite this file in the current directory:



cat > request.json << 'EOF'
{
  "contents": {
    "role": "USER",
    "parts": [
      {
        "fileData": {
          "fileUri": "FILE_URI",
          "mimeType": "MIME_TYPE"
        }
      },
      {
        "text": "TEXT"
      }
    ]
  }
}
EOF
Then execute the following command to send your REST request:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/global/publishers/google/models/gemini-2.0-flash:generateContent"
You should receive a JSON response similar to the following.

Response
Note the following in the URL for this sample:
Use the generateContent method to request that the response is returned after it's fully generated. To reduce the perception of latency to a human audience, stream the response as it's being generated by using the streamGenerateContent method.
The multimodal model ID is located at the end of the URL before the method (for example, gemini-2.0-flash). This sample might support other models as well.
Set optional model parameters
Each model has a set of optional parameters that you can set. For more information, see Content generation parameters.

Document tokenization
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms, and the Additional Terms for Generative AI Preview Products. You can process personal data for this product or feature as outlined in the Cloud Data Processing Addendum, subject to the obligations and restrictions described in the agreement under which you access Google Cloud. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

With Gemini 3 Pro, document tokenization uses a variable sequence length which replaces the Pan and Scan method used in previous models for better quality and latency.

You can specify a media resolution for PDF inputs, which affects how documents are tokenized and how many tokens are used for each document. You can set media_resolution in generationConfig to apply to all media in the request, or set it for individual media parts, which will override the top-level setting.

The following resolutions are available for Gemini 3 Pro:

MEDIA_RESOLUTION_HIGH: 1120 tokens for PDFs
MEDIA_RESOLUTION_MEDIUM: 560 tokens for PDFs
MEDIA_RESOLUTION_LOW: 280 tokens for PDFs
MEDIA_RESOLUTION_UNSPECIFIED: 560 for PDFs (default)
This code sample demonstrates how to adjust media_resolution:



from google import genai
from google.genai import types

client = genai.Client()

response = client.models.generate_content(
  model="gemini-3-pro-preview",
  contents=[
      types.Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/image/a-man-and-a-dog.png",
              mime_type="image/jpeg",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_HIGH
          ),
      ),
      Part(
          file_data=types.FileData(
              file_uri="gs://cloud-samples-data/generative-ai/video/behind_the_scenes_pixel.mp4",
              mime_type="video/mp4",
          ),
          media_resolution=types.PartMediaResolution(
              level=types.PartMediaResolutionLevel.MEDIA_RESOLUTION_LOW
          ),
      ),
      "When does the image appear in the video? What is the context?",
  ],
)
print(response.text)
PDF best practices

When using PDFs, use the following best practices and information for the best results:

If your prompt contains a single PDF, place the PDF before the text prompt in your request.
If you have a long document, consider splitting it into multiple PDFs to process it.
Use PDFs created with text rendered as text instead of using text in scanned images. This format ensures text is machine-readable so that it's easier for the model to edit, search, and manipulate compared to scanned image PDFs. This practice provides optimal results when working with text-heavy documents like contracts.
Limitations

While Gemini multimodal models are powerful in many multimodal use cases, it's important to understand the limitations of the models:

Spatial reasoning: The models aren't precise at locating text or objects in PDFs. They might only return the approximated counts of objects.
Accuracy: The models might hallucinate when interpreting handwritten text in PDF documents.






THIS IS THIS IS API API REFERENCEEEEEESSS

Generate content with the Gemini API in Vertex AI

bookmark_border
Use generateContent or streamGenerateContent to generate content with Gemini.

The Gemini model family includes models that work with multimodal prompt requests. The term multimodal indicates that you can use more than one modality, or type of input, in a prompt. Models that aren't multimodal accept prompts only with text. Modalities can include text, audio, video, and more.

Create a Google Cloud account to get started
To start using the Gemini API in Vertex AI, create a Google Cloud account.

After creating your account, use this document to review the Gemini model request body, model parameters, response body, and some sample requests.

When you're ready, see the Gemini API in Vertex AI quickstart to learn how to send a request to the Gemini API in Vertex AI using a programming language SDK or the REST API.

Supported models
All Gemini models support content generation.

Note: Adding a lot of images to a request increases response latency.
Parameter list
See examples for implementation details.

Request body


{
  "cachedContent": string,
  "contents": [
    {
      "role": string,
      "parts": [
        {
          // Union field data can be only one of the following:
          "text": string,
          "inlineData": {
            "mimeType": string,
            "data": string
          },
          "fileData": {
            "mimeType": string,
            "fileUri": string
          },
          // End of list of possible types for union field data.

          "videoMetadata": {
            "startOffset": {
              "seconds": integer,
              "nanos": integer
            },
            "endOffset": {
              "seconds": integer,
              "nanos": integer
            },
            "fps": double
          },
          "mediaResolution": enum
        }
      ]
    }
  ],
  "systemInstruction": {
    "role": string,
    "parts": [
      {
        "text": string
      }
    ]
  },
  "tools": [
    {
      "functionDeclarations": [
        {
          "name": string,
          "description": string,
          "parameters": {
            object (OpenAPI Object Schema)
          }
        }
      ]
    }
  ],
  "safetySettings": [
    {
      "category": enum (HarmCategory),
      "threshold": enum (HarmBlockThreshold)
    }
  ],
  "generationConfig": {
    "temperature": number,
    "topP": number,
    "topK": number,
    "candidateCount": integer,
    "maxOutputTokens": integer,
    "presencePenalty": float,
    "frequencyPenalty": float,
    "stopSequences": [
      string
    ],
    "responseMimeType": string,
    "responseSchema": schema,
    "seed": integer,
    "responseLogprobs": boolean,
    "logprobs": integer,
    "audioTimestamp": boolean,
    "thinkingConfig": {
      "thinkingBudget": integer,
      "thinkingLevel": enum
    },
    "mediaResolution": enum
  },
  "labels": {
    string: string
  }
}
The request body contains data with the following parameters:

Parameters
cachedContent

Optional: string

The name of the cached content used as context to serve the prediction. Format: projects/{project}/locations/{location}/cachedContents/{cachedContent}

contents

Required: Content

The content of the current conversation with the model.

For single-turn queries, this is a single instance. For multi-turn queries, this is a repeated field that contains conversation history and the latest request.

systemInstruction

Optional: Content

Available for gemini-2.0-flash and gemini-2.0-flash-lite.

Instructions for the model to steer it toward better performance. For example, "Answer as concisely as possible" or "Don't use technical terms in your response".

The text strings count toward the token limit.

The role field of systemInstruction is ignored and doesn't affect the performance of the model.

Note: Only text should be used in parts and content in each part should be in a separate paragraph.
tools

Optional. A piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of knowledge and scope of the model. See Function calling.

toolConfig

Optional. See Function calling.

safetySettings

Optional: SafetySetting

Per request settings for blocking unsafe content.

Enforced on GenerateContentResponse.candidates.

generationConfig

Optional: GenerationConfig

Generation configuration settings.

labels

Optional: string

Metadata that you can add to the API call in the format of key-value pairs.

contents
The base structured data type containing multi-part content of a message.

This class consists of two main properties: role and parts. The role property denotes the individual producing the content, while the parts property contains multiple elements, each representing a segment of data within a message.

Parameters
role

string

The identity of the entity that creates the message. The following values are supported:

user: This indicates that the message is sent by a real person, typically a user-generated message.
model: This indicates that the message is generated by the model.
The model value is used to insert messages from the model into the conversation during multi-turn conversations.

parts

Part

A list of ordered parts that make up a single message. Different parts may have different IANA MIME types.

For limits on the inputs, such as the maximum number of tokens or the number of images, see the model specifications on the Google models page.

To compute the number of tokens in your request, see Get token count.

parts
A data type containing media that is part of a multi-part Content message.

Parameters
text

Optional: string

A text prompt or code snippet.

inlineData

Optional: Blob

Inline data in raw bytes.

For gemini-2.0-flash-lite and gemini-2.0-flash, you can specify up to 3000 images by using inlineData.

fileData

Optional: fileData

Data stored in a file.

functionCall

Optional: FunctionCall.

It contains a string representing the FunctionDeclaration.name field and a structured JSON object containing any parameters for the function call predicted by the model.

See Function calling.

functionResponse

Optional: FunctionResponse.

The result output of a FunctionCall that contains a string representing the FunctionDeclaration.name field and a structured JSON object containing any output from the function call. It is used as context to the model.

See Function calling.

videoMetadata

Optional: VideoMetadata

For video input, the start and end offset of the video in Duration format, and the frame rate of the video . For example, to specify a 10 second clip starting at 1:00 with a frame rate of 10 frames per second, set the following:

"startOffset": { "seconds": 60 }
"endOffset": { "seconds": 70 }
"fps": 10.0
The metadata should only be specified while the video data is presented in inlineData or fileData.

mediaResolution

Optional: enum

Controls how input media is processed. If specified, this overrides the mediaResolution setting in generationConfig. LOW reduces tokens per image/video, possibly losing detail but allowing longer videos in context. Supported values: HIGH, MEDIUM, LOW.

blob
Content blob. If possible send as text rather than raw bytes.

Parameters
mimeType

string

The media type of the file specified in the data or fileUri fields. Acceptable values include the following:
Click to expand MIME types

For gemini-2.0-flash-lite and gemini-2.0-flash, the maximum length of an audio file is 8.4 hours and the maximum length of a video file (without audio) is one hour. For more information, see Gemini audio and video requirements.

Text files must be UTF-8 encoded. The contents of the text file count toward the token limit.

There is no limit on image resolution.

data

bytes

The base64 encoding of the image, PDF, or video to include inline in the prompt. When including media inline, you must also specify the media type (mimeType) of the data.

Size limit: 7 MB for images

FileData
URI or web-URL data.

Parameters
mimeType

string

IANA MIME type of the data.

fileUri

string

The URI or URL of the file to include in the prompt. Acceptable values include the following:

Cloud Storage bucket URI: The object must either be publicly readable or reside in the same Google Cloud project that's sending the request. For gemini-2.0-flash and gemini-2.0-flash-lite, the size limit is 2 GB.
HTTP URL: The file URL must be publicly readable. You can specify one video file, one audio file, and up to 10 image files per request. Audio files, video files, and documents can't exceed 15 MB.
YouTube video URL:The YouTube video must be either owned by the account that you used to sign in to the Google Cloud console or is public. Only one YouTube video URL is supported per request.
When specifying a fileURI, you must also specify the media type (mimeType) of the file. If VPC Service Controls is enabled, specifying a media file URL for fileURI is not supported.

functionCall
A predicted functionCall returned from the model that contains a string representing the functionDeclaration.name and a structured JSON object containing the parameters and their values.

Parameters
name

string

The name of the function to call.

args

Struct

The function parameters and values in JSON object format.

See Function calling for parameter details.

functionResponse
The resulting output from a FunctionCall that contains a string representing the FunctionDeclaration.name. Also contains a structured JSON object with the output from the function (and uses it as context for the model). This should contain the result of a FunctionCall made based on model prediction.

Parameters
name

string

The name of the function to call.

response

Struct

The function response in JSON object format.

videoMetadata
Metadata describing the input video content.

Parameters
startOffset

Optional: google.protobuf.Duration

The start offset of the video.

endOffset

Optional: google.protobuf.Duration

The end offset of the video.

fps

Optional: double

The frame rate of the video sent to the model. Defaults to 1.0 if not specified. The minimum accepted value is as low as, but not including, 0.0. The maximum value is 24.0.

safetySetting
Safety settings.

Parameters
category

Optional: HarmCategory

The safety category to configure a threshold for. Acceptable values include the following:

Click to expand safety categories

HARM_CATEGORY_SEXUALLY_EXPLICIT
HARM_CATEGORY_HATE_SPEECH
HARM_CATEGORY_HARASSMENT
HARM_CATEGORY_DANGEROUS_CONTENT
threshold

Optional: HarmBlockThreshold

The threshold for blocking responses that could belong to the specified safety category based on probability.

OFF
BLOCK_NONE
BLOCK_LOW_AND_ABOVE
BLOCK_MEDIUM_AND_ABOVE
BLOCK_ONLY_HIGH
method

Optional: HarmBlockMethod

Specify if the threshold is used for probability or severity score. If not specified, the threshold is used for probability score.

harmCategory
Harm categories that block content.

Parameters
HARM_CATEGORY_UNSPECIFIED

The harm category is unspecified.

HARM_CATEGORY_HATE_SPEECH

The harm category is hate speech.

HARM_CATEGORY_DANGEROUS_CONTENT

The harm category is dangerous content.

HARM_CATEGORY_HARASSMENT

The harm category is harassment.

HARM_CATEGORY_SEXUALLY_EXPLICIT

The harm category is sexually explicit content.

harmBlockThreshold
Probability thresholds levels used to block a response.

Parameters
HARM_BLOCK_THRESHOLD_UNSPECIFIED

Unspecified harm block threshold.

BLOCK_LOW_AND_ABOVE

Block low threshold and higher (i.e. block more).

BLOCK_MEDIUM_AND_ABOVE

Block medium threshold and higher.

BLOCK_ONLY_HIGH

Block only high threshold (i.e. block less).

BLOCK_NONE

Block none.

OFF

Switches off safety if all categories are turned OFF

harmBlockMethod
A probability threshold that blocks a response based on a combination of probability and severity.

Parameters
HARM_BLOCK_METHOD_UNSPECIFIED

The harm block method is unspecified.

SEVERITY

The harm block method uses both probability and severity scores.

PROBABILITY

The harm block method uses the probability score.

generationConfig
Configuration settings used when generating the prompt.

Parameters
temperature

Optional: float

The temperature is used for sampling during response generation, which occurs when topP and topK are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.

If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature. If the model enters infinite generation, increasing the temperature to at least 0.1 may lead to improved results.

1.0 is the recommended starting value for temperature.

Range for gemini-2.0-flash-lite: 0.0 - 2.0 (default: 1.0)
Range for gemini-2.0-flash: 0.0 - 2.0 (default: 1.0)
For more information, see Content generation parameters.

topP

Optional: float

If specified, nucleus sampling is used.

Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is 0.5, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.

Specify a lower value for less random responses and a higher value for more random responses.

Range: 0.0 - 1.0
Default for gemini-2.0-flash-lite: 0.95
Default for gemini-2.0-flash: 0.95
candidateCount

Optional: int

The number of response variations to return. For each request, you're charged for the output tokens of all candidates, but are only charged once for the input tokens.

Specifying multiple candidates is a Preview feature that works with generateContent (streamGenerateContent is not supported). The following models are supported:

gemini-2.0-flash-lite: 1-8, default: 1
gemini-2.0-flash: 1-8, default: 1
maxOutputTokens

Optional: int

Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.

Specify a lower value for shorter responses and a higher value for potentially longer responses.

For more information, see Content generation parameters.

stopSequences

Optional: List[string]

Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. If a string appears multiple times in the response, then the response truncates where it's first encountered. The strings are case-sensitive.

For example, if the following is the returned response when stopSequences isn't specified:

public static string reverse(string myString)

Then the returned response with stopSequences set to ["Str", "reverse"] is:

public static string

Maximum 5 items in the list.

For more information, see Content generation parameters.

presencePenalty

Optional: float

Positive penalties.

Positive values penalize tokens that already appear in the generated text, increasing the probability of generating more diverse content.

The maximum value for presencePenalty is up to, but not including, 2.0. Its minimum value is -2.0.

frequencyPenalty

Optional: float

Positive values penalize tokens that repeatedly appear in the generated text, decreasing the probability of repeating content.

This maximum value for frequencyPenalty is up to, but not including, 2.0. Its minimum value is -2.0.

responseMimeType

Optional: string (enum)

The output response MIME type of the generated candidate text.

The following MIME types are supported:

application/json: JSON response in the candidates.
text/plain (default): Plain text output.
text/x.enum: For classification tasks, output an enum value as defined in the response schema.
Specify the appropriate response type to avoid unintended behaviors. For example, if you require a JSON-formatted response, specify application/json and not text/plain.

text/plain isn't supported for use with responseSchema.
responseSchema

Optional: schema

The schema that generated candidate text must follow. For more information, see Control generated output.

To use this parameter, you must specify a supported mime type other than text/plain for the responseMimeType parameter.

seed

Optional: int

When seed is fixed to a specific value, the model makes a best effort to provide the same response for repeated requests. Deterministic output isn't guaranteed. Also, changing the model or parameter settings, such as the temperature, can cause variations in the response even when you use the same seed value. By default, a random seed value is used.

responseLogprobs

Optional: boolean

If true, returns the log probabilities of the tokens that were chosen by the model at each step. By default, this parameter is set to false.

logprobs

Optional: int

Returns the log probabilities of the top candidate tokens at each generation step. The model's chosen token might not be the same as the top candidate token at each step. Specify the number of candidates to return by using an integer value in the range of 1-20.

You must enable responseLogprobs to use this parameter.

audioTimestamp

Optional: boolean

Available for the following models:

Gemini 2.0 Flash-Lite
Gemini 2.0 Flash
Enables timestamp understanding for audio-only files.

This is a preview feature.

thinkingConfig

Optional: object

Configuration for the model's thinking process for Gemini 2.5 and higher models.

The thinkingConfig object contains the following fields:

thinkingBudget: integer. By default, the model automatically controls how much it thinks up to a maximum of 8,192 tokens.
thinkingLevel: enum. Controls the amount of internal reasoning the model performs before generating a response. Higher levels may improve quality on complex tasks but increase latency and cost. Supported values are LOW and HIGH.
mediaResolution

Optional: enum

Controls how input media is processed. LOW reduces tokens per image/video, possibly losing detail but allowing longer videos in context. Supported values: HIGH, MEDIUM, LOW.

Response body


{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": string
          }
        ]
      },
      "finishReason": enum (FinishReason),
      "safetyRatings": [
        {
          "category": enum (HarmCategory),
          "probability": enum (HarmProbability),
          "blocked": boolean
        }
      ],
      "citationMetadata": {
        "citations": [
          {
            "startIndex": integer,
            "endIndex": integer,
            "uri": string,
            "title": string,
            "license": string,
            "publicationDate": {
              "year": integer,
              "month": integer,
              "day": integer
            }
          }
        ]
      },
      "avgLogprobs": double,
      "logprobsResult": {
        "topCandidates": [
          {
            "candidates": [
              {
                "token": string,
                "logProbability": float
              }
            ]
          }
        ],
        "chosenCandidates": [
          {
            "token": string,
            "logProbability": float
          }
        ]
      }
    }
  ],
  "usageMetadata": {
    "promptTokenCount": integer,
    "candidatesTokenCount": integer,
    "totalTokenCount": integer
  },
  "modelVersion": string
}
Response element	Description
modelVersion	The model and version used for generation. For example: gemini-2.0-flash-lite-001.
text	The generated text.
finishReason	The reason why the model stopped generating tokens. If empty, the model has not stopped generating the tokens. Because the response uses the prompt for context, it's not possible to change the behavior of how the model stops generating tokens.
FINISH_REASON_STOP: Natural stop point of the model or provided stop sequence.
FINISH_REASON_MAX_TOKENS: The maximum number of tokens as specified in the request was reached.
FINISH_REASON_SAFETY: Token generation was stopped because the response was flagged for safety reasons. Note that Candidate.content is empty if content filters block the output.
FINISH_REASON_RECITATION: The token generation was stopped because the response was flagged for unauthorized citations.
FINISH_REASON_BLOCKLIST: Token generation was stopped because the response includes blocked terms.
FINISH_REASON_PROHIBITED_CONTENT: Token generation was stopped because the response was flagged for prohibited content, such as child sexual abuse material (CSAM).
FINISH_REASON_IMAGE_PROHIBITED_CONTENT: Token generation was stopped because the image provided in the prompt was flagged for prohibited content.
FINISH_REASON_NO_IMAGE: Token generation was stopped because an image was expected in the prompt, but none was provided.
FINISH_REASON_SPII: Token generation was stopped because the response was flagged for sensitive personally identifiable information (SPII).
FINISH_REASON_MALFORMED_FUNCTION_CALL: Candidates were blocked because of malformed and unparsable function call.
FINISH_REASON_OTHER: All other reasons that stopped the token
FINISH_REASON_UNSPECIFIED: The finish reason is unspecified.
category	The safety category to configure a threshold for. Acceptable values include the following:
Click to expand safety categories

probability	The harm probability levels in the content.
HARM_PROBABILITY_UNSPECIFIED
NEGLIGIBLE
LOW
MEDIUM
HIGH
blocked	A boolean flag associated with a safety attribute that indicates if the model's input or output was blocked.
startIndex	An integer that specifies where a citation starts in the content. The startIndex is in bytes and calculated from the response encoded in UTF-8.
endIndex	An integer that specifies where a citation ends in the content. The endIndex is in bytes and calculated from the response encoded in UTF-8.
url	The URL of a citation source. Examples of a URL source might be a news website or a GitHub repository.
title	The title of a citation source. Examples of source titles might be that of a news article or a book.
license	The license associated with a citation.
publicationDate	The date a citation was published. Its valid formats are YYYY, YYYY-MM, and YYYY-MM-DD.
avgLogprobs	Average log probability of the candidate.
logprobsResult	Returns the top candidate tokens (topCandidates) and the actual chosen tokens (chosenCandidates) at each step.
token	Generative AI models break down text data into tokens for processing, which can be characters, words, or phrases.
logProbability	A log probability value that indicates the model's confidence for a particular token.
promptTokenCount	Number of tokens in the request.
candidatesTokenCount	Number of tokens in the response(s).
totalTokenCount	Number of tokens in the request and response(s).
Note: For billing purposes, tokens consumed by document inputs to Gemini 3 Pro and later models are counted as image tokens.
Examples
Text Generation
Generate a text response from a text input.

Gen AI SDK for Python
Python (OpenAI)
Go



from google import genai
from google.genai.types import HttpOptions

client = genai.Client(http_options=HttpOptions(api_version="v1"))
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How does AI work?",
)
print(response.text)
# Example response:
# Okay, let's break down how AI works. It's a broad field, so I'll focus on the ...
#
# Here's a simplified overview:
# ...
Using multimodal prompt
Generate a text response from a multimodal input, such as text and an image.

Gen AI SDK for Python
Python (OpenAI)
Go



from google import genai
from google.genai.types import HttpOptions, Part

client = genai.Client(http_options=HttpOptions(api_version="v1"))
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=[
        "What is shown in this image?",
        Part.from_uri(
            file_uri="gs://cloud-samples-data/generative-ai/image/scones.jpg",
            mime_type="image/jpeg",
        ),
    ],
)
print(response.text)
# Example response:
# The image shows a flat lay of blueberry scones arranged on parchment paper. There are ...
Streaming text response
Generate a streaming model response from a text input.

Gen AI SDK for Python
Python (OpenAI)
Go



from google import genai
from google.genai.types import HttpOptions

client = genai.Client(http_options=HttpOptions(api_version="v1"))

for chunk in client.models.generate_content_stream(
    model="gemini-2.5-flash",
    contents="Why is the sky blue?",
):
    print(chunk.text, end="")
# Example response:
# The
#  sky appears blue due to a phenomenon called **Rayleigh scattering**. Here's
#  a breakdown of why:
# ...
Model versions
To use the auto-updated version, specify the model name without the trailing version number, for example gemini-2.0-flash instead of gemini-2.0-flash-001.

For more information, see Gemini model versions and lifecycle.

Function calling reference

bookmark_border
Function calling improves the LLM's ability to provide relevant and contextual answers.

You can provide custom functions to a generative AI model with the Function Calling API. The model doesn't directly invoke these functions, but instead generates structured data output that specifies the function name and suggested arguments.

This output enables the calling of external APIs or information systems such as databases, customer relationship management systems, and document repositories. The resulting API output can be used by the LLM to improve response quality.

For more conceptual documentation on function calling, see Function calling.

Supported models
Gemini 3 Pro preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.5 Flash with Live API native audio preview
Gemini 2.0 Flash with Live API preview
Gemini 2.0 Flash
Gemini 2.0 Flash-Lite
Limitations:

The maximum number of function declarations that can be provided with the request is 128.
Example syntax
Syntax to send a function call API request.

curl


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \

https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent \
-d '{
  "contents": [{
    ...
  }],
  "tools": [{
    "function_declarations": [
      {
        ...
      }
    ]
  }]
}'
Parameter list
See examples for implementation details.

FunctionDeclaration
Defines a function that the model can generate JSON inputs for based on OpenAPI 3.0 specifications.

Parameters
name

string

The name of the function to call. Must start with a letter or an underscore. Must be a-z, A-Z, 0-9, or contains underscores, dots, or dashes, with a maximum length of 64.

description

Optional: string

The description and purpose of the function. The model uses this to decide how and whether to call the function. For the best results, we recommend that you include a description.

parameters

Optional: Schema

Describes the parameters of the function in the OpenAPI JSON Schema Object format: OpenAPI 3.0 specification.

response

Optional: Schema

Describes the output from the function in the OpenAPI JSON Schema Object format: OpenAPI 3.0 specification.

For more information, see Function calling

Schema
Defines the format of the input and output data in a function call based on the OpenAPI 3.0 Schema specification.

Parameters
type	
string

Enum. The type of the data. Must be one of:

STRING
INTEGER
BOOLEAN
NUMBER
ARRAY
OBJECT
description	
Optional: string

Description of the data.

enum	
Optional: string[]

Possible values of the element of primitive type with enum format.

items	
Optional: Schema[]

Schema of the elements of Type.ARRAY

properties	
Optional: Schema

Schema of the properties of Type.OBJECT

required	
Optional: string[]

Required properties of Type.OBJECT.

nullable	
Optional: bool

Indicates if the value may be null.

FunctionCallingConfig
The FunctionCallingConfig controls the behavior of the model and determines what type of function to call.

Parameters
mode

Optional: enum/string[]

AUTO: Default model behavior. The model can make predictions in either a function call form or a natural language response form. The model decides which form to use based on the context.
NONE: The model doesn't make any predictions in the form of function calls.
ANY: The model is constrained to always predict a function call. If allowed_function_names is not provided, the model picks from all of the available function declarations. If allowed_function_names is provided, the model picks from the set of allowed functions.
allowed_function_names

Optional: string[]

Function names to call. Only set when the mode is ANY. Function names should match [FunctionDeclaration.name]. With mode set to ANY, the model will predict a function call from the set of function names provided.

stream_function_call_arguments

Optional: boolean

If true, function call arguments are streamed back incrementally in chunks. Default is false.

functionCall
A predicted functionCall returned from the model that contains a string representing the functionDeclaration.name and a structured JSON object containing the parameters and their values.

Parameters
name

string

The name of the function to call.

args

Struct

The function parameters and values in JSON object format.

See Function calling for parameter details.

thought_signature

Optional: string

An opaque token that encapsulates the model's internal reasoning state. If present, it must be included in the next turn of the conversation to maintain context for multi-step tool use.

functionResponse
The resulting output from a FunctionCall that contains a string representing the FunctionDeclaration.name. Also contains a structured JSON object with the output from the function (and uses it as context for the model). This should contain the result of a FunctionCall made based on model prediction.

Parameters
name

string

The name of the function to call.

response

Struct

The function response in JSON object format.

parts

Optional: FunctionResponsePart[]

Optional multimodal parts of the function response. Can be used to return images, audio, or video along with structured JSON response.

FunctionResponsePart
The FunctionResponsePart contains multimodal data from the function call.

Parameters
inline_data

Optional: FunctionResponseBlob

Inline data of the function response part.

file_data

Optional: FunctionResponseFileData

URI based data of the function response part.

FunctionResponseBlob
The FunctionResponseBlob contains inline data for a FunctionResponsePart.

Parameters
mime_type

string

The IANA MIME type of the inline data.

data

bytes

The raw binary data.

display_name

Optional: string

The display name of the inline data.

FunctionResponseFileData
The FunctionResponseFileData contains URI based data for a FunctionResponsePart.

Parameters
mime_type

string

The IANA MIME type of the file data.

file_uri

string

The URI of the file data.

display_name

Optional: string

The display name of the file data.

Examples
Send a function declaration
The following example is a basic example of sending a query and a function declaration to the model:

REST
Gen AI SDK for Python
Node.js
Java
Go
REST (OpenAI)
Python (OpenAI)



const {
  VertexAI,
  FunctionDeclarationSchemaType,
} = require('@google-cloud/vertexai');

const functionDeclarations = [
  {
    function_declarations: [
      {
        name: 'get_current_weather',
        description: 'get weather in a given location',
        parameters: {
          type: FunctionDeclarationSchemaType.OBJECT,
          properties: {
            location: {type: FunctionDeclarationSchemaType.STRING},
            unit: {
              type: FunctionDeclarationSchemaType.STRING,
              enum: ['celsius', 'fahrenheit'],
            },
          },
          required: ['location'],
        },
      },
    ],
  },
];

/**
 * TODO(developer): Update these variables before running the sample.
 */
async function functionCallingBasic(
  projectId = 'PROJECT_ID',
  location = 'us-central1',
  model = 'gemini-2.0-flash-001'
) {
  // Initialize Vertex with your Cloud project and location
  const vertexAI = new VertexAI({project: projectId, location: location});

  // Instantiate the model
  const generativeModel = vertexAI.preview.getGenerativeModel({
    model: model,
  });

  const request = {
    contents: [
      {role: 'user', parts: [{text: 'What is the weather in Boston?'}]},
    ],
    tools: functionDeclarations,
  };
  const result = await generativeModel.generateContent(request);
  console.log(JSON.stringify(result.response.candidates[0].content));
}
Send a function declaration with FunctionCallingConfig
The following example demonstrates how to pass a FunctionCallingConfig to the model:

The functionCallingConfig ensures that the model output is always a specific function call. To configure:

Set the function calling mode to ANY.
Specify the function names that you want to use in allowed_function_names. If allowed_function_names is empty, any of the provided functions can be returned.

REST
Gen AI SDK for Python
Node.js
Go
REST (OpenAI)
Python (OpenAI)



const {
  VertexAI,
  FunctionDeclarationSchemaType,
} = require('@google-cloud/vertexai');

const functionDeclarations = [
  {
    function_declarations: [
      {
        name: 'get_product_sku',
        description:
          'Get the available inventory for a Google products, e.g: Pixel phones, Pixel Watches, Google Home etc',
        parameters: {
          type: FunctionDeclarationSchemaType.OBJECT,
          properties: {
            productName: {type: FunctionDeclarationSchemaType.STRING},
          },
        },
      },
      {
        name: 'get_store_location',
        description: 'Get the location of the closest store',
        parameters: {
          type: FunctionDeclarationSchemaType.OBJECT,
          properties: {
            location: {type: FunctionDeclarationSchemaType.STRING},
          },
        },
      },
    ],
  },
];

const toolConfig = {
  function_calling_config: {
    mode: 'ANY',
    allowed_function_names: ['get_product_sku'],
  },
};

const generationConfig = {
  temperature: 0.95,
  topP: 1.0,
  maxOutputTokens: 8192,
};

/**
 * TODO(developer): Update these variables before running the sample.
 */
async function functionCallingAdvanced(
  projectId = 'PROJECT_ID',
  location = 'us-central1',
  model = 'gemini-2.0-flash-001'
) {
  // Initialize Vertex with your Cloud project and location
  const vertexAI = new VertexAI({project: projectId, location: location});

  // Instantiate the model
  const generativeModel = vertexAI.preview.getGenerativeModel({
    model: model,
  });

  const request = {
    contents: [
      {
        role: 'user',
        parts: [
          {text: 'Do you have the White Pixel 8 Pro 128GB in stock in the US?'},
        ],
      },
    ],
    tools: functionDeclarations,
    tool_config: toolConfig,
    generation_config: generationConfig,
  };
  const result = await generativeModel.generateContent(request);
  console.log(JSON.stringify(result.response.candidates[0].content));
}

Prompt management

bookmark_border
This page introduces a unified set of capabilities to define, save, retrieve, and manage prompts within your Google Cloud project using the Vertex AI SDK prompt management module.

Overview
Vertex AI offers tooling to help manage prompt templates and prompt data. Prompt templates can be versioned and used with generative models on Vertex AI. Each prompt can be assembled and versioned in Vertex AI Studio or the Vertex AI SDK.

Prompt management in the Vertex AI SDK includes full enterprise support, including support for Customer-Managed Encryption Keys (CMEK) and VPC Service Controls (VPCSC).

Prompt management capabilities
To use any of the generative AI capabilities from the Vertex AI SDK, do the following:

Install the latest version of the Vertex AI SDK.



pip install --upgrade google-cloud-aiplatform
Create a generative AI client using the following Python code sample:



    import vertexai
    from vertexai import types

    # Instantiate GenAI client from Vertex SDK
    # Replace with your project ID and location
    client = vertexai.Client(project='my-project', location='my-location')
After you've created a generative AI client, you can use any of the following prompt management capabilities in the Vertex AI SDK:

Create a local prompt.
Save a prompt to a Google Cloud project.
Get a saved prompt.
List prompts and versions.
Delete a prompt.
Restore a prompt version.
Create a local prompt
This section provides an example of how to define a types.Prompt variable for use throughout this page.

The Prompt class is used for defining, assembling, and using prompts. The prompt_data attribute is defined for the Prompt class and includes the following:

Attribute
model

Required: The model name.

contents

Required: The content of the conversation with the model. Only single-turn prompts are supported.

generation_config

Optional: Generation configuration

tool_config

Optional: This configuration is shared for all tools provided in the request.

tools

Optional: A list of Tools the model might use to generate the next response. A Tool is a piece of code that enables the system to interact with external systems to perform an action, or set of actions, outside of the knowledge and scope of the model.

safety_settings

Optional: The request settings for blocking unsafe content, which are enforced on GenerateContentResponse.candidates.

system_instruction

Optional: The user-provided system instructions for the model.

variables

Optional: If your prompt contains a template variable, provide the values to use for that variable. For example, if your prompt text contents are "Hello, {name}". Your variables list should include a dictionary of all possible values for the "{name}" variable. Example:


        "variables": [
            {"name": {"text": "Alice"}},
            {"name": {"text": "Bob"}},
        ],
        
This code sample demonstrates how to define a types.Prompt variable.



  import vertexai
  from vertexai import types
  from google.genai import types

  prompt = types.Prompt(
      prompt_data=types.PromptData(
        contents=[genai_types.Content(parts=[genai_types.Part(text="Hello, {name}! How are you?")])],
        variables=[
          {"name": genai_types.Part(text="Alice")},
          {"name": genai_types.Part(text="Bob")},
        ],
        model="your-model",
      ),
  )

Save a prompt to a Google Cloud project
This section presents the parameters and an example for how to save a prompt to a Google Cloud project.

Parameters
This table describes the parameters used by the create method:

Parameters
prompt

The data for a specific prompt.

config

Optional: A types.CreatePromptConfig object representing the configuration for creating a prompt.

This table describes the parameters used by the create_version method:

Parameters
prompt

Required: The data for a specific prompt.

config

Optional: A types.CreatePromptConfig object representing the configuration for creating a prompt version.

A Prompt is returned by the create_version method.

Example
To save a prompt to a Google Cloud project, use the client.prompts.create and client.prompts.create_version methods.

The client.prompts.create method creates a prompt resource in a Google Cloud project. The client.prompts.create_version method creates a prompt version within that resource, and you can access the resource in the Google Cloud console.

The client.prompts.create method takes a Prompt object as input and creates a prompt in the Google Cloud project. The client.prompts.create_version method also requires passing in prompt_id, which is the ID of the prompt resource to create the version under. A new Prompt object is returned, which is associated with the Google Cloud project. Any updates made to a Prompt object are local until create or create_version is called.

The following code sample shows you how to save a prompt and a prompt version:



  # Save `Prompt` to a Google Cloud project.
  # Returns a new `Prompt` object associated with the saved prompt resource.
  prompt_resource = client.prompts.create(prompt=prompt)
  prompt_version_resource = client.prompts.create_version(prompt=prompt, prompt_id=prompt_resource.prompt_id)
Get a saved prompt
This section presents the parameters and an example for how to get a prompt and a prompt version.

There are two methods: client.prompts.get and client.prompts.get_version.

Parameters
This table describes the parameters used by the client.prompts.get method:

Parameters
prompt_id

Required: The ID for the prompt to retrieve.

config

Optional: A types.GetPromptConfig object representing the configuration for getting a prompt.

This table describes the parameters used by the client.prompts.get_version method:

Parameters
prompt_id

Required: The ID for the prompt to retrieve.

version_id

Required: The ID of the prompt version that you want to retrieve.

config

Optional: A types.GetPromptConfig object representing the configuration for getting a prompt.

A Prompt is returned by the get and get_version methods.

Examples
To get (load) a prompt that has been saved to the Google Cloud project, use the client.prompts.get method. This method takes the prompt ID as input and returns the corresponding Prompt object. This code sample shows how to load a saved prompt:



  # Get prompt
  retrieved_prompt = client.prompts.get(prompt_id=prompt_resource.prompt_id)
The following code sample shows you how to get a version of a prompt.



  retrieved_prompt_version = client.prompts.get_version(prompt_id='your-prompt-id', version_id='your-prompt-version-id')
The following code demonstrates how to transform your prompt to call generate_content in the generative AI SDK.



  from google import genai
  from google.genai import types as genai_types

  genai_client = genai.Client(vertexai=True, project="my-project", location="my-location")

  response = genai_client.models.generate_content(
      model=retrieved_prompt.prompt_data.model,
      contents=retrieved_prompt.assemble_contents(),
  )
List prompts and versions
This section presents the parameters and an example for how to list prompts and prompt versions.

There are two methods: client.prompts.list and client.prompts.list_versions.

Parameters
This table describes the parameters used by the list method:

Parameters
config

Optional: A types.ListPromptsConfig object representing the configuration for listing prompts.

This table describes the parameters used by the list_versions method:

Parameters
prompt_id

Required: The ID of the prompt to list versions for.

config

Optional: A types.ListPromptsConfig object representing the configuration for listing prompt versions.

Both the list and list_versions methods return an Iterator of types.PromptRef objects. The PromptRef contains a reference to a prompt.

Example
To see the prompt ID and model for all prompts saved in your Google Cloud project, use the list method.

The following code sample demonstrates how to retrieve a PromptRef for all saved prompts in the current project:



  prompt_refs = list(client.prompts.list())

  # Get a prompt from the list
  prompt1 = client.prompts.get(prompt_id=prompt_refs[0].prompt_id)
The following code sample demonstrates how to list prompt and version IDs for all prompt versions saved within the prompt:



  prompt_versions_metadata = client.prompts.list_versions(prompt_id="123456789")

  # Get a specific prompt version from the versions metadata list
  prompt1 = client.prompts.get_version(
      prompt_id=prompt_versions_metadata[0].prompt_id,
      version_id=prompt_versions_metadata[0].version_id
  )
Delete a prompt
This section presents the parameters and an example for how to delete a prompt.

There are two methods: delete and delete_version.

Parameters
This table describes the parameters used by the delete method:

Important: The delete method deletes a prompt and all of the versions associated with the prompt. After a prompt is deleted, it can't be restored.
Parameters
prompt_id

The ID of the prompt to delete.

config

Optional: A types.DeletePromptConfig object representing the configuration for deleting a prompt.

This table describes the parameters used by the delete_version method:

Parameters
prompt_id

The ID for the prompt to delete a version from.

version_id

The version of the prompt to delete.

config

Optional: A types.DeletePromptConfig object representing the configuration for deleting a prompt version.

Examples
To delete a prompt and all of its versions, use the delete method.



  client.prompts.delete(prompt_id=retrieved_prompt.prompt_id)
To delete a specific version from a prompt resource, use the delete_version method.



  client.prompts.delete_version(prompt_id=retrieved_prompt.prompt_id, version_id='your-version-id')
Restore a prompt version
This section presents the parameters and an example for how to restore a prompt version.

Parameters
This table describes the parameters used by the restore_version method:

Parameters
prompt_id

The ID for a specific prompt.

version_id

The version of the prompt to restore.

config

The configuration for restoring a prompt version.

A Prompt object is returned by the restore_version method.

Example
A prompt resource also contains version history that stores previous saved versions of the prompt. You can use the restore_version() method to restore an older version as the latest version of the prompt. This returns a Prompt object.



  # Restore to prompt version id 1
  restored_prompt = client.prompts.restore_version(prompt_id=retrieved_prompt.prompt_id, version_id='1')

  Grounding API

bookmark_border
In generative AI, grounding is the ability to connect model output to verifiable sources of information. If you provide models with access to specific data sources, then grounding tethers their output to these data and reduces the chances of inventing content.

With Vertex AI, you can ground model outputs in the following ways:

Ground with Google Search - ground a model with publicly-available web data.
Ground with Google Maps - ground a model with geospatial data from Google Maps.
Ground to your data - ground a model with your data from Vertex AI Search as a data store.
For more information about grounding, see Grounding overview.

Supported models
Gemini 3 Pro preview
Gemini 3 Pro Image preview
Gemini 2.5 Pro
Gemini 2.5 Flash preview
Gemini 2.5 Flash-Lite preview
Gemini 2.5 Flash
Gemini 2.5 Flash-Lite
Gemini 2.5 Flash with Live API native audio preview
Gemini 2.0 Flash with Live API preview
Gemini 2.0 Flash
Parameter list
See examples for implementation details.

googleSearch
Ground the response with publicly-available web data from Google Search.

googleMaps
Ground the response with publicly-available geospatial data from Google Maps.

The API input includes the following parameter:

Input parameter
enable_widget

Required: boolean

Flag that can be set to true or false. A value of true returns a token using the API response that you can use with the Google Maps context widget user interface.

The API response structure includes the following parameter:

Response parameter
grounding_metadata

Required: Object

The primary field that contains grounding information.

grounding_support: A sub-field indicating the level of grounding support.
grounding_chunks.maps: A sub-field containing the places sources used to generate the grounded response.
place_answer_sources.review_snippets: A sub-field within grounding_chunks.maps that appears when a place answer is used to answer a query. Place answers provide deeper contextual information about a specific place using data, such as user reviews. The place answer is backed by a list of sources like user reviews.
Attributes
A place or user review source has the following attributes:

Attributes
title

Required: Object

The title of the source.

uri

Required: string

A URI linking to the source.

place_id

Required: string

A unique identifier for the place.

review_id

Required: string

A unique identifier for review.

retrieval
Ground the response with private data from Vertex AI Search as a data store. Defines a retrieval tool that the model can call to access external knowledge.

Parameters
vertexAiSearch

Required: VertexAISearch

Ground with Vertex AI Search data sources.

VertexAISearch
Parameters
datastore

Required: string

Fully-qualified data store resource ID from Vertex AI Search, in the following format: projects/{project}/locations/{location}/collections/default_collection/dataStores/{datastore}

Examples
This section provides examples for grounding a response on public web data using Google Search and grounding a response on private data using Vertex AI Search.

Ground response on public web data using Google Search
Ground the response with Google Search public data. Include the google_search_retrieval tool in the request. No additional parameters are required.

Python
Go
Java
Install


pip install --upgrade google-genai
To learn more, see the SDK reference documentation.

Set environment variables to use the Gen AI SDK with Vertex AI:



# Replace the `GOOGLE_CLOUD_PROJECT` and `GOOGLE_CLOUD_LOCATION` values
# with appropriate values for your project.
export GOOGLE_CLOUD_PROJECT=GOOGLE_CLOUD_PROJECT
export GOOGLE_CLOUD_LOCATION=global
export GOOGLE_GENAI_USE_VERTEXAI=True



from google import genai
from google.genai.types import (
    GenerateContentConfig,
    GoogleSearch,
    HttpOptions,
    Tool,
)

client = genai.Client(http_options=HttpOptions(api_version="v1"))

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="When is the next total solar eclipse in the United States?",
    config=GenerateContentConfig(
        tools=[
            # Use Google Search Tool
            Tool(google_search=GoogleSearch())
        ],
    ),
)

print(response.text)
# Example response:
# 'The next total solar eclipse in the United States will occur on ...'
Ground response on private data using Vertex AI Search
Ground the response with data from a Vertex AI Search data store. For more information, see AI Applications.

Before you ground a response with private data, create a data store and a search app.

WARNING: For the time being, this "grounding" interface does not support Vertex AI Search "chunk mode".

Gen AI SDK for Python



from google import genai
from google.genai.types import (
    GenerateContentConfig,
    HttpOptions,
    Retrieval,
    Tool,
    VertexAISearch,
)

client = genai.Client(http_options=HttpOptions(api_version="v1"))

# Load Data Store ID from Vertex AI Search
# datastore = "projects/111111111111/locations/global/collections/default_collection/dataStores/data-store-id"

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="How do I make an appointment to renew my driver's license?",
    config=GenerateContentConfig(
        tools=[
            # Use Vertex AI Search Tool
            Tool(
                retrieval=Retrieval(
                    vertex_ai_search=VertexAISearch(
                        datastore=datastore,
                    )
                )
            )
        ],
    ),
)

print(response.text)
# Example response:
# 'The process for making an appointment to renew your driver's license varies depending on your loca


Image generation API

bookmark_border
The Imagen API lets you generate high quality images in seconds, using text prompt to guide the generation.

View Imagen for Generation model card

Supported model versions

Caution: The following Imagen 4 preview models will be removed on November 30, 2025 : imagen-4.0-generate-preview-06-06, imagen-4.0-ultra-generate-preview-06-06, and imagen-4.0-fast-generate-preview-06-06. To avoid service disruption, migrate all workflows that use Imagen 4 preview models before November 30, 2025 , 2025, to the following Imagen 4 Generally Available models: imagen-4.0-generate-001, imagen-4.0-ultra-generate-001, imagen-4.0-fast-generate-001.
Imagen API supports the following models:

imagen-4.0-generate-001
imagen-4.0-fast-generate-001
imagen-4.0-ultra-generate-001
imagen-3.0-generate-002
imagen-3.0-generate-001
imagen-3.0-fast-generate-001
imagen-3.0-capability-001
Important: imagen-4.0-fast-generate-001 may generate undesireable results if the prompt is complex and you use enhanced prompts. To fix this, set enhancePrompt to false.
For more information about the features that each model supports, see Imagen models.

Example syntax
Syntax to create an image from a text prompt.

Syntax
Syntax to generate an image.

REST
Python


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \

https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_VERSION}:predict \
-d '{
  "instances": [
    {
      "prompt": "..."
    }
  ],
  "parameters": {
    "sampleCount": ...
  }
}'
Generate images
REST
Python
Parameters
addWatermark	
bool

Optional. Add an invisible watermark to the generated images.

The default value is true.

aspectRatio	
string

Optional. The aspect ratio for the generated output image. The default value is "1:1".

enhancePrompt	
boolean

Optional. An optional parameter to use an LLM-based prompt rewriting feature to deliver higher quality images that better reflect the original prompt's intent. Disabling this feature may impact image quality and prompt adherence.

language	
string

Optional. The language code that corresponds to your text prompt language. The following values are supported:

auto: Automatic detection. If Imagen detects a supported language, the prompt and an optional negative prompt are translated to English. If the language detected isn't supported, Imagen uses the input text verbatim, which might result in an unexpected output. No error code is returned.
en: English (if omitted, the default value)
zh or zh-CN: Chinese (simplified)
zh-TW: Chinese (traditional)
hi: Hindi
ja: Japanese
ko: Korean
pt: Portuguese
es: Spanish
negativePrompt	
string

Optional. A description of what to discourage in the generated images.

negativePrompt isn't supported by imagen-3.0-generate-002 and newer models.

outputOptions	
outputOptions

Optional. Describes the output image format in an outputOptions object.

prompt	
string

Required. The text prompt for the image.

personGeneration	
string

Optional. Allow generation of people by the model. The following values are supported:

"dont_allow": Disallow the inclusion of people or faces in images.
"allow_adult": Allow generation of adults only.
"allow_all": Allow generation of people of all ages.
The default value is "allow_adult".

safetySetting	
string

Optional. Adds a filter level to safety filtering. The following values are supported:

"block_low_and_above": Strongest filtering level, most strict blocking. Deprecated value: "block_most".
"block_medium_and_above": Block some problematic prompts and responses. Deprecated value: "block_some".
"block_only_high": Reduces the number of requests blocked due to safety filters. May increase objectionable content generated by Imagen. Deprecated value: "block_few".
"block_none": Block very few problematic prompts and responses. Access to this feature is restricted. Previous field value: "block_fewest".
The default value is "block_medium_and_above".

sampleCount	
int

Required. The number of images to generate. The default value is 4.

sampleImageSize	
string

Optional. Specifies the generated image's output resolution. The accepted values are "1K" or "2K". The default value is "1K".

seed	
Uint32

Optional. The random seed for image generation. This isn't available when addWatermark is set to true.

If enhancePrompt is set to true, the seed parameter won't work, because enhancePrompt generates a new prompt, which results in a new or different image.

storageUri	
Optional: string

Cloud Storage URI to store the generated images.

Output options object
The outputOptions object describes the image output.

Parameters
outputOptions.mimeType	
Optional: string

The image format that the output should be saved as. The following values are supported:

"image/png": Save as a PNG image
"image/jpeg": Save as a JPEG image
The default value is "image/png".

outputOptions.compressionQuality	
Optional: int

The level of compression if the output type is "image/jpeg". Accepted values are 0 through 100. The default value is 75.

Response
The response body from the REST request.

Parameter
predictions	An array of VisionGenerativeModelResult objects, one for each requested sampleCount. If any images are filtered by responsible AI, they are not included, unless includeRaiReason is set to true.
Vision generative model result object
Information about the model result.

Parameter
bytesBase64Encoded	
The base64 encoded generated image. Not present if the output image did not pass responsible AI filters.

mimeType	
The type of the generated image. Not present if the output image did not pass responsible AI filters.

raiFilteredReason	
The responsible AI filter reason. Only returned if includeRaiReason is enabled and this image was filtered out.

safetyAttributes.categories	
The safety attribute name. Only returned if includeSafetyAttributes is enabled, and the output image passed responsible AI filters.

safetyAttributes.scores	
The safety attribute score. Only returned if includeSafetyAttributes is enabled, and the output image passed responsible AI filters.

Examples
The following examples show how to use the Imagen models to generate images.

Generate images
REST
Python
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
MODEL_VERSION: The Imagen model version to use. For more information about available models, see Imagen models.

TEXT_PROMPT: The text prompt that guides what images the model generates. This field is required for both generation and editing.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
Additional optional parameters

HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/MODEL_VERSION:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT"
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/MODEL_VERSION:predict"
The following sample response is for a request with "sampleCount": 2. The response returns two prediction objects, with the generated image bytes base64-encoded.


{
  "predictions": [
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    },
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}
If you use a model that supports prompt enhancement, the response includes an additional prompt field with the enhanced prompt used for generation:


{
  "predictions": [
    {
      "mimeType": "MIME_TYPE",
      "prompt": "ENHANCED_PROMPT_1",
      "bytesBase64Encoded": "BASE64_IMG_BYTES_1"
    },
    {
      "mimeType": "MIME_TYPE",
      "prompt": "ENHANCED_PROMPT_2",
      "bytesBase64Encoded": "BASE64_IMG_BYTES_2"
    }
  ]
}

Edit images

bookmark_border
The Imagen lets you edit images in seconds, using text prompts, masks, and existing images to guide the edits.

View Imagen for Editing and Customization model card

Supported model versions
Imagen API supports the following models:

imagen-3.0-capability-001
For more information about the features that the model supports, see Imagen models.

HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagen-3.0-capability-001:predict \

-d '{
  "instances": [
    {
      "referenceImages": [
        {
          "referenceType": "REFERENCE_TYPE_RAW",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": string
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_MASK",
          "referenceId": 2,
          "referenceImage": {
            "bytesBase64Encoded": string
          },
          "maskImageConfig": {
            "maskMode": "MASK_MODE_USER_PROVIDED"
          }
        }
      ],
      "prompt": string
    }
  ],
  "parameters": {
    "addWatermark": boolean,
    "baseSteps": integer,
    "editMode": string,
    "guidanceScale": integer,
    "includeRaiReason": boolean,
    "includeSafetyAttributes": boolean,
    "language": string,
    "negativePrompt": string,
    "outputOptions": {
      "mimeType": string,
      "compressionQuality": integer
    },
    "personGeneration": string,
    "safetySetting": string,
    "sampleCount": integer,
    "seed": integer,
    "storageUri": string
  }
}'
Instances
Instances
prompt	
string

Optional. The text prompt for the image. If a prompt isn't specified, the model fills in content from the image context.

referenceImages	
List of ReferenceImage objects.

Required. For mask editing, exactly two reference images must be specified, one with REFERENCE_TYPE_RAW, and one with REFERENCE_TYPE_MASK.

referenceImages object
The referenceImages object describes the image assets for Imagen to edit.

Parameters
referenceType	
string

Required. The type of reference image. One of the following:

REFERENCE_TYPE_RAW: The base image to edit.
REFERENCE_TYPE_MASK: The mask image, whose non-zero values indicate where to edit the base image.
referenceId	
integer

Required. A unique identifier for the reference image. Not used for masked editing.

referenceImage.bytesBase64Encoded	
string

Required. Base64-encoded image bytes. Accepts PNG, JPEG, GIF, and BMP files. The maximum size is 20MB after transcoding to PNG. If you provide a mask image, it must be the same dimensions as the base image.

maskImageConfig.maskMode	
string

Required when referenceType is REFERENCE_TYPE_MASK. Must be one of the following:

MASK_MODE_USER_PROVIDED: Use the mask from referenceImage.bytesBase64Encoded.
MASK_MODE_BACKGROUND: Use an auto-generated mask from background segmentation.
MASK_MODE_FOREGROUND: Use an auto-generated mask from foreground segmentation.
MASK_MODE_SEMANTIC: Use an auto-generated mask from semantic segmentation with the given mask class.
maskImageConfig.dilation	
float

Optional. Range: [0, 1]. The percentage of image width to dilate (grow) the mask by. This can help compensate for imprecise masks. For best results, we recommend the following maskImageConfig.maskMode settings, we recommend the listed values:

EDIT_MODE_INPAINT_INSERTION: 0.01
EDIT_MODE_INPAINT_REMOVAL: 0.01
EDIT_MODE_BGSWAP: 0.0
EDIT_MODE_OUTPAINT: 0.01-0.03
maskImageConfig.maskClasses	
list[integer]

Optional. Mask classes for MASK_MODE_SEMANTIC mode.

Parameters
Parameters
addWatermark	
bool

Optional. Add an invisible watermark to the generated images.

The default value is true.

baseSteps	
integer

Optional. The number of sampling steps. A higher value has better image quality, while a lower value has better latency. Defaults to 75.

For smaller mask areas or for removal or insert modes, use 16 - 35 steps to reduce latency while returning a similar level of quality.

editMode	
string

Required for mask editing.

An enum with one of the following values:

EDIT_MODE_INPAINT_REMOVAL: Remove objects and fill in the image background in the mask area.
EDIT_MODE_INPAINT_INSERTION: Add objects from a given prompt.
EDIT_MODE_BGSWAP: Add background content in the mask area, while preserving the object content in the unmasked area. Useful for product editing.
EDIT_MODE_OUTPAINT: Extends the image into the mask area. Unlike EDIT_MODE_BGSWAP, this will generate object completion for partial objects at the image boundary.
guidanceScale	
integer

Optional. Controls how much the model adheres to the text prompt. Large values increase output and prompt alignment, but might compromise image quality.

Accepted range: 0 - 500

Default: 60 for insert mode, 75 for remove, bgswap, outpaint.

includeRaiReason	
boolean

Optional. Whether to include a safety reason for filtered images in the response. The default value is false.

includeSafetyAttributes	
boolean

Optional. Whether to report the safety scores of each image in the response. The default value is false.

language	
string

Optional. The language code that corresponds to your text prompt language. The following values are supported:

"auto": Automatic detection. If Imagen detects a supported language, the prompt and an optional negative prompt are translated to English. If the language detected isn't supported, Imagen uses the input text verbatim, which might result in an unexpected output. No error code is returned.
"en": English (if omitted, the default value)
"zh" or "zh-CN": Chinese (simplified)
"zh-TW": Chinese (traditional)
"hi": Hindi
"ja": Japanese
"ko": Korean
"pt": Portuguese
"es": Spanish
language is supported only by imagen-3.0-capability-001.

negativePrompt	
string

Optional. A description of what to discourage in the generated images.

outputOptions	
outputOptions

Optional. Describes the output image format in an outputOptions object.

personGeneration	
string

Optional. Allow generation of people by the model. The following values are supported:

"dont_allow": Disallow the inclusion of people or faces in images.
"allow_adult": Allow generation of adults only.
"allow_all": Allow generation of people of all ages.
For mask-based editing personGeneration defaults to allow_adult. For mask-free editing, personGeneration defaults to allow_adult.

sampleCount	
integer

Optional. The number of images to generate. The default value is 4.

seed	
Uint32

Optional. The random seed for image generation. This isn't available when addWatermark is set to true.

safetySetting	
string

Optional. Adds a filter level to safety filtering. The following values are supported:


"block_low_and_above": Strongest filtering level, most strict blocking. Deprecated value: "block_most".
"block_medium_and_above": Block some problematic prompts and responses. Deprecated value: "block_some".
"block_only_high": Reduces the number of requests blocked due to safety filters. May increase objectionable content generated by Imagen. Deprecated value: "block_few".
"block_none": Block very few problematic prompts and responses. Access to this feature is restricted. Previous field value: "block_fewest".
The default value is "block_medium_and_above".

safetySetting is supported only by imagen-3.0-capability-001.

storageUri	
string

Optional. The Cloud Storage URI to store the generated images.

Output options object
The outputOptions object describes the image output.

Parameters
outputOptions.mimeType	
string

Optional. The image format that the output should be saved as. The following values are supported:

"image/png": Save as a PNG image
"image/jpeg": Save as a JPEG image
The default value is "image/png".

outputOptions.compressionQuality	
integer

Optional. The level of compression if the output type is "image/jpeg". Accepted values are 0 through 100. The default value is 75.

Sample request
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
TEXT_PROMPT: Optional. A text prompt to guide the images that the model generates. For best results, use a description of the masked area and avoid single-word prompts. For example, use "a cute corgi" instead of "corgi".
B64_BASE_IMAGE: A base64-encoded image of the image being edited that is 10MB or less in size. For more information about base64-encoding, see Base64 encode and decode files.
B64_MASK_IMAGE: A base64-encoded black and white mask image that is 10MB or less in size.
MASK_DILATION: Optional. A float value between 0 and 1, inclusive, that represents the percentage of the image width to grow the mask by. Using dilation helps compensate for imprecise masks. We recommend a value of 0.01.
EDIT_STEPS: Optional. An integer that represents the number of sampling steps. A higher value offers better image quality, a lower value offers better latency.

We recommend that you try 35 steps to start. If the quality doesn't meet your requirements, then we recomment increasing the value towards an upper limit of 75.

SAMPLE_COUNT: Optional. An integer that describes the number of images to generate. The accepted range of values is 1-4. The default value is 4.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-3.0-capability-001:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "referenceImages": [
        {
          "referenceType": "REFERENCE_TYPE_RAW",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "B64_BASE_IMAGE"
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_MASK",
          "referenceImage": {
            "bytesBase64Encoded": "B64_MASK_IMAGE"
          },
          "maskImageConfig": {
            "maskMode": "MASK_MODE_USER_PROVIDED",
            "dilation": MASK_DILATION
          }
        }
      ]
    }
  ],
  "parameters": {
    "editConfig": {
      "baseSteps": EDIT_STEPS
    },
    "editMode": "EDIT_MODE_INPAINT_INSERTION",
    "sampleCount": SAMPLE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-3.0-capability-001:predict"
The following sample response is for a request with "sampleCount": 2. The response returns two prediction objects, with the generated image bytes base64-encoded.

{
  "predictions": [
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    },
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}
Class IDs
Use the following object class IDs to automatically create an image mask based on specific objects.

Search for a segmentation class or ID
Class ID (class_id)	Object
0	backpack
1	umbrella
2	bag
3	tie
4	suitcase
5	case
6	bird
7	cat
8	dog
9	horse
10	sheep
11	cow
12	elephant
13	bear
14	zebra
15	giraffe
16	animal (other)
17	microwave
18	radiator
19	oven
20	toaster
21	storage tank
22	conveyor belt
23	sink
24	refrigerator
25	washer dryer
26	fan
27	dishwasher
28	toilet
29	bathtub
30	shower
31	tunnel
32	bridge
33	pier wharf
34	tent
35	building
36	ceiling
37	laptop
38	keyboard
39	mouse
40	remote
41	cell phone
42	television
43	floor
44	stage
45	banana
46	apple
47	sandwich
48	orange
49	broccoli
50	carrot
51	hot dog
52	pizza
53	donut
54	cake
55	fruit (other)
56	food (other)
57	chair (other)
58	armchair
59	swivel chair
60	stool
61	seat
62	couch
63	trash can
64	potted plant
65	nightstand
66	bed
67	table
68	pool table
69	barrel
70	desk
71	ottoman
72	wardrobe
73	crib
74	basket
75	chest of drawers
76	bookshelf
77	counter (other)
78	bathroom counter
79	kitchen island
80	door
81	light (other)
82	lamp
83	sconce
84	chandelier
85	mirror
86	whiteboard
87	shelf
88	stairs
89	escalator
90	cabinet
91	fireplace
92	stove
93	arcade machine
94	gravel
95	platform
96	playingfield
97	railroad
98	road
99	snow
100	sidewalk pavement
101	runway
102	terrain
103	book
104	box
105	clock
106	vase
107	scissors
108	plaything (other)
109	teddy bear
110	hair dryer
111	toothbrush
112	painting
113	poster
114	bulletin board
115	bottle
116	cup
117	wine glass
118	knife
119	fork
120	spoon
121	bowl
122	tray
123	range hood
124	plate
125	person
126	rider (other)
127	bicyclist
128	motorcyclist
129	paper
130	streetlight
131	road barrier
132	mailbox
133	cctv camera
134	junction box
135	traffic sign
136	traffic light
137	fire hydrant
138	parking meter
139	bench
140	bike rack
141	billboard
142	sky
143	pole
144	fence
145	railing banister
146	guard rail
147	mountain hill
148	rock
149	frisbee
150	skis
151	snowboard
152	sports ball
153	kite
154	baseball bat
155	baseball glove
156	skateboard
157	surfboard
158	tennis racket
159	net
160	base
161	sculpture
162	column
163	fountain
164	awning
165	apparel
166	banner
167	flag
168	blanket
169	curtain (other)
170	shower curtain
171	pillow
172	towel
173	rug floormat
174	vegetation
175	bicycle
176	car
177	autorickshaw
178	motorcycle
179	airplane
180	bus
181	train
182	truck
183	trailer
184	boat ship
185	slow wheeled object
186	river lake
187	sea
188	water (other)
189	swimming pool
190	waterfall
191	wall
192	window
193	window blind
Customize images

bookmark_border
The Imagen API lets you create high quality images in seconds, using text prompts and reference images to guide subject or style generation.

View Imagen for Editing and Customization model card

Supported Models
Model	Code
Customization using reference images (few-shot)	imagen-3.0-capability-001
For more information about the features that each model supports, see Imagen models.

HTTP method and URL


POST https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/imagen-3.0-capability-001:predict
Example syntax
Syntax to customize an image from a text prompt and reference images.

Syntax
Syntax to customize an image.

REST


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/imagen-3.0-capability-001:predict \
-d '{
    "instances": [
      {
        // Use [1] to refer to the reference images with referenceId=1
        // [2] to refer to the reference images with referenceId=2,
        // following the same format for all reference IDs that you provide.
        "prompt": "${TEXT_PROMPT}",
        "referenceImages": [
          // A list of at most 4 reference image objects.
          [...]
        ]
      }
    ],
    "parameters": {
        [...]
    }
}'
Sample request body:

This request is for person customization with a face mesh control image and three reference images.


{
  "instances": [
    {
      "prompt": "Create an image about a man with short hair [1] in the pose of
       control image [2] to match the description: A pencil style sketch of a
       full-body portrait of a man with short hair [1] with hatch-cross drawing,
       hatch drawing of portrait with 6B and graphite pencils, white background,
       pencil drawing, high quality, pencil stroke, looking at camera, natural
       human eyes",
      "referenceImages": [
        {
          "referenceType": "REFERENCE_TYPE_CONTROL",
          "referenceId": 2,
          "referenceImage": {
            "bytesBase64Encoded": "${IMAGE_BYTES_1}"
          },
          "controlImageConfig": {
            "controlType": "CONTROL_TYPE_FACE_MESH",
            "enableControlImageComputation": true
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_SUBJECT",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "${IMAGE_BYTES_2}"
          },
          "subjectImageConfig": {
            "subjectDescription": "a man with short hair",
            "subjectType": "SUBJECT_TYPE_PERSON"
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_SUBJECT",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "${IMAGE_BYTES_3}"
          },
          "subjectImageConfig": {
            "subjectDescription": "a man with short hair",
            "subjectType": "SUBJECT_TYPE_PERSON"
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_SUBJECT",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "${IMAGE_BYTES_4}"
          },
          "subjectImageConfig": {
            "subjectDescription": "a man with short hair",
            "subjectType": "SUBJECT_TYPE_PERSON"
          }
        }
      ]
    }
  ],
  "parameters": {
    "negativePrompt": "wrinkles, noise, Low quality, dirty, low res, multi face,
      rough texture, messy, messy background, color background, photo realistic,
      photo, super realistic, signature, autograph, sign, text, characters,
      alphabet, letter",
    "seed": 1,
    "language": "en",
    "sampleCount": 4
  }
}
Parameter list
See examples for implementation details.

Customize images
REST
Parameters
referenceType	Required enumeration:
REFERENCE_TYPE_RAW
A raw reference image is required for editing use cases.
A raw reference image isn't needed for other use cases.
At most one raw reference image exists in one request.
The output image has the same size as the raw reference input image.
REFERENCE_TYPE_MASK
A mask reference image is required for masked editing use cases.
A mask reference image isn't required for other use cases.
If a raw reference image is present, the mask image has to be in the same size as the raw reference image.
The user can either provide their own mask, or let Imagen compute the mask for them from the provided reference image.
If mask reference image is empty and maskMode is not set to MASK_MODE_USER_PROVIDED, the mask is computed based on the raw reference image.
REFERENCE_TYPE_CONTROL
If a raw reference image is provided, then the control image's size must have the same dimensions as the raw reference image.
If you don't provide an image with type `REFERENCE_TYPE_CONTROL` in `bytesBase64Encoded` format, then the model computes the control image from the image provided as `REFERENCE_TYPE_RAW`.
REFERENCE_TYPE_SUBJECT
The user can provide multiple reference images with the same reference ID. For example, multiple images for the same subject can have the same reference ID. This could potentially improve the output quality.
REFERENCE_TYPE_STYLE
A style reference image that the model uses to guide the style of the generated image.
referenceId	Required integer

The reference ID. Use this reference ID in the prompt. For example, use [1] to refer to the reference images with referenceId=1, [2] to refer to the reference images with referenceId=2.
referenceImage.bytesBase64Encoded	Required string

A Base64 string for the encoded reference image.
maskImageConfig.maskMode	Optional enumeration:
MASK_MODE_USER_PROVIDED, if the reference image is a mask image.
MASK_MODE_BACKGROUND, to automatically generate a mask using background segmentation.
MASK_MODE_FOREGROUND, to automatically generate a mask using foreground segmentation.
MASK_MODE_SEMANTIC, to automatically generate a mask using semantic segmentation, and the given mask class.

Specified when referenceType is set as REFERENCE_TYPE_MASK.
maskImageConfig.dilation
Optional float. Range: [0, 1]

The percentage of image width to dilate this mask by.

Specified when referenceType is set as REFERENCE_TYPE_MASK.
maskImageConfig.maskClasses
Optional list[Integer].

Mask classes for MASK_MODE_SEMANTIC mode.

Specified when referenceType is set as REFERENCE_TYPE_MASK.
controlImageConfig.controlType
Required enumeration:
CONTROL_TYPE_FACE_MESH for face mesh (person customization).
CONTROL_TYPE_CANNY for canny edge.
CONTROL_TYPE_SCRIBBLE for scribble.

Specified when referenceType is set as REFERENCE_TYPE_CONTROL.
controlImageConfig.enableControlImageComputation
Optional bool.
Default: false.

Set to false if you provide your own control image.
Set to true if you want to let Imagen compute the control image from the reference image.

Specified when referenceType is set as REFERENCE_TYPE_CONTROL.
language	
Optional: string (imagen-3.0-capability-001, imagen-3.0.generate-001, and imagegeneration@006 only)

The language code that corresponds to your text prompt language. The following values are supported:

auto: Automatic detection. If Imagen detects a supported language, the prompt and an optional negative prompt are translated to English. If the language detected isn't supported, Imagen uses the input text verbatim, which might result in an unexpected output. No error code is returned.
en: English (if omitted, the default value)
es: Spanish
hi: Hindi
ja: Japanese
ko: Korean
pt: Portuguese
zh-TW: Chinese (traditional)
zh or zh-CN: Chinese (simplified)
subjectImageConfig.subjectDescription
Required string.

A short description of the subject in the image. For example, a woman with short brown hair.

Specified when referenceType is set as REFERENCE_TYPE_SUBJECT.
subjectImageConfig.subjectType
Required enumeration:
SUBJECT_TYPE_PERSON: Person subject type.
SUBJECT_TYPE_ANIMAL: Animal subject type.
SUBJECT_TYPE_PRODUCT: Product subject type.
SUBJECT_TYPE_DEFAULT: Default subject type.

Specified when referenceType is set as REFERENCE_TYPE_SUBJECT.
styleImageConfig.styleDescription
Optional string.
A short description for the style.

Specified when referenceType is set as REFERENCE_TYPE_STYLE.
Response
The response body from the REST request.

Parameter
predictions	
An array of VisionGenerativeModelResult objects, one for each requested sampleCount. If any images are filtered by responsible AI, they are not included.

Vision generative model result object
Information about the model result.

Parameter
bytesBase64Encoded	
The base64 encoded generated image. Not present if the output image did not pass responsible AI filters.

mimeType	
The type of the generated image. Not present if the output image did not pass responsible AI filters.

Examples
The following examples show how to use the Imagen model to customize images.

Customize images
REST
Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
LOCATION: Your project's region. For example, us-central1, europe-west2, or asia-northeast3. For a list of available regions, see Generative AI on Vertex AI locations.
TEXT_PROMPT: The text prompt guides what images the model generates. To use Imagen 3 Customization, include the referenceId of the reference image or images you provide in the format [$referenceId]. For example:
The following text prompt is for a request that has two reference images with "referenceId": 1. Both images have an optional description of "subjectDescription": "man with short hair": Create an image about a man with short hair to match the description: A pencil style sketch of a full-body portrait of a man with short hair [1] with hatch-cross drawing, hatch drawing of portrait with 6B and graphite pencils, white background, pencil drawing, high quality, pencil stroke, looking at camera, natural human eyes
"referenceId": The ID of the reference image, or the ID for a series of reference images that correspond to the same subject or style. In this example the two reference images are of the same person, so they share the same referenceId (1).
BASE64_REFERENCE_IMAGE: A reference image to guide image generation. The image must be specified as a base64-encoded byte string.
SUBJECT_DESCRIPTION: Optional. A text description of the reference image you can then use in the prompt field. For example:

      "prompt": "a full-body portrait of a man with short hair [1] with hatch-cross
      drawing",
      [...],
      "subjectDescription": "man with short hair"
    
IMAGE_COUNT: The number of generated images. Accepted integer values: 1-4. Default value: 4.
HTTP method and URL:



POST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagen-3.0-capability-001:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "referenceImages": [
        {
          "referenceType": "REFERENCE_TYPE_SUBJECT",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "BASE64_REFERENCE_IMAGE"
          },
          "subjectImageConfig": {
            "subjectDescription": "SUBJECT_DESCRIPTION",
            "subjectType": "SUBJECT_TYPE_PERSON"
          }
        },
        {
          "referenceType": "REFERENCE_TYPE_SUBJECT",
          "referenceId": 1,
          "referenceImage": {
            "bytesBase64Encoded": "BASE64_REFERENCE_IMAGE"
          },
          "subjectImageConfig": {
            "subjectDescription": "SUBJECT_DESCRIPTION",
            "subjectType": "SUBJECT_TYPE_PERSON"
          }
        }
      ]
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagen-3.0-capability-001:predict"
The following sample response is for a request with "sampleCount": 2. The response returns two prediction objects, with the generated image bytes base64-encoded.

{
  "predictions": [
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    },
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}
Class IDs
Use the following object class IDs to automatically create an image mask based on specific objects.

Search for a segmentation class or ID
Class ID (class_id)	Object
0	backpack
1	umbrella
2	bag
3	tie
4	suitcase
5	case
6	bird
7	cat
8	dog
9	horse
10	sheep
11	cow
12	elephant
13	bear
14	zebra
15	giraffe
16	animal (other)
17	microwave
18	radiator
19	oven
20	toaster
21	storage tank
22	conveyor belt
23	sink
24	refrigerator
25	washer dryer
26	fan
27	dishwasher
28	toilet
29	bathtub
30	shower
31	tunnel
32	bridge
33	pier wharf
34	tent
35	building
36	ceiling
37	laptop
38	keyboard
39	mouse
40	remote
41	cell phone
42	television
43	floor
44	stage
45	banana
46	apple
47	sandwich
48	orange
49	broccoli
50	carrot
51	hot dog
52	pizza
53	donut
54	cake
55	fruit (other)
56	food (other)
57	chair (other)
58	armchair
59	swivel chair
60	stool
61	seat
62	couch
63	trash can
64	potted plant
65	nightstand
66	bed
67	table
68	pool table
69	barrel
70	desk
71	ottoman
72	wardrobe
73	crib
74	basket
75	chest of drawers
76	bookshelf
77	counter (other)
78	bathroom counter
79	kitchen island
80	door
81	light (other)
82	lamp
83	sconce
84	chandelier
85	mirror
86	whiteboard
87	shelf
88	stairs
89	escalator
90	cabinet
91	fireplace
92	stove
93	arcade machine
94	gravel
95	platform
96	playingfield
97	railroad
98	road
99	snow
100	sidewalk pavement
101	runway
102	terrain
103	book
104	box
105	clock
106	vase
107	scissors
108	plaything (other)
109	teddy bear
110	hair dryer
111	toothbrush
112	painting
113	poster
114	bulletin board
115	bottle
116	cup
117	wine glass
118	knife
119	fork
120	spoon
121	bowl
122	tray
123	range hood
124	plate
125	person
126	rider (other)
127	bicyclist
128	motorcyclist
129	paper
130	streetlight
131	road barrier
132	mailbox
133	cctv camera
134	junction box
135	traffic sign
136	traffic light
137	fire hydrant
138	parking meter
139	bench
140	bike rack
141	billboard
142	sky
143	pole
144	fence
145	railing banister
146	guard rail
147	mountain hill
148	rock
149	frisbee
150	skis
151	snowboard
152	sports ball
153	kite
154	baseball bat
155	baseball glove
156	skateboard
157	surfboard
158	tennis racket
159	net
160	base
161	sculpture
162	column
163	fountain
164	awning
165	apparel
166	banner
167	flag
168	blanket
169	curtain (other)
170	shower curtain
171	pillow
172	towel
173	rug floormat
174	vegetation
175	bicycle
176	car
177	autorickshaw
178	motorcycle
179	airplane
180	bus
181	train
182	truck
183	trailer
184	boat ship
185	slow wheeled object
186	river lake
187	sea
188	water (other)
189	swimming pool
190	waterfall
191	wall
192	window
193	window blind
Virtual Try-On API

bookmark_border
Preview

This product or feature is a Generative AI Preview offering, subject to the "Pre-GA Offerings Terms" of the Google Cloud Service Specific Terms, as well as the Additional Terms for Generative AI Preview Products. For this Generative AI Preview offering, Customers may elect to use it for production or commercial purposes, or disclose Generated Output to third-parties, and may process personal data as outlined in the Cloud Data Processing Addendum, subject to the obligations and restrictions described in the agreement under which you access Google Cloud. Pre-GA products are available "as is" and might have limited support. For more information, see the launch stage descriptions.

Virtual Try-On lets you generate images of people modeling clothing products. You provide an image of a person and a sample clothing product, and then you use Virtual Try-On to generate images of the person wearing the product.

Supported model versions
Virtual Try-On supports the following models:

virtual-try-on-preview-08-04
For more information about the features that the model supports, see Imagen models.

HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:predict \

-d '{
  "instances": [
    {
      "personImage": {
        "image": {
          // Union field can be only one of the following:
          "bytesBase64Encoded": string,
          "gcsUri": string,
        }
      },
      "productImages": [
        {
          "image": {
            // Union field can be only one of the following:
            "bytesBase64Encoded": string,
            "gcsUri": string,
          }
        }
      ]
    }
  ],
  "parameters": {
    "addWatermark": boolean,
    "baseSteps": integer,
    "personGeneration": string,
    "safetySetting": string,
    "sampleCount": integer,
    "seed": integer,
    "storageUri": string,
    "outputOptions": {
      "mimeType": string,
      "compressionQuality": integer
    }
  }
}'
Instances
personImage

string

Required. An image of a person to try-on the clothing product, which can be either of the following:

A bytesBase64Encoded string that encodes an image.
A gcsUri string URI to a Cloud Storage bucket location.
productImages

string

Required. An image of a product to try-on a person, which can be either of the following:

A bytesBase64Encoded string that encodes an image.
A gcsUri string URI to a Cloud Storage bucket location.
Parameters
addWatermark	
bool

Optional. Add an invisible watermark to the generated images.

The default value is true.

baseSteps

int

Required. An integer that controls image generation, with higher steps trading higher quality for increased latency.

Integer values greater than 0. The default is 32.

personGeneration	
string

Optional. Allow generation of people by the model. The following values are supported:

"dont_allow": Disallow the inclusion of people or faces in images.
"allow_adult": Allow generation of adults only.
"allow_all": Allow generation of people of all ages.
The default value is "allow_adult".

safetySetting	
string

Optional. Adds a filter level to safety filtering. The following values are supported:

"block_low_and_above": Strongest filtering level, most strict blocking. Deprecated value: "block_most".
"block_medium_and_above": Block some problematic prompts and responses. Deprecated value: "block_some".
"block_only_high": Reduces the number of requests blocked due to safety filters. May increase objectionable content generated by Imagen. Deprecated value: "block_few".
"block_none": Block very few problematic prompts and responses. Access to this feature is restricted. Previous field value: "block_fewest".
The default value is "block_medium_and_above".

sampleCount

int

Required. The number of images to generate.

An integer value between 1 and 4, inclusive. The default value is 1.

seed	
Uint32

Optional. The random seed for image generation. This isn't available when addWatermark is set to true.

storageUri	
string

Optional. A string URI to a Cloud Storage bucket location to store the generated images.

outputOptions	
outputOptions

Optional. Describes the output image format in an outputOptions object.

Output options object
The outputOptions object describes the image output.

Parameters
outputOptions.mimeType	
Optional: string

The image output format.. The following values are supported:

"image/png": Save as a PNG image
"image/jpeg": Save as a JPEG image
The default value is "image/png".

outputOptions.compressionQuality	
Optional: int

The level of compression if the output type is "image/jpeg". Accepted values are 0 through 100. The default value is 75.

Sample request
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_PERSON_IMAGE: The Base64-encoded image of the person image.
BASE64_PRODUCT_IMAGE: The Base64-encoded image of the product image.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1 to 4.
GCS_OUTPUT_PATH: The Cloud Storage path to store the virtual try-on output to.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict
Request JSON body:



{
  "instances": [
    {
      "personImage": {
        "image": {
          "bytesBase64Encoded": "BASE64_PERSON_IMAGE"
        }
      },
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_PRODUCT_IMAGE"
          }
        }
      ]
    }
  ],
  "parameters": {
    "sampleCount": IMAGE_COUNT,
    "storageUri": "GCS_OUTPUT_PATH"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/virtual-try-on-preview-08-04:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]
}

Imagen product recontext API

bookmark_border
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

The Imagen product recontext on Vertex AI API lets you edit product images into different scenes or backgrounds. You provide an image of a product and a prompt, and Imagen product recontext generates images of your product in new scenes or with different backgrounds.

To request access to Imagen product recontext, complete the Vertex AI - Generative Media for Marketing Access Request form.

Supported model versions
Imagen product recontext supports the following models:

imagen-product-recontext-preview-06-30
For more information about the features that the model supports, see Imagen models.

HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:predict \

-d '{
  "instances": [
    {
      "prompt": string,
      "productImages": [
        // The following fields can be repeated up to 3 times for different
        // views of the same product.
        {
          "image": {
            // Union field can be only one of the following:
            "bytesBase64Encoded": string,
            "gcsUri": string,
            // End of list of possible types for union field.
          },
        }
      ]
    }
  ],
  "parameters": {
    "addWatermark": boolean,
    "enhancePrompt": boolean
    "personGeneration": string,
    "safetySetting": string,
    "sampleCount": integer,
    "seed": integer,
    "storageUri": string,
    "outputOptions": {
      "mimeType": string,
      "compressionQuality": integer
    }
  }
}'
Instances
Instances
prompt	
string

Optional. A text prompt to guide the scene generation.

productImages	
list[productImages]

A list of up to three different ProductImages of different views of the same product. The model uses the images to recontextualize the product into different scenes or backgrounds.

productImages object
The productImages object describes the image assets to place into different scenes or backgrounds.

productImages
Union field image.

Product image for imagen-product-recontext-preview-06-30 as input for different views of the product. Each image can be either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

bytesBase64Encoded	
string

A bytes base64-encoded string of an image or a video file.

gcsUri	
string

A string URI to a Cloud Storage bucket location.

Parameters
Parameters
addWatermark	
boolean

Optional. Add an invisible watermark to the generated images.

The default value is true.

enhancePrompt	
boolean

Optional. An optional parameter to use an LLM-based prompt rewriting feature to deliver higher quality images that better reflect the original prompt's intent. Disabling this feature may impact image quality and prompt adherence.

The default value is true.

personGeneration	
string

Optional. Allow generation of people by the model. The following values are supported:

"dont_allow": Disallow the inclusion of people or faces in images.
"allow_adult": Allow generation of adults only.
"allow_all": Allow generation of people of all ages.
The default value is "allow_adult".

safetySetting	
string

Optional. Adds a filter level to safety filtering. The following values are supported:

"block_low_and_above": Strongest filtering level, most strict blocking. Deprecated value: "block_most".
"block_medium_and_above": Block some problematic prompts and responses. Deprecated value: "block_some".
"block_only_high": Reduces the number of requests blocked due to safety filters. Might increase objectionable content generated by Imagen. Deprecated value: "block_few".
"block_none": Block very few problematic prompts and responses. Access to this feature is restricted. Previous field value: "block_fewest".
The default value is "block_medium_and_above".

sampleCount

int

The number of images to generate.

An integer value in the range of 1-4. The default value is 1.

seed	
Uint32

Optional. The random seed for image generation. This isn't available when addWatermark is set to true.

If enhancePrompt is set to true, the seed parameter won't work, because enhancePrompt generates a new prompt, which results in a new or different image.

storageUri	
string

Optional. A string URI to a Cloud Storage bucket location that is used to store the generated images. If a Cloud Storage bucket isn't provided, then base64-encoded image bytes are returned in the response.

outputOptions	
outputOptions

Optional. Describes the output image format in an outputOptions object.

Output options object
The outputOptions object describes the image output.

Parameters
outputOptions.mimeType	
Optional: string

The image output format. The following values are supported:

"image/png": Save as a PNG image.
"image/jpeg": Save as a JPEG image.
The default value is "image/png".

outputOptions.compressionQuality	
Optional: int

The level of compression if the output type is "image/jpeg". Accepted values are 0- 100. The default value is 75.

Sample request
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
TEXT_PROMPT: Optional. A text prompt to guide the images that the model generates.
BASE64_SUBJECT_IMAGE: A base64-encoded image of the subject image.
PERSON_SETTING: Optional: A string value that controls the type of person or face generation that the model allows. You can use the following values for personGeneration:
"allow_adult": Allows generation of adults only, except for celebrity generation. Celebrity generation isn't allowed for any setting. This is the default setting.
"allow_all": Allows generation of people of all ages, except for celebrity generation. Celebrity generation is not allowed for any setting.
"dont_allow": Doesn't allow generation of people or faces in generated output.
SAFETY_SETTING: Optional: A string value that controls the safety filter threshold for generated images. You can use the following values to select your safety setting:
"block_low_and_above": The most stringent safety threshold. "block_low_and_above" filters the largest number of generated images.
"block_medium_and_above": The medium safety threshold, which balances filtering for potentially harmful and safe content. "block_medium_and_above" is the default safety setting.
"block_only_high": The lowest safety threshold, which reduces the number of requests blocked due to safety filter. Using "block_only_high" safety threshold might increase the number of objectionable images generated by the model.
WATERMARK_SETTING: Optional: A boolean. When this values is set to true, the model adds a digital watermark that you can use to verify the generated image. The default value is true.
IMAGE_COUNT: The number of images to generate. The accepted range of values is 1-4.
PROMPT_SETTING: Optional: A boolean. When this value is set to true, the model uses enhanced prmopts. The default value is true.
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-product-recontext-preview-06-30:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "productImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_SUBJECT_IMAGE"
          },
        }
      ]
    }
  ],
  "parameters": {
    "personGeneration": PERSON_SETTING,
    "safetySetting": SAFETY_SETTING,
    "addWatermark": WATERMARK_SETTING,
    "sampleCount": IMAGE_COUNT,
    "enhancePrompt": PROMPT_SETTING
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-product-recontext-preview-06-30:predict"
The request returns image objects. In this example, two image objects are returned, with two prediction objects as base64-encoded images.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    },
    {
      "bytesBase64Encoded": "BASE64_IMG_BYTES",
      "mimeType": "image/png"
    }
  ]
}

Imagen upscale API

bookmark_border
Preview

This product or feature is subject to the "Pre-GA Offerings Terms" in the General Service Terms section of the Service Specific Terms. Pre-GA products and features are available "as is" and might have limited support. For more information, see the launch stage descriptions.

Supported model versions
Imagen upscale supports the following models:

imagen-4.0-upscale-preview
For more information about the features that the model supports, see Imagen models.

HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/MODEL_ID:predict \

-d '{
  "instances": [
    {
      "prompt": string,
      "image": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
      },
    }
  ],
  "parameters": {
    "mode": "upscale",
    "storageUri": string,
    "outputOptions": {
      "mimeType": string,
      "compressionQuality": integer
    },
    "upscaleConfig": {
      "upscaleFactor": string
    }
  }
}'
Instances
Instances
Union field image.

The image that you are upscaling. You can provide either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

bytesBase64Encoded	
string

A bytes base64-encoded string of an image or a video file.

gcsUri	
string

A string URI to a Cloud Storage bucket location.

Parameters
Parameters
mode	
string

Required. For upscale requests pass the string "upscale".

storageUri	
string

Optional. A string URI to a Cloud Storage bucket location that is used to store the generated images. If a Cloud Storage bucket isn't provided, then base64-encoded image bytes are returned in the response.

outputOptions	
outputOptions

Optional. Describes the output image format in an outputOptions object.

upscaleConfig.upscaleFactor	
string

The scaling factor for the upscaled image. The following values are accepted:

"x2"
"x3"
"x4"
Output options object
The outputOptions object describes the image output.

Parameters
outputOptions.mimeType	
Optional: string

The image output format. The following values are supported:

"image/png": Save as a PNG image.
"image/jpeg": Save as a JPEG image.
The default value is "image/png".

outputOptions.compressionQuality	
Optional: int

The level of compression if the output type is "image/jpeg". Accepted values are 0- 100. The default value is 75.

Sample request
REST
Before using any of the request data, make the following replacements:

REGION: The region that your project is located in. For more information about supported regions, see Generative AI on Vertex AI locations.
PROJECT_ID: Your Google Cloud project ID.
BASE64_SUBJECT_IMAGE: A base64-encoded image of the subject image.
ADD_WATERMARK: An optional Boolean value. Set to true to enable watermarked images, or false to disable watermarked images. The default value is true.
GCS_IMAGE_PATH: A Cloud Storage path to an image file.
GCS_OUTPUT_PATH: the Cloud Storage path to store the generated output to.
OUTPUT_MIMETYPE: An optional string that defines the output file type of the image. The following values are accepted: "image/png" or "image/jpeg". The default is "image/png".
COMPRESSION_QUALITY: An optional integer value that specifies the level of detail that the model preserves for JPEG images. The following range of values are accepted: 0 - 100. The higher value specifies a higher compression level. The default is 75.
UPSCALE_FACTOR: The scaling factor for the upscaled image. The final output resolution of the image after applying the upscale factor must not exceed 17 megapixels. The following values are accepted:
"x2"
"x3"
"x4"
HTTP method and URL:



POST https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-4.0-upscale-preview:predict
Request JSON body:



{
  "instances": [
    {
      "prompt": "Upscale the image",
      "image": {
        // use one of the following to specify the image to upscale
        "bytesBase64Encoded": "BASE64_SUBJECT_IMAGE"
        "gcsUri": "GCS_IMAGE_PATH"
        // end of base image input options
      },
    }
  ],
  "parameters": {
    "mode": "upscale",
    "storageUri": "GCS_OUTPUT_PATH",
    "outputOptions": {
      "mimeType": "OUTPUT_MIMETYPE",
      "compressionQuality": COMPRESSION_QUALITY
    },
    "upscaleConfig": {
      "upscaleFactor": "UPSCALE_FACTOR"
    }
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/REGION/publishers/google/models/imagen-4.0-upscale-preview:predict"
The request returns an image object. In this example, an image objects is returned as a prediction object with a base64-encoded image.

{
  "predictions": [
    {
      "mimeType": "image/png",
      "bytesBase64Encoded": "BASE64_IMG_BYTES"
    }
  ]
}



Veo on Vertex AI video generation API

bookmark_border
Veo is the name of the model that supports video generation. Veo generates a video from a text prompt or an image prompt that you provide. For more information about Veo, see Veo video generation overview.

To explore this model in the console, see the Video Generation model card in the Model Garden.

Try Veo on Vertex AI (Vertex AI Studio)

Try Veo in a Colab

Supported Models
Veo API supports the following models:

veo-2.0-generate-001
veo-2.0-generate-exp
veo-2.0-generate-preview
veo-3.0-generate-001
veo-3.0-fast-generate-001
veo-3.0-generate-001
veo-3.0-fast-generate-001
veo-3.1-generate-001
veo-3.1-fast-generate-001
veo-3.1-generate-preview (Preview)
veo-3.1-fast-generate-001 (Preview)
For more information, see Veo models.

HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:predictLongRunning \

-d '{
  "instances": [
    {
      "prompt": string,
      "image": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "lastFrame": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "video": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string
      },
      "mask": {
        // Union field can be only one of the following:
        "bytesBase64Encoded": string,
        "gcsUri": string,
        // End of list of possible types for union field.
        "mimeType": string,
        "maskMode": string
      },
      "referenceImages": [
        // A list of up to three asset images or at most one style image for the
        // model to use when generating videos.
        //
        // referenceImages is supported by the following models:
        //
        // *   veo-2.0-generate-exp (Preview)
        // *   veo-3.1-generate-preview (Preview)
        {
        "image:" {
          // Union field can be only one of the following:
          "bytesBase64Encoded": string,
          "gcsUri": string,
          // End of list of possible types for union field.
          "mimeType": string
        },
        "referenceType": string
        }
      ]
    }
  ],
  "parameters": {
    "aspectRatio": string,
    "compressionQuality": string,
    "durationSeconds": integer,
    "enhancePrompt": boolean, // Veo 2 models only
    "generateAudio": boolean,
    "negativePrompt": string,
    "personGeneration": string,
    "resizeMode": string, // Veo 3 image-to-video only
    "resolution": string, // Veo 3 models only
    "sampleCount": integer,
    "seed": uint32,
    "storageUri": string
  }
}'
Instances
Instances
prompt

string

Required for text-to-video.
Optional if an input image prompt is provided (image-to-video).

A text string to guide the first eight seconds in the video. For example:

A fast-tracking shot through a bustling dystopian sprawl with bright neon signs, flying cars and mist, night, lens flare, volumetric lighting
A neon hologram of a car driving at top speed, speed of light, cinematic, incredible details, volumetric lighting
Many spotted jellyfish pulsating under water. Their bodies are transparent and glowing in deep ocean
extreme close-up with a shallow depth of field of a puddle in a street. reflecting a busy futuristic Tokyo city with bright neon signs, night, lens flare
Timelapse of the northern lights dancing across the Arctic sky, stars twinkling, snow-covered landscape
A lone cowboy rides his horse across an open plain at beautiful sunset, soft light, warm colors
image	
Union field

Optional. An image to guide video generation, which can be either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

lastFrame	
Union field

Optional. An image of the last frame of a video to fill the space between. lastFrame can be either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

lastFrame is supported by the following models:

veo-2.0-generate-001 (Preview)
veo-3.0-generate-exp (Preview)
veo-3.1-generate-preview (Preview)
veo-3.1-fast-generate-preview (Preview)
veo-3.1-generate-001
veo-3.1-fast-generate-001
video	
Union field

Optional. A Veo generated input video to extend in length. The input video can be between 1 and 30 seconds in length. You can provide either a bytesBase64Encoded string that encodes a video or a gcsUri string URI to a Cloud Storage bucket location.

video is supported by the following models:

veo-2.0-generate-001 (Preview)
veo-3.0-generate-exp (Preview)
veo-3.1-generate-preview (Preview)
veo-3.1-fast-generate-preview (Preview)
mask	
Union field

Optional. An image of a mask to apply to a video to add or remove an object from a video. mask can be either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

mask is supported by veo-2.0-generate-preview in Preview.

referenceImages	
list[referenceImages]

Optional. A list of up to three asset images or at most one style images that describes the referenceImages for the model to use when generating videos.

Important: Veo 3.1 models don't support referenceImages.style. Use veo-2.0-generate-exp when using style images.
referenceImages is supported by the following models:

veo-2.0-generate-exp (Preview)
veo-3.1-generate-preview (Preview)
referenceImages.image	
Union field

Optional. Contains the reference images to use as subject matter input. Each image can be either a bytesBase64Encoded string that encodes an image or a gcsUri string URI to a Cloud Storage bucket location.

referenceImages.referenceType	
string

Required in a referenceImages object. Specifies the type of reference image provided. The following values are supported:

"asset": The reference image provides assets for the generated video, such as: the scene, an object, or a character.
"style": The reference image provides style information for the generated videos, such as: scene colors, lighting, or texture.

Important: Veo 3.1 models don't support referenceImages.style. Use veo-2.0-generate-exp when using style images.
bytesBase64Encoded	
string

A bytes base64-encoded string of an image or video file. Used with the following objects:

image
video
lastFrame
referenceImages.image
gcsUri	
string

A string URI to a Cloud Storage bucket location. Used with the following objects:

image
video
lastFrame
referenceImages.image
mimeType	
string

Required for the following objects:

image
video
mask
lastFrame
referenceImages.image
Specifies the mime type of a video or image.

For images, the following mime types are accepted:

image/jpeg
image/png
image/webp
For videos, the following mime types are accepted:

video/mov
video/mpeg
video/mp4
video/mpg
video/avi
video/wmv
video/mpegps
video/flv
Parameters
Parameters
aspectRatio	
string

Optional. Specifies the aspect ratio of generated videos. The following are accepted values:

"16:9"
"9:16"
The default value is "16:9".

compressionQuality	
string

Optional. Specifies the compression quality of the generated videos. The accepted values are "optimized" or "lossless".

The devault is "optimized".

durationSeconds	
integer

Required. The length in seconds of video files that you want to generate.

The following are the accepted values:

Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4,6, or 8. The default is 8.
When using referenceImages: 8.
For more information, see Veo models.

enhancePrompt	
boolean

Optional. Use Gemini to enhance your prompts. Accepted values are true or false. The default value is true.

enhancePrompt is supported by the following models:

veo-2.0-generate-001
veo-2.0-generate-preview
veo-2.0-generate-exp
generateAudio	
boolean

Required for Veo 3 models. Generate audio for the video. Accepted values are true or false.

generateAudio isn't supported by veo-2.0-generate-001 or veo-2.0-generate-exp.

For more information about available Veo models, see Veo models.

negativePrompt	
string

Optional. A text string that describes anything you want to discourage the model from generating. For example:

overhead lighting, bright colors
people, animals
multiple cars, wind
personGeneration	
string

Optional. The safety setting that controls whether people or face generation is allowed. One of the following:

"allow_adult" (default value): allow generation of adults only
"dont_allow": disallows inclusion of people/faces in images
"allow_all": Allows the generation of people of all ages. To use this value, your project must be on an allowlist.
resizeMode	
string

Optional. Veo 3 models only, used with image for image-to-video. The resize mode that the model uses to resize the video. Accepted values are "pad" (default) or "crop".

resolution	
string

Optional. Veo 3 models only. The resolution of the generated video. Accepted values are "720p" (default) or "1080p".

sampleCount	
int

Optional. The number of output videos requested. Accepted values are 1-4.

seed	
uint32

Optional. A number to request to make generated videos deterministic. Adding a seed number with your request without changing other parameters will cause the model to produce the same videos.

The accepted range is 0-4,294,967,295.

storageUri	
string

Optional. A Cloud Storage bucket URI to store the output video, in the format gs://BUCKET_NAME/SUBDIRECTORY. If a Cloud Storage bucket isn't provided, base64-encoded video bytes are returned in the response.

Sample requests
Use the following examples to create your own video request:

Text-to-video generation request
REST
To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string respresenting the model ID to use. The following are accepted values:
Veo 2: "veo-2.0-generate-001"
Veo 3:"veo-3.0-generate-001"
Veo 3:"veo-3.0-fast-generate-001"
Veo 3:"veo-3.0-generate-preview" (Preview)
Veo 3:"veo-3.0-fast-generate-preview" (Preview)
Veo 3.1: "veo-3.1-generate-001"
Veo 3.1: "veo-3.1-fast-generate-001"
TEXT_PROMPT: The text prompt used to guide video generation.
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
DURATION: An integer representing the length of the generated video files. The following are the accepted values for each model:
Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4, 6, or 8. The default is 8.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT"
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": "RESPONSE_COUNT"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Image-to-video generation request
REST
To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

PROJECT_ID: A string representing your Google Cloud project ID.
MODEL_ID: A string respresenting the model ID to use. The following are accepted values:
Veo 2:veo-2.0-generate-001
Veo 3:veo-3.0-generate-001
Veo 3.1:veo-3.1-generate-001
Veo 3.1:veo-3.1-fast-generate-001
TEXT_PROMPT: The text prompt used to guide video generation.
INPUT_IMAGE: A base64-encoded string that represents the input image. For best quality, we recommend that the input image's resolution be 720p (1280 x 720 pixels) or higher, and have an aspect ratio of either 16:9 or 9:16. Images of other aspect ratios or sizes may be resized or centrally cropped when the image is uploaded.
MIME_TYPE: A string representing the MIME type of the input image. Only the images of the following MIME types are supported:
"image/jpeg"
"image/png"
OUTPUT_STORAGE_URI: Optional: A string representing the Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: "gs://video-bucket/output/".
RESIZE_MODE: A string that represents the resize mode to use. The following are accepted values:
"crop": Crop the video to fit the new size.
"pad": Pad the video to fit the new size.
RESPONSE_COUNT: The number of video files to generate. The accepted range of values is 1-4.
DURATION: An integer representing the length of the generated video files. The following are the accepted values for each model:
Veo 2 models: 5-8. The default is 8.
Veo 3 models: 4, 6, or 8. The default is 8.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "image": {
        "bytesBase64Encoded": "INPUT_IMAGE",
        "mimeType": "MIME_TYPE"
      }
    }
  ],
  "parameters": {
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
    "resizeMode": "RESIZE_MODE"
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Video request using asset images
REST
To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: A string representing the model ID to use. The following are accepted values:
Veo 2: veo-2.0-generate-exp
Veo 3: veo-3.1-generate-preview
TEXT_PROMPT: The text prompt used to guide video generation.
BASE64_ENCODED_IMAGE: A base64-bytes encoded subject image. You can repeat this field and mimeType to specify up to three subject images.
IMAGE_MIME_TYPE: The MIME type of the input image. Only one of the following:

image/jpeg
image/png
You can repeat this field and bytesBase64Encoded to specify up to three subject images.

OUTPUT_STORAGE_URI: Optional: The Cloud Storage bucket to store the output videos. If not provided, a Base64-bytes encoded video is returned in the response. For example: gs://video-bucket/output/.
RESPONSE_COUNT: The number of video files you want to generate. Accepted integer values: 1-4.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      // The following fields can be repeated for up to three total
      // images.
      "referenceImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_ENCODED_IMAGE",
            "mimeType": "IMAGE_MIME_TYPE"
          },
          "referenceType": "asset"
        }
      ]
    }
  ],
  "parameters": {
    "durationSeconds": 8,
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Video request using a style image
REST
To test a text prompt by using the Vertex AI Veo API, send a POST request to the publisher model endpoint.

Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: A string representing the model ID to use. Use the following value: veo-2.0-generate-exp.

Important: Veo 3.1 models don't support referenceImages.style. Use veo-2.0-generate-exp when using style images.
TEXT_PROMPT: The text prompt used to guide video generation.
BASE64_ENCODED_IMAGE: A base64-bytes encoded style image.
IMAGE_MIME_TYPE: The MIME type of the input image. Only one of the following:
image/jpeg
image/png
OUTPUT_STORAGE_URI: Optional: The Cloud Storage bucket to store the output videos. If not provided, video bytes are returned in the response. For example: gs://video-bucket/output/.
RESPONSE_COUNT: The number of video files you want to generate. Accepted integer values: 1-4.
Additional optional parameters

HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning
Request JSON body:



{
  "instances": [
    {
      "prompt": "TEXT_PROMPT",
      "referenceImages": [
        {
          "image": {
            "bytesBase64Encoded": "BASE64_ENCODED_IMAGE",
            "mimeType": "IMAGE_MIME_TYPE"
          },
          "referenceType": "style"
        }
      ]
    }
  ],
  "parameters": {
    "durationSeconds": 8,
    "storageUri": "OUTPUT_STORAGE_URI",
    "sampleCount": RESPONSE_COUNT
  }
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:predictLongRunning"
This request returns a full operation name with a unique operation ID. Use this full operation name to poll that status of the video generation request.

{
  "name":
  "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/a1b07c8e-7b5a-4aba-bb34-3e1ccb8afcc8"
}
Poll the status of the video generation long-running operation
Check the status of the video generation long-running operation.

REST

Before using any of the request data, make the following replacements:

PROJECT_ID: Your Google Cloud project ID.
MODEL_ID: The model ID to use.
OPERATION_ID: The unique operation ID returned in the original generate video request.
HTTP method and URL:



POST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation
Request JSON body:



{
  "operationName": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
To send your request, choose one of these options:

curl
PowerShell
Note: The following command assumes that you have logged in to the gcloud CLI with your user account by running gcloud init or gcloud auth login , or by using Cloud Shell, which automatically logs you into the gcloud CLI . You can check the currently active account by running gcloud auth list.
Save the request body in a file named request.json, and execute the following command:



curl -X POST \
     -H "Authorization: Bearer $(gcloud auth print-access-token)" \
     -H "Content-Type: application/json; charset=utf-8" \
     -d @request.json \
     "https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID:fetchPredictOperation"
This request returns information about the operation, including if the operation is still running or is done.
Response
Response body (generate video request)
Sending a text-to-video or image-to-video request returns the following response:



{
  "name": string
}
Response element	Description
name	The full operation name of the long-running operation that begins after a video generation request is sent.
Sample response (generate video request)


{
  "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID"
}
Response body (poll long-running operation)
Polling the status of the original video generation long-running operation returns a response similar to the following:



{
   "name": string,
   "done": boolean,
   "response":{
      "@type":"type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
      "raiMediaFilteredCount": integer,
      "videos":[
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
         {
           "gcsUri": string,
           "mimeType": string
         },
      ]
   }
}
Note: If you didn't specify a Cloud Storage bucket URI in storageUri, then the videos object returns bytesBase64Encoded strings.
Response element	Description
bytesBase64Encoded	A Base64 bytes encoded string that represents the video object.
done	A boolean value that indicates whether the operation is complete.
encoding	The video encoding type.
gcsUri	The Cloud Storage URI of the generated video.
name	The full operation name of the long-running operation that begins after a video generation request is sent.
raiMediaFilteredCount	Returns a count of videos that Veo filtered due to responsible AI policies. If no videos are filtered, the returned count is 0.
raiMediaFilteredReasons	Lists the reasons for any Veo filtered videos due to responsible AI policies. For more information, see Safety filter code categories.
response	The response body of the long-running operation.
video	The generated video.
Sample response (poll long-running operation)


{
   "name": "projects/PROJECT_ID/locations/us-central1/publishers/google/models/MODEL_ID/operations/OPERATION_ID",
   "done":true,
   "response":{
      "@type":"type.googleapis.com/cloud.ai.large_models.vision.GenerateVideoResponse",
      "raiMediaFilteredCount": 0,
      "videos":[
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_0.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_1.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_2.mp4",
          "mimeType":"video/mp4"
        },
        {
          "gcsUri":"gs://STORAGE_BUCKET/TIMESTAMPED_SUBDIRECTORY/sample_3.mp4",
          "mimeType":"video/mp4"
        }
      ]
   }
}
More information
For more information about using Veo on Vertex AI, see Generate videos using text and image prompts using Veo.


Lyria API

bookmark_border
Lyria is a new foundation model for high-quality audio generation, capable of creating diverse soundscapes and musical pieces from text prompts. Lyria enables users to generate high-quality instrumental music from text prompts.

To explore this model in the console, see the Lyria model card in the Model Garden (accessible using the Media Studio tab).

Try Lyria on Vertex AI (Vertex AI Studio)

Try Lyria in a Colab

Supported Models
The Lyria API supports the following model:

lyria-002
HTTP request


curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://LOCATION[-aiplatform.googleapis.com/v1/projects/](https://-aiplatform.googleapis.com/v1/projects/)PROJECT_ID/locations/LOCATION/publishers/google/models/lyria-002:predict \
  -d '{
    "instances": [
      {
        "prompt": "string",
        "negative_prompt": "string", // Optional
        "seed": 0 // Optional. Cannot be used with sample_count.
      }
    ],
    "parameters": {
      "sample_count": 1 // Optional. Cannot be used with seed.
    }
  }'
Use the following parameters for the Lyria model. For more information, see the Lyria Model Garden card details.

Parameter
prompt

(in instances object)

string

Required. The text description in US English (en-us) of the audio to generate.

Example: "An energetic electronic dance track with a fast tempo."

negative_prompt

(in instances object)

string

Optional. A description of what to exclude from the generated audio.

Example: "vocals, slow tempo"

seed

(in instances object)

integer

Optional. A seed for deterministic generation. If provided, the model will attempt to produce the same audio given the same prompt and other parameters.

Cannot be used with sample_count in the same request.

Example: 12345

sample_count

(in parameters object)

integer

Optional. The number of audio samples to generate. Default is 1 if not specified and seed is not used.

Cannot be used with seed in the same request.

Example: 2

Sample request
Use the following request to generate instrumental music from a text prompt:

Text-to-music generation request
curl
JSON
JSON with sample_count


curl -X POST \
-H "Authorization: Bearer $(gcloud auth print-access-token)" \
-H "Content-Type: application/json" \
https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/lyria-002:predict \
-d '{
  "instances": [
    {
      "prompt": "A calm acoustic folk song with a gentle guitar melody and soft strings.",
      "negative_prompt": "drums, electric guitar",
      "seed": 98765
    }
  ],
  "parameters": {}
}'
Response body
A successful request returns a JSON object containing the generated audio data. Each generated audio clip is 30 seconds long and provided as a WAV audio file at a 48kHz sample rate.



{
  "predictions": [
    {
      "audioContent": "BASE64_ENCODED_WAV_STRING_SAMPLE_1",
      "mimeType": "audio/wav"
    }
    // Additional audio samples will be listed here if sample_count > 1
    // e.g.,
    //{"audioContent": "BASE64_ENCODED_WAV_STRING_SAMPLE_2",
    // "mimeType": "audio/wav"
    //}
  ],
  "deployedModelId": "xxxxxxxxxxxxxxx", // Actual ID may vary based on deployment
  "model": "projects/PROJECT_ID/locations/LOCATION/publishers/google/models/lyria-002",
  "modelDisplayName": "Lyria 2"
}
Response element
predictions	
array

An array of generated audio samples. Each object in the array represents one audio clip.

predictions[].audioContent	
string

Base64-encoded string of the generated WAV audio data.

predictions[].mimeType	
string

The MIME type of the audio data. For Lyria, this is "audio/wav".

deployedModelId	
string

The ID of the deployed model that processed the request (if applicable for the endpoint type).

model	
string

The full resource name of the model that processed the request.

modelDisplayName	
string

The display name of the model.

Best practices and limitations
Refer to the Lyria Model Card for detailed best practices on prompting, language support (US English only for prompts), generation times, output format (WAV, 48 kHz, 30s instrumental clips), safety measures, and deployment information.

Key points:

Detailed Prompts: Generally lead to better audio.
Specify: Genre, mood, instrumentation, tempo.
Negative Prompting: Use negative_prompt to exclude elements.
Output: 30-second WAV audio clips, 48 kHz, instrumental only.
Safety: Content safety filters, recitation checking, artist intent checks, and SynthID watermarking are applied.
Pricing
Lyria 2 usage is priced at $0.06 per 30 seconds of output music generated. For more details, see Vertex AI Pricing.